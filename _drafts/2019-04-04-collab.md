---
title: "Introduction to Collaborative Filtering and Neural Network"
date: 2019-04-04
permalink: /collab/
tags: [fastai, deep learning, collaborative filtering, recommender system, neural net]
excerpt: "A sneak peak on collaborative filtering and neural network"
mathjax: "true"
published: false
---

In this blogpost, we will explore a little on **collaborative filtering**, so what is it? It's basically having some sort of information about what a person like, and then recommeding them other products/services that they might also like.

One of the popular applications of it is to build a **recommender system**, for example Netflix used it to  try to recommend their users the movies/series that they might like, with a percentage score to reflect how close is the movie in comparison with the users' movie taste. Another example would be say for Amazon, they use it to recommend product to their users based on their purchase history, browsing history, reviews that they left, etc.

# Colab Cloud Server VM setup & FastAI Configurations

First we need to use GPU for deep learning applications, so instead of buying, we can rent it from cloud servers. There are many other server options, but basically I chose Colab because issa FREE.

To learn how to set up a Colab Server which supports fastai library and its applications, click [here](https://course.fast.ai/start_colab.html).

NB: This is a free service that may not always be available, and requires extra steps to ensure your work is saved. Be sure to read the docs on the Colab web-site to ensure you understand the limitations of the system.


```python
# Permit collaboratory instance to read/write to Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
root_dir = "/content/gdrive/My Drive/"
base_dir = root_dir + 'fastai-v3/'
```

    Mounted at /content/gdrive



```python
  !curl -s https://course.fast.ai/setup/colab | bash
```

    Updating fastai...
    Done.



```python
%reload_ext autoreload
%autoreload 2
%matplotlib inline
```


```python
from fastai.collab import *
from fastai.tabular import *
```

# Collaborative filtering dataset

For collaborative filtering, there’s a really nice dataset called [MovieLens](https://grouplens.org/datasets/movielens/) created by GroupLens group and you can download various different sizes (20 million ratings, 100,000 ratings).

However, this blogpost focuses on learning the concept behind a simple neural network, so we will play with an even smaller dataset created by fast.ai to kickstart, which we will import with the built-in factory method `untar_data`.

`collab` models use data in a `DataFrame` of user, items, and ratings.


```python
user,item,title = 'userId','movieId','title'
```


```python
path = untar_data(URLs.ML_SAMPLE)
path
```




    PosixPath('/content/data/movie_lens_sample')




```python
ratings = pd.read_csv(path/'ratings.csv')
ratings.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>userId</th>
      <th>movieId</th>
      <th>rating</th>
      <th>timestamp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>73</td>
      <td>1097</td>
      <td>4.0</td>
      <td>1255504951</td>
    </tr>
    <tr>
      <th>1</th>
      <td>561</td>
      <td>924</td>
      <td>3.5</td>
      <td>1172695223</td>
    </tr>
    <tr>
      <th>2</th>
      <td>157</td>
      <td>260</td>
      <td>3.5</td>
      <td>1291598691</td>
    </tr>
    <tr>
      <th>3</th>
      <td>358</td>
      <td>1210</td>
      <td>5.0</td>
      <td>957481884</td>
    </tr>
    <tr>
      <th>4</th>
      <td>130</td>
      <td>316</td>
      <td>2.0</td>
      <td>1138999234</td>
    </tr>
  </tbody>
</table>
</div>



So this dataset is stored in a CSV, we can easily read it with pandas, we can see that it stores a list of user IDs and movie IDs, its not important to know who the users are or what movies are the dataset referring to at the moment, and then there's rating which indicates the movie rating rated by the users and a timestamp (but we will ignore the timestamp for now).


## Creating `DataBunch` object with `CollabDataBunch`


```python
data = CollabDataBunch.from_df(ratings, seed=42)
```


```python
# this ensures that the activation output is between 0 and 5, will talk more about this
y_range = [0,5.5]
```


```python
learn = collab_learner(data, n_factors=50, y_range=y_range)
```

As usual, we will put data into the learner object `collab_learner` these two arguments are worth to mention.

`n_factors`: The architecture, you have to tell it how many factors(n_factors) you want to use

`y_range`: tell it what the range of scores are.



```python
learn.fit_one_cycle(3, 5e-3)
```


Total time: 00:02 <p><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.612068</td>
      <td>0.968084</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.850343</td>
      <td>0.671973</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.650419</td>
      <td>0.663683</td>
      <td>00:00</td>
    </tr>
  </tbody>
</table>


After learning the data, we can now use this learner to pick a user ID and a movie ID to predict if the user likes the movie. However, we are not concern with how good the model is at the moment since this is a small dataset, our primary focus it to dig deeper into how all this can be done.

## A simple collaborative filtering example with Microsoft Excel

So this particular data is grabbed from MovieLens data filtering 15 users that watched the most movies and movies that were the most watched. This dataset has a few gaps but is definitely not a sparse matrix.

**Note:** In practice, data is most probably not stored this way, because then it would be a sparse matrix, meaning most users don't watch most movies, and most movies aren't watched by most users, you then have a lot of blanks in the dataset, making it a cost-inefficient way to store data.

![non_sparse](https://i.imgur.com/qbba3b7.png)

So looking at this dataset, we know that user 293 hasn't watched movie 49, so we want to build a model that tells us if he/she would like the movie. So we need to come up with some function that can represent that decision.

Here’s a simple possible approach. We’re going to take this idea of doing some matrix multiplications. First, we initialized five totally random numbers for each movie and user respectively, as you can see highlighted in red above is a vector of 5 random numbers for `userid` 14 and highlighted in purple is the vector of 5 random numbers for `movieid` 27.

We then drag across and do this for every other possible combination in the dataset, if there's missing value, a value of zero will be inserted into the cell.

Now, they don't mean anything yet, but we can now use gradient descent to try to make the user vector (left) and the movie vector (top)  give us results that are closer to the table above.

![mat_mul](https://i.imgur.com/2IRtNLS.jpg)

We will set this up as a linear model, and use **Root Mean Squared Error (RMSE)** as the loss function.

Let's take user 14 and movie 27 as an example, the rating should be 3, but in the matrix multiplacation of two random vectors, it gives a rating of 0.91, the loss would then be  $(3-0.91)^2$, we then do this for every other possible combinations, add them all up, and then take the mean of it.

In Excel, we can do this easily with **`SQRT(SUMXMY2(RANGE1:RANGE2)/COUNT(RANGE1))`**

We then use Excel “Solver” which does normal gradient descent to minimize the loss function. we select cell that represents our **loss function (cell V41)** and cells that represent the **matrix multiplications (H19 to V23) and (B25 to F39)**


![solver](https://i.imgur.com/65BAI9H.png)

and voila...

![mat_mul](https://i.imgur.com/fNwzsCw.png)

This is a basic starting point of a neural net is that which takes matrix multiplication of two matrices, and that formed the first layer of the neural network, so what we have witnessed is basically a single linear layer neural network that uses gradient descent to solve a collaborative filtering problem.

Let's make this clearer, we have `RANGE1` and `RANGE2`, and we minimize the loss function so that the matrix multiplication of the weights from `RANGE2` return values that are close to the values in `RANGE1`.

Question is, what is the whole point of this? Read on.

# Understanding the source code of `collab_learner`



```
learn = collab_learner(data, n_factors=50, y_range=y_range)
```



Previously, we used `collab_learner` to get a model, we will use an editor to dig through the source code to learn more about it. Personally, I browse the source code on Github using [Sourcegraph chrome extension](https://chrome.google.com/webstore/detail/sourcegraph/dgjhfomjieaadpoljlnidmbgkdffpack?hl=en).

What you want to do first is visit fastai documentation about collab [here](https://docs.fast.ai/collab.html#CollabLearner), and then click the [source](https://github.com/fastai/fastai/blob/master/fastai/collab.py#L70) next to the function.

If you are viewing it with Sourcegraph like I did, it should look something like this..

![collab_source](https://i.imgur.com/Z01dfM8.png)



The key thing to focus for now is that this functions create a model called `EmbeddingDotBias`,  and so we navigate to the definition of it.

![embdotbias](https://i.imgur.com/8c7rSfw.png)

The models that are being created by fastai are actually PyTorch models and a PyTorch model is called an `nn.Module`.

Touching a little on the OOP of Python,  `__ init __()` is how we create the model, and `forward()` is how we run the model.

There's no mention of how to calculate gradient in the function because PyTorch automatically does it for us, all we have to do is define how to calculate the output of the model.

So in this case, the model contains:

- a set of weights for a user - `self.u_weight`
- a set of weights for an item - `self.i_weight`
- a set of biases for a user - `self.u_bias`
- a set of biases for an item - `self.i_bias`

and all of this comes from `embedding`, so what is it?




## Embeddings

![emb](https://i.imgur.com/cG4PI5X.png)

In PyTorch, they have a lot of standard neural network layers set up for us. So it creates an embedding by calling `nn.Embedding` and  `trunc_normal_` to randomize it. The result is a normal random numbers for the embeddings.

**So what is embeddings?**

It is a matrix of weights. Specifically, an embedding is a matrix of weights that looks something like the matrix multiplication weights in the Excel example previously (i.e. `userid` weight matrices and `movieid` weight matrices).

![emb_wm](https://i.imgur.com/VYIsv3V.png)

An embedding matrix is just a weight matrix that is designed to be something that we can index into it as an array, and grab one vector out of it. In this case, we have an embedding matrix for a user and an embedding matrix for a movie, and we have been taking the dot product of them.

But that is not enough, we need to add a **bias term** to the movie weight matrices, and a bias term to the user weight matrices, why do we need to add bias terms? Loosely speaking, the bias terms in this example reflect how popular the movie is, and how much a user like to watch movie.

Mathematically speaking, adding bias terms to the weight matrix multiplications allows the neural network function to shift from the origin, which is beyond the scope of this blogpost, but you can always check out these useful resources I found:
- [Why do we need the bias term in ML algorithms such as linear regression and neural networks?](https://www.quora.com/Why-do-we-need-the-bias-term-in-ML-algorithms-such-as-linear-regression-and-neural-networks)
- [How Neural Networks Work - Bias - Part 3](https://www.youtube.com/watch?v=LnLGZxE5GX4)

---

Reviewing the `EmbeddingDotBias` function

![embdotbias](https://i.imgur.com/8c7rSfw.png)





see that in the `forward()` function, `dot` is the dot product of the embeddings, and `res` is the sum of the dot product of the embeddings plus the `users` bias term and `items` bias term, and then pass the result to a sigmoid activation function.

If you then multiply that by `y_range[1]` minus `y_range[0]` plus `y_range[0]`, then that’s going to give you something that’s between `y_range[0]` and `y_range[1]`. Technically speaking, this is unnecessary because our parameters could learn a set of weights that gives about the right number.

But we still do it so that  it can spend more of its weights predicting the thing we care about which is deciding who’s going to like which movie. It's a way to tweak around to get a better performing neural network.  

# Important Terminology

These are some of the most important terminology used in neural network, but for now, we need to understand the **bolded terminology**

- **Inputs**
- **Weights/parameters**
  - Random
- **Activations**
- **Activation functions / nonlinearities (e.g. ReLU / sigmoid)**
- **Output**
- **Loss**
- Metric
- Cross-entropy
- Softmax
- Fine tuning
- Layer deletion and random weights
- Freezing & unfreezing








Personally I think it helps a lot to start watching YouTube videos that teaches the concept of neural network so we can learn what these terminology means, so I found a few thats useful for noobs.

- [How Neural Network Works Youtube Playlist by James Oliver](https://www.youtube.com/watch?v=oFTpK_v9llw&list=PLMT-ZFEXpkZ0FDIQrq_dpY4oQqTU9tLOw) (approximately 30 mins)
- [Deep Learning Youtube Playlist by 3Blue1Brown](https://www.youtube.com/watch?v=aircAruvnKk&list=PLLMP7TazTxHrgVk7w1EKpLBIDoC50QrPS) (approximately 1 hour+)
- [Series on Neural Networks Youtube Playlist by Luis Serrano]() (approximately 1.5 hours)

This blogpost didn't really showcase the power of a neural network in building a recommender system, because we are mostly learning about the concept of neural network, because in the upcoming blogposts, we will build a collaborative filtering recommender system on top of what we learned in this blogpost.  See you soon!

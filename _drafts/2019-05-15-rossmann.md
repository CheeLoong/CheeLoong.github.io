---
title: "Neural Net on Rossmann with Preprocessors, Dropout, and Batchnorm"
date: 2019-05-15
permalink: /rossmann/
tags: [fastai, preprocessors, dropout, batchnorm, rossmann, kaggle]
excerpt: "Let's build neural net on Rossmann!"
mathjax: "true"
published: false
---

In this blogpost, we will be using [Rossmann dataset](https://www.kaggle.com/c/rossmann-store-sales/data) which was featured on Kaggle 3 years ago.

It has historical sales data for 1,115 Rossmann stores. The task is to forecast the "Sales" column for the test set, we have explored feature engineering and data cleaning part of the dataset, and now we will talk about the preprocessors that were commonly used in deep learning, as well as a new regularization technique called dropout regularization, we will also learn batch normalization which helps batch data to reduce the volatility in the loss, which allows us to use a higher learning rate to train faster.  

As usual, let's do the cloud server set up before proceeding..

# Colab Cloud Server VM setup & FastAI Configurations

First we need to use GPU for deep learning applications, so instead of buying, we can rent it from cloud servers. There are many other server options, but basically I chose Colab because issa FREE.

To learn how to set up a Colab Server which supports fastai library and its applications, click [here](https://course.fast.ai/start_colab.html).

NB: This is a free service that may not always be available, and requires extra steps to ensure your work is saved. Be sure to read the docs on the Colab web-site to ensure you understand the limitations of the system.


```python
# Permit collaboratory instance to read/write to Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
root_dir = "/content/gdrive/My Drive/"
base_dir = root_dir + 'fastai-v3/'
```

    Mounted at /content/gdrive



```python
  !curl -s https://course.fast.ai/setup/colab | bash
```

    Updating fastai...
    Done.



```python
%reload_ext autoreload
%autoreload 2
%matplotlib inline
```


```python
from fastai.basics import *
```


```python
from fastai.tabular import *
```

# Rossmann

## Data preparation


```python
d_path = "/content/data/"
folder = "rossmann"

path = Path(d_path + folder)
path.mkdir(parents=True, exist_ok=True)
```

To create the feature-engineered train_clean and test_clean from the Kaggle competition data, visit the [previous blogpost](https://cheeloong.github.io/clean-rossmann/#), save the work as pickle files, upload it to a data folder in google drive, and read it with `pd.read_pickle`


```python
train_df = pd.read_pickle(path/'train_clean')
```


```python
train_df.shape
```




    (844338, 93)



Since there are so many columns, it's probably a good idea to view the transposed format of the dataframe.


```python
train_df.head().T
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>index</th>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
    </tr>
    <tr>
      <th>Store</th>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>5</td>
    </tr>
    <tr>
      <th>DayOfWeek</th>
      <td>5</td>
      <td>5</td>
      <td>5</td>
      <td>5</td>
      <td>5</td>
    </tr>
    <tr>
      <th>Date</th>
      <td>2015-07-31 00:00:00</td>
      <td>2015-07-31 00:00:00</td>
      <td>2015-07-31 00:00:00</td>
      <td>2015-07-31 00:00:00</td>
      <td>2015-07-31 00:00:00</td>
    </tr>
    <tr>
      <th>Sales</th>
      <td>5263</td>
      <td>6064</td>
      <td>8314</td>
      <td>13995</td>
      <td>4822</td>
    </tr>
    <tr>
      <th>Customers</th>
      <td>555</td>
      <td>625</td>
      <td>821</td>
      <td>1498</td>
      <td>559</td>
    </tr>
    <tr>
      <th>Open</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>Promo</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>StateHoliday</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>SchoolHoliday</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>Year</th>
      <td>2015</td>
      <td>2015</td>
      <td>2015</td>
      <td>2015</td>
      <td>2015</td>
    </tr>
    <tr>
      <th>Month</th>
      <td>7</td>
      <td>7</td>
      <td>7</td>
      <td>7</td>
      <td>7</td>
    </tr>
    <tr>
      <th>Week</th>
      <td>31</td>
      <td>31</td>
      <td>31</td>
      <td>31</td>
      <td>31</td>
    </tr>
    <tr>
      <th>Day</th>
      <td>31</td>
      <td>31</td>
      <td>31</td>
      <td>31</td>
      <td>31</td>
    </tr>
    <tr>
      <th>Dayofweek</th>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
    </tr>
    <tr>
      <th>Dayofyear</th>
      <td>212</td>
      <td>212</td>
      <td>212</td>
      <td>212</td>
      <td>212</td>
    </tr>
    <tr>
      <th>Is_month_end</th>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
      <td>True</td>
    </tr>
    <tr>
      <th>Is_month_start</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>Is_quarter_end</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>Is_quarter_start</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>Is_year_end</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>Is_year_start</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>Elapsed</th>
      <td>1438300800</td>
      <td>1438300800</td>
      <td>1438300800</td>
      <td>1438300800</td>
      <td>1438300800</td>
    </tr>
    <tr>
      <th>StoreType</th>
      <td>c</td>
      <td>a</td>
      <td>a</td>
      <td>c</td>
      <td>a</td>
    </tr>
    <tr>
      <th>Assortment</th>
      <td>a</td>
      <td>a</td>
      <td>a</td>
      <td>c</td>
      <td>a</td>
    </tr>
    <tr>
      <th>CompetitionDistance</th>
      <td>1270</td>
      <td>570</td>
      <td>14130</td>
      <td>620</td>
      <td>29910</td>
    </tr>
    <tr>
      <th>CompetitionOpenSinceMonth</th>
      <td>9</td>
      <td>11</td>
      <td>12</td>
      <td>9</td>
      <td>4</td>
    </tr>
    <tr>
      <th>CompetitionOpenSinceYear</th>
      <td>2008</td>
      <td>2007</td>
      <td>2006</td>
      <td>2009</td>
      <td>2015</td>
    </tr>
    <tr>
      <th>Promo2</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Promo2SinceWeek</th>
      <td>1</td>
      <td>13</td>
      <td>14</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>Min_Sea_Level_PressurehPa</th>
      <td>1015</td>
      <td>1017</td>
      <td>1017</td>
      <td>1014</td>
      <td>1016</td>
    </tr>
    <tr>
      <th>Max_VisibilityKm</th>
      <td>31</td>
      <td>10</td>
      <td>31</td>
      <td>10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>Mean_VisibilityKm</th>
      <td>15</td>
      <td>10</td>
      <td>14</td>
      <td>10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>Min_VisibilitykM</th>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>Max_Wind_SpeedKm_h</th>
      <td>24</td>
      <td>14</td>
      <td>14</td>
      <td>23</td>
      <td>14</td>
    </tr>
    <tr>
      <th>Mean_Wind_SpeedKm_h</th>
      <td>11</td>
      <td>11</td>
      <td>5</td>
      <td>16</td>
      <td>11</td>
    </tr>
    <tr>
      <th>Max_Gust_SpeedKm_h</th>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>Precipitationmm</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>CloudCover</th>
      <td>1</td>
      <td>4</td>
      <td>2</td>
      <td>6</td>
      <td>4</td>
    </tr>
    <tr>
      <th>Events</th>
      <td>Fog</td>
      <td>Fog</td>
      <td>Fog</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>WindDirDegrees</th>
      <td>13</td>
      <td>309</td>
      <td>354</td>
      <td>282</td>
      <td>290</td>
    </tr>
    <tr>
      <th>StateName</th>
      <td>Hessen</td>
      <td>Thueringen</td>
      <td>NordrheinWestfalen</td>
      <td>Berlin</td>
      <td>Sachsen</td>
    </tr>
    <tr>
      <th>CompetitionOpenSince</th>
      <td>2008-09-15 00:00:00</td>
      <td>2007-11-15 00:00:00</td>
      <td>2006-12-15 00:00:00</td>
      <td>2009-09-15 00:00:00</td>
      <td>2015-04-15 00:00:00</td>
    </tr>
    <tr>
      <th>CompetitionDaysOpen</th>
      <td>2510</td>
      <td>2815</td>
      <td>3150</td>
      <td>2145</td>
      <td>107</td>
    </tr>
    <tr>
      <th>CompetitionMonthsOpen</th>
      <td>24</td>
      <td>24</td>
      <td>24</td>
      <td>24</td>
      <td>3</td>
    </tr>
    <tr>
      <th>Promo2Since</th>
      <td>1900-01-01 00:00:00</td>
      <td>2010-03-29 00:00:00</td>
      <td>2011-04-04 00:00:00</td>
      <td>1900-01-01 00:00:00</td>
      <td>1900-01-01 00:00:00</td>
    </tr>
    <tr>
      <th>Promo2Days</th>
      <td>0</td>
      <td>1950</td>
      <td>1579</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Promo2Weeks</th>
      <td>0</td>
      <td>25</td>
      <td>25</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>AfterSchoolHoliday</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>BeforeSchoolHoliday</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>AfterStateHoliday</th>
      <td>57</td>
      <td>67</td>
      <td>57</td>
      <td>67</td>
      <td>57</td>
    </tr>
    <tr>
      <th>BeforeStateHoliday</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>AfterPromo</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>BeforePromo</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>SchoolHoliday_bw</th>
      <td>5</td>
      <td>5</td>
      <td>5</td>
      <td>5</td>
      <td>5</td>
    </tr>
    <tr>
      <th>StateHoliday_bw</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Promo_bw</th>
      <td>5</td>
      <td>5</td>
      <td>5</td>
      <td>5</td>
      <td>5</td>
    </tr>
    <tr>
      <th>SchoolHoliday_fw</th>
      <td>7</td>
      <td>1</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>StateHoliday_fw</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Promo_fw</th>
      <td>5</td>
      <td>1</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>93 rows × 5 columns</p>
</div>




```python
n = len(train_df); n
```




    844338



## Preprocessors  - Experimenting with a sample


```python
# permute a sequence (like shuffling) of index, then take the first 2000 and sort em
idx = np.random.permutation(range(n))[:2000]
idx.sort()

small_train_df = train_df.iloc[idx[:1000]]
small_test_df = train_df.iloc[idx[1000:]]

small_cont_vars = ['CompetitionDistance', 'Mean_Humidity']
small_cat_vars = ['Store', 'DayOfWeek', 'PromoInterval']

small_train_df = small_train_df[small_cat_vars + small_cont_vars + ['Sales']]
small_test_df = small_test_df[small_cat_vars + small_cont_vars + ['Sales']]
```


```python
small_train_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Store</th>
      <th>DayOfWeek</th>
      <th>PromoInterval</th>
      <th>CompetitionDistance</th>
      <th>Mean_Humidity</th>
      <th>Sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>16</th>
      <td>17</td>
      <td>5</td>
      <td>Jan,Apr,Jul,Oct</td>
      <td>50.0</td>
      <td>62</td>
      <td>8430</td>
    </tr>
    <tr>
      <th>76</th>
      <td>77</td>
      <td>5</td>
      <td>Jan,Apr,Jul,Oct</td>
      <td>1090.0</td>
      <td>54</td>
      <td>10335</td>
    </tr>
    <tr>
      <th>234</th>
      <td>235</td>
      <td>5</td>
      <td>Jan,Apr,Jul,Oct</td>
      <td>5710.0</td>
      <td>54</td>
      <td>6756</td>
    </tr>
    <tr>
      <th>857</th>
      <td>859</td>
      <td>5</td>
      <td>NaN</td>
      <td>21770.0</td>
      <td>50</td>
      <td>8854</td>
    </tr>
    <tr>
      <th>1042</th>
      <td>1045</td>
      <td>5</td>
      <td>NaN</td>
      <td>26990.0</td>
      <td>61</td>
      <td>12216</td>
    </tr>
  </tbody>
</table>
</div>




```python
small_test_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Store</th>
      <th>DayOfWeek</th>
      <th>PromoInterval</th>
      <th>CompetitionDistance</th>
      <th>Mean_Humidity</th>
      <th>Sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>410285</th>
      <td>661</td>
      <td>6</td>
      <td>NaN</td>
      <td>2140.0</td>
      <td>61</td>
      <td>10044</td>
    </tr>
    <tr>
      <th>411411</th>
      <td>672</td>
      <td>5</td>
      <td>NaN</td>
      <td>240.0</td>
      <td>64</td>
      <td>7428</td>
    </tr>
    <tr>
      <th>411426</th>
      <td>687</td>
      <td>5</td>
      <td>NaN</td>
      <td>2770.0</td>
      <td>71</td>
      <td>8073</td>
    </tr>
    <tr>
      <th>411540</th>
      <td>801</td>
      <td>5</td>
      <td>NaN</td>
      <td>48330.0</td>
      <td>77</td>
      <td>3953</td>
    </tr>
    <tr>
      <th>411610</th>
      <td>871</td>
      <td>5</td>
      <td>NaN</td>
      <td>10620.0</td>
      <td>60</td>
      <td>5868</td>
    </tr>
  </tbody>
</table>
</div>



### Categorify

We can see one of the column is called `PromoInterval`. It has these strings(Jan, Apr, Jul, Oct) and sometimes it’s missing in pandas i.e, NAN.

Categorify takes strings to find all of the possible unique values of it and create a list of them and then it’s going to turn the strings into numbers.


```python
categorify = Categorify(small_cat_vars, small_cont_vars)
categorify(small_train_df)
categorify(small_test_df, test=True)
```


```python
small_train_df.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Store</th>
      <th>DayOfWeek</th>
      <th>PromoInterval</th>
      <th>CompetitionDistance</th>
      <th>Mean_Humidity</th>
      <th>Sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>16</th>
      <td>17</td>
      <td>5</td>
      <td>Jan,Apr,Jul,Oct</td>
      <td>50.0</td>
      <td>62</td>
      <td>8430</td>
    </tr>
    <tr>
      <th>76</th>
      <td>77</td>
      <td>5</td>
      <td>Jan,Apr,Jul,Oct</td>
      <td>1090.0</td>
      <td>54</td>
      <td>10335</td>
    </tr>
    <tr>
      <th>234</th>
      <td>235</td>
      <td>5</td>
      <td>Jan,Apr,Jul,Oct</td>
      <td>5710.0</td>
      <td>54</td>
      <td>6756</td>
    </tr>
    <tr>
      <th>857</th>
      <td>859</td>
      <td>5</td>
      <td>NaN</td>
      <td>21770.0</td>
      <td>50</td>
      <td>8854</td>
    </tr>
    <tr>
      <th>1042</th>
      <td>1045</td>
      <td>5</td>
      <td>NaN</td>
      <td>26990.0</td>
      <td>61</td>
      <td>12216</td>
    </tr>
    <tr>
      <th>1291</th>
      <td>179</td>
      <td>4</td>
      <td>NaN</td>
      <td>480.0</td>
      <td>73</td>
      <td>9668</td>
    </tr>
    <tr>
      <th>1484</th>
      <td>373</td>
      <td>4</td>
      <td>Jan,Apr,Jul,Oct</td>
      <td>11120.0</td>
      <td>60</td>
      <td>5028</td>
    </tr>
    <tr>
      <th>1991</th>
      <td>881</td>
      <td>4</td>
      <td>Feb,May,Aug,Nov</td>
      <td>180.0</td>
      <td>73</td>
      <td>5571</td>
    </tr>
    <tr>
      <th>2356</th>
      <td>131</td>
      <td>3</td>
      <td>NaN</td>
      <td>920.0</td>
      <td>67</td>
      <td>4325</td>
    </tr>
    <tr>
      <th>3082</th>
      <td>858</td>
      <td>3</td>
      <td>Jan,Apr,Jul,Oct</td>
      <td>3370.0</td>
      <td>77</td>
      <td>5528</td>
    </tr>
  </tbody>
</table>
</div>



It looks as if nothing's changed, because pandas have turned this into a categorical variable which internally is storing numbers but externally is showing us the strings. But we can look inside `PromoInterval` to look at the classes with `.cat.categories`


```python
small_train_df.PromoInterval.cat.categories
```




    Index(['Feb,May,Aug,Nov', 'Jan,Apr,Jul,Oct', 'Mar,Jun,Sept,Dec'], dtype='object')



we can also look at the internally stored numbers with `.cat.codes`, their code is the same number as their index in `cat.categories`, if the cell has a value of `NaN`, then the code would be `-1`



```python
small_train_df['PromoInterval'].cat.codes[:10]
```




    16      1
    76      1
    234     1
    857    -1
    1042   -1
    1291   -1
    1484    1
    1991    0
    2356   -1
    3082    1
    dtype: int8



### FillMissing

FillMissing is a preprocessor that adds a column of boolean value whether


```python
fill_missing = FillMissing(small_cat_vars, small_cont_vars)
fill_missing(small_train_df)
fill_missing(small_test_df, test=True)
```


```python
train_df['PromoInterval'].isnull().sum()
```




    423292




```python
small_train_df[small_train_df['CompetitionDistance_na'] == True]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Store</th>
      <th>DayOfWeek</th>
      <th>PromoInterval</th>
      <th>CompetitionDistance</th>
      <th>Mean_Humidity</th>
      <th>Sales</th>
      <th>CompetitionDistance_na</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>40045</th>
      <td>879</td>
      <td>6</td>
      <td>Feb,May,Aug,Nov</td>
      <td>2460.0</td>
      <td>76</td>
      <td>3745</td>
      <td>True</td>
    </tr>
    <tr>
      <th>116534</th>
      <td>622</td>
      <td>4</td>
      <td>NaN</td>
      <td>2460.0</td>
      <td>73</td>
      <td>3855</td>
      <td>True</td>
    </tr>
    <tr>
      <th>137432</th>
      <td>291</td>
      <td>3</td>
      <td>NaN</td>
      <td>2460.0</td>
      <td>71</td>
      <td>8491</td>
      <td>True</td>
    </tr>
    <tr>
      <th>183188</th>
      <td>291</td>
      <td>4</td>
      <td>NaN</td>
      <td>2460.0</td>
      <td>68</td>
      <td>7940</td>
      <td>True</td>
    </tr>
    <tr>
      <th>226335</th>
      <td>291</td>
      <td>5</td>
      <td>NaN</td>
      <td>2460.0</td>
      <td>89</td>
      <td>7956</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
</div>



The missing values in the categorical variables are left untouched (their missing value will be replaced by code 0 in the `TabularDataBunch`.

On another hand, the missing values in the continuous variables will be replaced by its median value by default or changeable with `fill_strategy` args. There will also be an additional column of boolean specifying if the row was missing or not with a suffix `_na`. In our `small_train_df` sample, the new column is `CompetitionDistance_na`.

Why do we need a column of boolean reminding us about the missing value? It turns out that the fact that the data is missing is of itself interesting which helps with predicting the outcome, that's why we want to keep that information in a column of boolean.

## Preparing full data set


```python
train_df = pd.read_pickle(path/'train_clean')
test_df = pd.read_pickle(path/'test_clean')
```


```python
len(train_df),len(test_df)
```




    (844338, 41088)



With fast.ai, we most lilkely do not call the preprocessors manually, instead we would define a bunch of Transforms that will be applied to our variables, here we transform all categorical variables into categories. We also replace missing values for continuous variables by the median column value and normalize those.


```python
procs=[FillMissing, Categorify, Normalize]
```

When we are creating a `TabularDataBunch`, it's important to list out the categorical and continous variables that we will put in the model


```python
cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',
    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',
    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',
    'SchoolHoliday_fw', 'SchoolHoliday_bw']

cont_vars = ['CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',
   'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h',
   'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',
   'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday']
```

Notice that `Day` is included as categorical variable, because we are expecting a different purchasing behaviour in payday for instance (e.g. 15th of the month, `Day = 15`), sometimes it's good to think about our variables and decide if we should put it as a categorical, as long as the cardinality (no. of categories) isn't too high and that it makes intuitive sense to include them as so.

Then we proceed to defining the dataframe with our desired columns for the model.


```python
dep_var = 'Sales'
df = train_df[cat_vars + cont_vars + [dep_var,'Date']].copy()
```

### Validation Set

Let's recap real quick..


```python
test_df['Date'].min(), test_df['Date'].max()
```




    (Timestamp('2015-08-01 00:00:00'), Timestamp('2015-09-17 00:00:00'))



Our test set covers all that information we have from `2015-08-01` to `2015-09-17` in order to predict the sales value.


```python
train_df['Date'].min(), train_df['Date'].max()
```




    (Timestamp('2013-01-01 00:00:00'), Timestamp('2015-07-31 00:00:00'))



Our train set covers sales and all the other information from `2013-01-01` to `2015-07-31`.


```python
len(test_df)
```




    41088



We have exactly 41,088 rows of information about the sales in our test set, and we want to extract a validation set which will be the same number of records at the end of the time period that the test is for Kaggle.

Since, `train_df` is arranged in a descending order for date (meaning end of the time period comes on top of the df), we can just grab the first row, all the way to 41088th row. On 41088th row of `train_df`, the `date` is `2015-06-19`, if we take exactly 41088 rows, we will only be taking a subset of the records in `2015-06-19`, which is why we want to instead get the max index for that date to extract the records we want to be in the validation set.


```python
# cut-off index for validation set
cut = train_df['Date'][(train_df['Date'] == train_df['Date'][len(test_df)])].index.max()
cut
```




    41395




```python
# these are the indices for validation set
valid_idx = range(cut)
```


```python
df[dep_var].head()
```




    0     5263
    1     6064
    2     8314
    3    13995
    4     4822
    Name: Sales, dtype: int64



### Creating `TabularDataBunch` from a `TabularList`

This is where we create an [ItemList](https://docs.fast.ai/data_block.html#ItemList) for tabular data with standard data block API, to read more about this, click [here](https://docs.fast.ai/tabular.data.html#TabularDataBunch).


```python
df.shape
```




    (844338, 40)




```python
data = (TabularList.from_df(df, path=path, cat_names=cat_vars, cont_names=cont_vars, procs=procs)
                .split_by_idx(valid_idx) # split these to validation set
                .label_from_df(cols=dep_var, label_cls=FloatList, log=True) # label with dependant variable
                .add_test(TabularList.from_df(test_df, path=path, cat_names=cat_vars, cont_names=cont_vars))
                .databunch())
```

Let's breakdown some not so straightforward code in the labelling functions from data block APIs.

```
label_cls = FloatList
```

The datatype for our dependant variable 'Sales' is not a float, but an `int64`, if this was a float, fast.ai would assume us to do a regression, but since this is an `int64`, fast.ai would assume that we want to do a classification.

That is why when we create a `TabularList`, we want to specify to treat it as float by passing the args `label_cls = FloatList`, which then turn this to a regression task rather a classification one.

```
log = True
```

All of the labeling functions in the data block APIs will pass on any keywords they don't recognize to the label class `label_cls`, in this case it will pass on `log = True`, which means we will take the logarithm of `y`, we do this because evaluation metrics for this dataset is *Root Mean Squared Percentage Error (RMSPE)*, if we take the logarithm of `y` in *RMSPE*, that essentially becomes *Root Mean Squared Error (RMSE)* which is the default evaluation metrics for regression task.

The reason why we pass on this argument here is because often in regression task, we tend to predict something that has a long-tailed distribution, which means percentage differences tells more than absolute differences, so when that is the case, we would pass in `log = True` to measure the *RMSPE*.


```python
data
```




    TabularDataBunch;

    Train: LabelList (802943 items)
    x: TabularList
    Store 1115; DayOfWeek 5; Year 2015; Month 6; Day 19; StateHoliday False; CompetitionMonthsOpen 0; Promo2Weeks 25; StoreType d; Assortment c; PromoInterval Mar,Jun,Sept,Dec; CompetitionOpenSinceYear 1900; Promo2SinceYear 2012; State HE; Week 25; Events Rain; Promo_fw 1.0; Promo_bw 5.0; StateHoliday_fw 0.0; StateHoliday_bw 0.0; SchoolHoliday_fw 0.0; SchoolHoliday_bw 0.0; CompetitionDistance_na False; CloudCover_na False; CompetitionDistance -0.0130; Max_TemperatureC 0.1333; Mean_TemperatureC 0.4683; Min_TemperatureC 0.8602; Max_Humidity 0.0780; Mean_Humidity -0.3412; Min_Humidity -0.0811; Max_Wind_SpeedKm_h -0.1780; Mean_Wind_SpeedKm_h 0.1902; CloudCover 0.2552; trend 0.4066; trend_DE 0.5969; AfterStateHoliday -0.7329; BeforeStateHoliday 1.1848; Promo 1.1119; SchoolHoliday -0.4796; ,Store 1; DayOfWeek 4; Year 2015; Month 6; Day 18; StateHoliday False; CompetitionMonthsOpen 24; Promo2Weeks 0; StoreType c; Assortment a; PromoInterval #na#; CompetitionOpenSinceYear 2008; Promo2SinceYear 1900; State HE; Week 25; Events Rain; Promo_fw 2.0; Promo_bw 4.0; StateHoliday_fw 0.0; StateHoliday_bw 0.0; SchoolHoliday_fw 0.0; SchoolHoliday_bw 0.0; CompetitionDistance_na False; CloudCover_na False; CompetitionDistance -0.5358; Max_TemperatureC 0.8549; Mean_TemperatureC 1.0322; Min_TemperatureC 1.1756; Max_Humidity 0.8653; Mean_Humidity -0.6479; Min_Humidity -1.0487; Max_Wind_SpeedKm_h 0.3768; Mean_Wind_SpeedKm_h 0.1902; CloudCover 0.2552; trend 0.4066; trend_DE 0.5969; AfterStateHoliday -0.7645; BeforeStateHoliday 1.1848; Promo 1.1119; SchoolHoliday -0.4796; ,Store 2; DayOfWeek 4; Year 2015; Month 6; Day 18; StateHoliday False; CompetitionMonthsOpen 24; Promo2Weeks 25; StoreType a; Assortment a; PromoInterval Jan,Apr,Jul,Oct; CompetitionOpenSinceYear 2007; Promo2SinceYear 2010; State TH; Week 25; Events Rain; Promo_fw 2.0; Promo_bw 4.0; StateHoliday_fw 0.0; StateHoliday_bw 0.0; SchoolHoliday_fw 0.0; SchoolHoliday_bw 0.0; CompetitionDistance_na False; CloudCover_na False; CompetitionDistance -0.6255; Max_TemperatureC 0.6144; Mean_TemperatureC 0.8912; Min_TemperatureC 1.0179; Max_Humidity 0.8653; Mean_Humidity -0.1112; Min_Humidity -0.6413; Max_Wind_SpeedKm_h 0.4878; Mean_Wind_SpeedKm_h 0.3579; CloudCover 0.2552; trend -0.4997; trend_DE 0.5969; AfterStateHoliday -0.4486; BeforeStateHoliday 1.1848; Promo 1.1119; SchoolHoliday -0.4796; ,Store 3; DayOfWeek 4; Year 2015; Month 6; Day 18; StateHoliday False; CompetitionMonthsOpen 24; Promo2Weeks 25; StoreType a; Assortment a; PromoInterval Jan,Apr,Jul,Oct; CompetitionOpenSinceYear 2006; Promo2SinceYear 2011; State NW; Week 25; Events Rain; Promo_fw 2.0; Promo_bw 4.0; StateHoliday_fw 0.0; StateHoliday_bw 0.0; SchoolHoliday_fw 0.0; SchoolHoliday_bw 0.0; CompetitionDistance_na False; CloudCover_na False; CompetitionDistance 1.1122; Max_TemperatureC 0.7346; Mean_TemperatureC 1.0322; Min_TemperatureC 1.1756; Max_Humidity 0.8653; Mean_Humidity -0.4945; Min_Humidity -0.8959; Max_Wind_SpeedKm_h 0.7097; Mean_Wind_SpeedKm_h 0.3579; CloudCover 0.2552; trend 1.0410; trend_DE 0.5969; AfterStateHoliday -0.7645; BeforeStateHoliday 1.1848; Promo 1.1119; SchoolHoliday -0.4796; ,Store 4; DayOfWeek 4; Year 2015; Month 6; Day 18; StateHoliday False; CompetitionMonthsOpen 24; Promo2Weeks 0; StoreType c; Assortment c; PromoInterval #na#; CompetitionOpenSinceYear 2009; Promo2SinceYear 1900; State BE; Week 25; Events Rain; Promo_fw 2.0; Promo_bw 4.0; StateHoliday_fw 0.0; StateHoliday_bw 0.0; SchoolHoliday_fw 0.0; SchoolHoliday_bw 0.0; CompetitionDistance_na False; CloudCover_na False; CompetitionDistance -0.6191; Max_TemperatureC 0.8549; Mean_TemperatureC 1.0322; Min_TemperatureC 1.0179; Max_Humidity 0.8653; Mean_Humidity -0.4179; Min_Humidity -0.8450; Max_Wind_SpeedKm_h 0.1549; Mean_Wind_SpeedKm_h 0.3579; CloudCover 0.2552; trend -0.1372; trend_DE 0.5969; AfterStateHoliday -0.4486; BeforeStateHoliday 1.1848; Promo 1.1119; SchoolHoliday -0.4796;
    y: FloatList
    9.022926,8.443546,8.547528,8.92758,9.091557
    Path: /content/data/rossmann;

    Valid: LabelList (41395 items)
    x: TabularList
    Store 1; DayOfWeek 5; Year 2015; Month 7; Day 31; StateHoliday False; CompetitionMonthsOpen 24; Promo2Weeks 0; StoreType c; Assortment a; PromoInterval #na#; CompetitionOpenSinceYear 2008; Promo2SinceYear 1900; State HE; Week 31; Events Fog; Promo_fw 5.0; Promo_bw 5.0; StateHoliday_fw 0.0; StateHoliday_bw 0.0; SchoolHoliday_fw 7.0; SchoolHoliday_bw 5.0; CompetitionDistance_na False; CloudCover_na False; CompetitionDistance -0.5358; Max_TemperatureC 1.0954; Mean_TemperatureC 0.8912; Min_TemperatureC 0.3871; Max_Humidity 0.6029; Mean_Humidity -1.5678; Min_Humidity -1.6598; Max_Wind_SpeedKm_h 0.1549; Mean_Wind_SpeedKm_h -0.1452; CloudCover -2.8229; trend 1.8567; trend_DE 1.8939; AfterStateHoliday 0.5940; BeforeStateHoliday 1.1848; Promo 1.1119; SchoolHoliday 2.0852; ,Store 2; DayOfWeek 5; Year 2015; Month 7; Day 31; StateHoliday False; CompetitionMonthsOpen 24; Promo2Weeks 25; StoreType a; Assortment a; PromoInterval Jan,Apr,Jul,Oct; CompetitionOpenSinceYear 2007; Promo2SinceYear 2010; State TH; Week 31; Events Fog; Promo_fw 1.0; Promo_bw 5.0; StateHoliday_fw 0.0; StateHoliday_bw 0.0; SchoolHoliday_fw 1.0; SchoolHoliday_bw 5.0; CompetitionDistance_na False; CloudCover_na False; CompetitionDistance -0.6255; Max_TemperatureC 0.6144; Mean_TemperatureC 0.4683; Min_TemperatureC 0.2293; Max_Humidity 0.8653; Mean_Humidity -0.9545; Min_Humidity -1.3033; Max_Wind_SpeedKm_h -0.9548; Mean_Wind_SpeedKm_h -0.1452; CloudCover -0.9761; trend 1.4035; trend_DE 1.8939; AfterStateHoliday 0.9099; BeforeStateHoliday 1.1848; Promo 1.1119; SchoolHoliday 2.0852; ,Store 3; DayOfWeek 5; Year 2015; Month 7; Day 31; StateHoliday False; CompetitionMonthsOpen 24; Promo2Weeks 25; StoreType a; Assortment a; PromoInterval Jan,Apr,Jul,Oct; CompetitionOpenSinceYear 2006; Promo2SinceYear 2011; State NW; Week 31; Events Fog; Promo_fw 5.0; Promo_bw 5.0; StateHoliday_fw 0.0; StateHoliday_bw 0.0; SchoolHoliday_fw 5.0; SchoolHoliday_bw 5.0; CompetitionDistance_na False; CloudCover_na False; CompetitionDistance 1.1122; Max_TemperatureC 0.8549; Mean_TemperatureC 0.4683; Min_TemperatureC 0.0716; Max_Humidity 0.8653; Mean_Humidity -1.0312; Min_Humidity -1.3542; Max_Wind_SpeedKm_h -0.9548; Mean_Wind_SpeedKm_h -1.1514; CloudCover -2.2073; trend 1.9473; trend_DE 1.8939; AfterStateHoliday 0.5940; BeforeStateHoliday 1.1848; Promo 1.1119; SchoolHoliday 2.0852; ,Store 4; DayOfWeek 5; Year 2015; Month 7; Day 31; StateHoliday False; CompetitionMonthsOpen 24; Promo2Weeks 0; StoreType c; Assortment c; PromoInterval #na#; CompetitionOpenSinceYear 2009; Promo2SinceYear 1900; State BE; Week 31; Events #na#; Promo_fw 1.0; Promo_bw 5.0; StateHoliday_fw 0.0; StateHoliday_bw 0.0; SchoolHoliday_fw 1.0; SchoolHoliday_bw 5.0; CompetitionDistance_na False; CloudCover_na False; CompetitionDistance -0.6191; Max_TemperatureC 0.6144; Mean_TemperatureC 0.6092; Min_TemperatureC 0.5448; Max_Humidity 0.0780; Mean_Humidity -1.0312; Min_Humidity -1.0487; Max_Wind_SpeedKm_h 0.0439; Mean_Wind_SpeedKm_h 0.6933; CloudCover 0.2552; trend 0.8597; trend_DE 1.8939; AfterStateHoliday 0.9099; BeforeStateHoliday 1.1848; Promo 1.1119; SchoolHoliday 2.0852; ,Store 5; DayOfWeek 5; Year 2015; Month 7; Day 31; StateHoliday False; CompetitionMonthsOpen 3; Promo2Weeks 0; StoreType a; Assortment a; PromoInterval #na#; CompetitionOpenSinceYear 2015; Promo2SinceYear 1900; State SN; Week 31; Events #na#; Promo_fw 1.0; Promo_bw 5.0; StateHoliday_fw 0.0; StateHoliday_bw 0.0; SchoolHoliday_fw 1.0; SchoolHoliday_bw 5.0; CompetitionDistance_na False; CloudCover_na False; CompetitionDistance 3.1344; Max_TemperatureC 0.7346; Mean_TemperatureC 0.7502; Min_TemperatureC 0.7025; Max_Humidity -1.4966; Mean_Humidity -1.4912; Min_Humidity -1.2524; Max_Wind_SpeedKm_h -0.9548; Mean_Wind_SpeedKm_h -0.1452; CloudCover -0.9761; trend 1.5848; trend_DE 1.8939; AfterStateHoliday 0.5940; BeforeStateHoliday 1.1848; Promo 1.1119; SchoolHoliday 2.0852;
    y: FloatList
    8.568457,8.710125,9.025696,9.546455,8.480944
    Path: /content/data/rossmann;

    Test: LabelList (41088 items)
    x: TabularList
    Store 1; DayOfWeek 4; Year 2015; Month 9; Day 17; StateHoliday False; CompetitionMonthsOpen 24; Promo2Weeks 0; StoreType c; Assortment a; PromoInterval #na#; CompetitionOpenSinceYear 2008; Promo2SinceYear 1900; State HE; Week 38; Events Rain; Promo_fw 1.0; Promo_bw 4.0; StateHoliday_fw 0.0; StateHoliday_bw 0.0; SchoolHoliday_fw 0.0; SchoolHoliday_bw 0.0; CompetitionDistance_na False; CloudCover_na False; CompetitionDistance -0.5358; Max_TemperatureC 0.2536; Mean_TemperatureC 0.6092; Min_TemperatureC 1.1756; Max_Humidity 0.8653; Mean_Humidity 1.5754; Min_Humidity 1.9049; Max_Wind_SpeedKm_h 0.7097; Mean_Wind_SpeedKm_h 0.3579; CloudCover 0.2552; trend 0.4066; trend_DE 0.1645; AfterStateHoliday 2.1104; BeforeStateHoliday 1.1848; Promo 1.1119; SchoolHoliday -0.4796; ,Store 3; DayOfWeek 4; Year 2015; Month 9; Day 17; StateHoliday False; CompetitionMonthsOpen 24; Promo2Weeks 25; StoreType a; Assortment a; PromoInterval Jan,Apr,Jul,Oct; CompetitionOpenSinceYear 2006; Promo2SinceYear 2011; State NW; Week 38; Events Rain; Promo_fw 1.0; Promo_bw 4.0; StateHoliday_fw 0.0; StateHoliday_bw 0.0; SchoolHoliday_fw 0.0; SchoolHoliday_bw 0.0; CompetitionDistance_na False; CloudCover_na False; CompetitionDistance 1.1122; Max_TemperatureC 0.3738; Mean_TemperatureC 0.6092; Min_TemperatureC 1.0179; Max_Humidity 0.8653; Mean_Humidity 0.2721; Min_Humidity 0.3772; Max_Wind_SpeedKm_h 1.5975; Mean_Wind_SpeedKm_h 2.3704; CloudCover 0.2552; trend 0.3159; trend_DE 0.1645; AfterStateHoliday 2.1104; BeforeStateHoliday 1.1848; Promo 1.1119; SchoolHoliday -0.4796; ,Store 7; DayOfWeek 4; Year 2015; Month 9; Day 17; StateHoliday False; CompetitionMonthsOpen 24; Promo2Weeks 0; StoreType a; Assortment c; PromoInterval #na#; CompetitionOpenSinceYear 2013; Promo2SinceYear 1900; State SH; Week 38; Events Rain; Promo_fw 1.0; Promo_bw 4.0; StateHoliday_fw 0.0; StateHoliday_bw 0.0; SchoolHoliday_fw 0.0; SchoolHoliday_bw 0.0; CompetitionDistance_na False; CloudCover_na False; CompetitionDistance 2.3770; Max_TemperatureC 0.4941; Mean_TemperatureC 0.8912; Min_TemperatureC 1.1756; Max_Humidity 0.3404; Mean_Humidity -0.1112; Min_Humidity 0.5300; Max_Wind_SpeedKm_h 0.7097; Mean_Wind_SpeedKm_h 0.3579; CloudCover -0.3604; trend -0.4997; trend_DE 0.1645; AfterStateHoliday 2.4263; BeforeStateHoliday 1.1848; Promo 1.1119; SchoolHoliday -0.4796; ,Store 8; DayOfWeek 4; Year 2015; Month 9; Day 17; StateHoliday False; CompetitionMonthsOpen 11; Promo2Weeks 0; StoreType a; Assortment a; PromoInterval #na#; CompetitionOpenSinceYear 2014; Promo2SinceYear 1900; State SH; Week 38; Events Rain; Promo_fw 1.0; Promo_bw 4.0; StateHoliday_fw 0.0; StateHoliday_bw 0.0; SchoolHoliday_fw 0.0; SchoolHoliday_bw 0.0; CompetitionDistance_na False; CloudCover_na False; CompetitionDistance 0.2651; Max_TemperatureC 0.4941; Mean_TemperatureC 0.8912; Min_TemperatureC 1.1756; Max_Humidity 0.3404; Mean_Humidity -0.1112; Min_Humidity 0.5300; Max_Wind_SpeedKm_h 0.7097; Mean_Wind_SpeedKm_h 0.3579; CloudCover -0.3604; trend -0.4997; trend_DE 0.1645; AfterStateHoliday 2.4263; BeforeStateHoliday 1.1848; Promo 1.1119; SchoolHoliday -0.4796; ,Store 9; DayOfWeek 4; Year 2015; Month 9; Day 17; StateHoliday False; CompetitionMonthsOpen 24; Promo2Weeks 0; StoreType a; Assortment c; PromoInterval #na#; CompetitionOpenSinceYear 2000; Promo2SinceYear 1900; State NW; Week 38; Events Rain; Promo_fw 1.0; Promo_bw 4.0; StateHoliday_fw 0.0; StateHoliday_bw 0.0; SchoolHoliday_fw 0.0; SchoolHoliday_bw 0.0; CompetitionDistance_na False; CloudCover_na False; CompetitionDistance -0.4384; Max_TemperatureC 0.3738; Mean_TemperatureC 0.6092; Min_TemperatureC 1.0179; Max_Humidity 0.8653; Mean_Humidity 0.2721; Min_Humidity 0.3772; Max_Wind_SpeedKm_h 1.5975; Mean_Wind_SpeedKm_h 2.3704; CloudCover 0.2552; trend 0.3159; trend_DE 0.1645; AfterStateHoliday 2.1104; BeforeStateHoliday 1.1848; Promo 1.1119; SchoolHoliday -0.4796;
    y: EmptyLabelList
    ,,,,
    Path: /content/data/rossmann



Everything is looking good so far, let's build the learner and start training!

## Model

The idea of `y_range` was mentioned in [part 2 of collaborative filtering blogpost](https://cheeloong.github.io/collabnn/#), basically we want to make sure the prediction range is kept within the ground truth.

Because we are taking log of `y`, we need to make sure the `y_range` is also taking the logarithm, which is why the `y_range` code is as follows:


```python
max_log_y = np.log(np.max(train_df['Sales'])*1.2)
y_range = torch.tensor([0, max_log_y], device=defaults.device)
```


```python
# time for tabular_learner
learn = tabular_learner(data, layers=[1000,500], ps=[0.001,0.01], emb_drop=0.04,
                        y_range=y_range, metrics=exp_rmspe)
```

We pass in `TabularDataBunch` into our `tabular_learner`, and then we pass in the architecture in the `layers`, the architecture for tabular data that we are using is literally the most basic fully connected network like the one we used in this [colloborative filtering blogpost](https://cheeloong.github.io/collabnn/#).

`layers = [1000,500]` means that our intermediate weight matrix is going to take in 1000 input activations and spit outs 500 output activations, which means 500,000 elements in that weight matrix, that is a lot for a dataset with a few hundred thousand rows, this is going to overfit, and that is why we need to apply regularization. One way is to use weight decay, like we learned in the previous blogpost, but in this case we want to regularize more, and we will use something called drop out regularization.


### Dropout Regularization

Let's reference to this [paper](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf).

<img src="https://i.imgur.com/LaHwYa2.png" width="800"  align="middle">

The diagram on the left is normal neural network and the diagram on the right is the same network after applying dropout. Every arrow shows multiplications between weights and activations. A circle represents sum.

For each mini batch, we throw away a different subset of activations (**not the parameters!**), we throw them away with a dropout probability of P, a common value would be 0.5.

Another important thing is that during training time, we apply dropout, but not test time, this means that if our P is 0.5, we drop about 50% of the activations at training time, and at test time we use all the activations (which means activation level is about twice as higher at test time), that is why the paper suggests to multiply the weights by whatever the P is at test time, as shown in the diagram below.

<img src="https://i.imgur.com/gglvCBr.png" width="800"  align="middle">

However, in PyTorch and most libraries, the non-zeros activations are rescaled at train time with $$ \frac{1} {1 - P} $$

How is this similar with the approach suggested by the paper?

I am so not good with explaining this, but I find [this discussion](https://forums.fast.ai/t/code-deep-dive-understanding-the-application-of-dropout/8515) helpful.

```
ps = [0.001, 0.01]
```

`ps` is the p-values for the dropout of each layer, so we can just pass in a list like above. In this case we are passing `0.001` dropout p-value on the first layer, and `0.01` on the second layer.


```
emb_drop=0.04
```

 We are using special dropout in the embedding layer, why? If we look into fastai source code line 31-33 for the `TabularModel`


<img src="https://i.imgur.com/UDrMHR8.png" width="800">

We call each embedding, concatenate it into a single matrix, then call `emb_drop`, which is just an instance of a `nn.Dropout` module in line 14, this drops the activation layer of embeddings.



```python
learn.model
```




    TabularModel(
      (embeds): ModuleList(
        (0): Embedding(1116, 81)
        (1): Embedding(8, 5)
        (2): Embedding(4, 3)
        (3): Embedding(13, 7)
        (4): Embedding(32, 11)
        (5): Embedding(3, 3)
        (6): Embedding(26, 10)
        (7): Embedding(27, 10)
        (8): Embedding(5, 4)
        (9): Embedding(4, 3)
        (10): Embedding(4, 3)
        (11): Embedding(24, 9)
        (12): Embedding(9, 5)
        (13): Embedding(13, 7)
        (14): Embedding(53, 15)
        (15): Embedding(22, 9)
        (16): Embedding(7, 5)
        (17): Embedding(7, 5)
        (18): Embedding(4, 3)
        (19): Embedding(4, 3)
        (20): Embedding(9, 5)
        (21): Embedding(9, 5)
        (22): Embedding(3, 3)
        (23): Embedding(3, 3)
      )
      (emb_drop): Dropout(p=0.04)
      (bn_cont): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (layers): Sequential(
        (0): Linear(in_features=233, out_features=1000, bias=True)
        (1): ReLU(inplace)
        (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): Dropout(p=0.001)
        (4): Linear(in_features=1000, out_features=500, bias=True)
        (5): ReLU(inplace)
        (6): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (7): Dropout(p=0.01)
        (8): Linear(in_features=500, out_features=1, bias=True)
      )
    )



Notice that in our model there is something call `BatchNorm1d`, and its applying to `16` variables, which is the total number of continuous variables we have in our model. If we look at the `TabularModel` source code from above, if the variable is continuous, we would apply batch normalization.


### Batch Normalization

<img src="https://i.imgur.com/DVnpMv8.png" width="400">

Here we have a layer which is taking in activations over a mini-batch, it then calculate mean, variance, and normalize it, basic math stuffs going on, and the most important bit of this layer, is that it take the normalized activations and multiplying by a $\gamma$ parameter and adding a $\beta$ parameter to scale and shift.

The beta $\beta$ is a vector of biases (normal bias layer), and the $\gamma$ is also a vector of biases (multiplicative bias layer).

**Here's why this works:**

Supposed we are building a neural net to predict the outcome of a movie review which range from 1 to 5, but our activations at the last layer range from -1 to 1, which means that the model is way off in terms of scale and mean, we can train a new set of weights that tries to increase the scale and mean, potentially with the help of momentum, rmsprop, adam, etc, it's going to be very hard to get there, it's going to take a long time, or maybe we won't even get there.

BUT, by introducing the two new parameters $\beta$ and $\gamma$, this is because \$gamma$\ have direct gradient influence on the scale, and the $\beta$ has direct gradient influence on the mean, so Batch Normalization helps the activation output in terms of scaling in and out, or shifting up and down.


**Momentum in this context:**

In our model, we can see that `momentum = 0.01`, that is not the same momentum which we discussed in the gradient descent optimizaton blogpost, but instead, it's an exponentially weighted moving average. In our batch normalization, fastai don't actually use different mean and variance for different batch, but instead they use exponentially weighted moving average of the mean and variance, which we can learn more about if we actually read the [previous blogpost](https://cheeloong.github.io/sgd-mnist/#).

A small number in momentum in this context means that the variation in the mean is smaller from mini-batch to mini-batch, and vice versa.


```python
# total number of continuous variables in our model
len(data.train_ds.cont_names)
```




    16



Then, we start doing the usual stuff, visualize learning rate, fit model, save, plot losses, fit again...yada yada.


```python
learn.lr_find()
```





    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.



```python
learn.recorder.plot()
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/rossmann/output_74_0.png" alt="">



```python
learn.fit_one_cycle(5, 1e-3, wd=0.2)
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>exp_rmspe</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.022751</td>
      <td>0.020424</td>
      <td>0.132390</td>
      <td>02:39</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.019870</td>
      <td>0.088583</td>
      <td>0.147926</td>
      <td>02:39</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.015825</td>
      <td>0.016004</td>
      <td>0.125746</td>
      <td>02:38</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.011220</td>
      <td>0.013211</td>
      <td>0.109654</td>
      <td>02:41</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.010657</td>
      <td>0.011011</td>
      <td>0.102247</td>
      <td>02:41</td>
    </tr>
  </tbody>
</table>

```python
learn.save('1')
```


```python
learn.recorder.plot_losses()
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/rossmann/output_78_0.png" width="400" alt="">


Let's try to do late submission on Kaggle and see how well we do, first download our model by writing the model to csv, save it and upload it to [Rossmann Kaggle Competition](https://www.kaggle.com/c/rossmann-store-sales/overview).


```python
test_preds=learn.get_preds(DatasetType.Test)
test_df["Sales"]=np.exp(test_preds[0].data).numpy().T[0]
test_df[["Id","Sales"]]=test_df[["Id","Sales"]].astype("int")
test_df[["Id","Sales"]].to_csv("rossmann_submission.csv",index=False)
```

<img src="https://i.imgur.com/hUYwmX2.png" width="800">

Since this is a late submission, our profile won't actually show on the leaderboard, but roughly we will be placed on 888th.


<img src="https://i.imgur.com/heBtDC8.png" width="800">


Private score of 0.12039, not the best huh (I probably should've trained a few more times, may get a better score, but I've been in this cafe for over 7 hours, I should ciao) thats like top 27% (888th from 3303 teams), definitely there's room for improvement, but for now let's call it a day. Thanks for reading, and as usual, huge thank you to the folks at [fast.ai](https://www.fast.ai/).

---
title: "Multi-label Classification of Satellite Images"
date: 2019-03-18
permalink: /planets/
tags: [fastai, deep learning, multi-label classification, satellite image]
excerpt: "Multi-label classification using resnet50"
mathjax: "true"
---

# Multi-label prediction with Planet Amazon dataset

## Understanding the data

The challenge is to  label satellite image chips with atmospheric conditions and various classes of land cover/land use. Resulting algorithms will help the global community better understand where, how, and why deforestation happens all over the world - and ultimately how to respond.

The class labels for this task were chosen in collaboration with Planet's Impact team and represent a reasonable subset of phenomena of interest in the Amazon basin. The labels can broadly be broken into three groups:
- atmospheric conditions,
- common land cover/land use phenomena
- rare land cover/land use phenomena.

<img src="{{ site.url }}{{ site.baseurl }}/assets/images/planets/output_01_0.png" alt="">

Each chip will have one and potentially more than one atmospheric label and zero or more common and rare labels. Chips that are labeled as cloudy should have no other labels, but there may be labelling errors.

Click [here](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data) to learn more about the dataset.

## Colab Cloud Server VM setup & FastAI Configurations

First we need to use GPU for deep learning applications, so instead of buying, we can rent it from cloud servers. There are many other server options, but basically I chose Colab because issa FREE.

To learn how to set up a Colab Server which supports fastai library and its applications, click here.

NB: This is a free service that may not always be available, and requires extra steps to ensure your work is saved. Be sure to read the docs on the Colab web-site to ensure you understand the limitations of the system.


```python
# Permit collaboratory instance to read/write to Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
root_dir = "/content/gdrive/My Drive/"
base_dir = root_dir + 'fastai-v3/'
```

    Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code

    Enter your authorization code:
    Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·
    Mounted at /content/gdrive



```python
  !curl -s https://course.fast.ai/setup/colab | bash
```

    Updating fastai...
    [31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.[0m
    [31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.[0m
    Done.



```python
%reload_ext autoreload
%autoreload 2
%matplotlib inline
```


```python
from fastai.vision import *
```

## Getting the data

To install stuff from Kaggle you have to download Kaggle API tool.
In Jupyter notebooks you will find commented lines, you can uncomment and run them

Let's download the planet dataset from Kaggle by using the [Kaggle API](https://github.com/Kaggle/kaggle-api) as it's going to be pretty useful if we want to join a competition or use other Kaggle datasets later on.

First, install the Kaggle API by uncommenting the following line and executing it, or by executing it in your terminal (depending on your platform you may need to modify this slightly to either add `source activate fastai` or similar, or prefix `pip` with a path. Have a look at how `conda install` is called for your platform in the appropriate *Returning to work* section of https://course.fast.ai/. (Depending on your environment, you may also need to append "--user" to the command.)


```python
# Install Kaggle API
! pip3 install kaggle --upgrade
```

    Requirement already up-to-date: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)
    Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)
    Requirement already satisfied, skipping upgrade: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)
    Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.3.9)
    Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)
    Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)
    Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)
    Requirement already satisfied, skipping upgrade: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.0)
    Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)
    Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)
    Requirement already satisfied, skipping upgrade: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)


Then you need to upload your credentials from Kaggle on your instance. Login to kaggle and click on your profile picture on the top left corner, then 'My account'. Scroll down until you find a button named 'Create New API Token' and click on it. This will trigger the download of a file named 'kaggle.json'.

Upload this file to the directory this notebook is running in, by clicking "Upload" on your main Jupyter page, then uncomment and execute the next two commands (or run them in a terminal). For Windows, uncomment the last two commands.


```python
# ! mkdir -p ~/.kaggle/
# ! mv kaggle.json ~/.kaggle/

# For Windows, uncomment these two commands
# ! mkdir %userprofile%\.kaggle
# ! move kaggle.json %userprofile%\.kaggle
```

You're all set to download the data from [planet competition](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space). You **first need to go to its main page and accept its rules**, and run the two cells below (uncomment the shell commands to download and unzip the data). If you get a `403 forbidden` error it means you haven't accepted the competition rules yet (you have to go to the competition page, click on *Rules* tab, and then scroll to the bottom to find the *accept* button).


```python
# create the data directory
path = Config.data_path()/'planet'
path.mkdir(parents=True, exist_ok=True)
path
```




    PosixPath('/root/.fastai/data/planet')



Now to download the data, uncomment the lines below.


```python
# ! kaggle competitions download -c planet-understanding-the-amazon-from-space -f train-jpg.tar.7z -p {path}  
# ! kaggle competitions download -c planet-understanding-the-amazon-from-space -f train_v2.csv -p {path}  
# ! unzip -q -n {path}/train_v2.csv.zip -d {path}
```

    Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'
    Downloading train-jpg.tar.7z to /root/.fastai/data/planet
     99% 593M/600M [00:05<00:00, 168MB/s]
    100% 600M/600M [00:05<00:00, 110MB/s]
    Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'
    Downloading train_v2.csv.zip to /root/.fastai/data/planet
      0% 0.00/159k [00:00<?, ?B/s]
    100% 159k/159k [00:00<00:00, 51.0MB/s]


To extract the content of this file, we'll need 7zip, so uncomment the following line if you need to install it (or run `sudo apt install p7zip-full` in your terminal).


```python
# colab does not use conda, so if you are using conda, the following line will not work
# ! conda install -y -c haasad eidl7zip

# instead try this (should be pre-installed)
!apt-get install p7zip-full
```

    Reading package lists... Done
    Building dependency tree       
    Reading state information... Done
    p7zip-full is already the newest version (16.02+dfsg-6).
    The following package was automatically installed and is no longer required:
      libnvidia-common-410
    Use 'apt autoremove' to remove it.
    0 upgraded, 0 newly installed, 0 to remove and 10 not upgraded.


And now we can unpack the data (uncomment to run - this might take a few minutes to complete).


```python
# ! 7za -bd -y -so x {path}/train-jpg.tar.7z | tar xf - -C {path.as_posix()}
```

## Multiclassification

Contrary to the pets dataset studied in last lesson, here each picture can have multiple labels. If we take a look at the csv file containing the labels (in 'train_v2.csv' here) we see that each 'image_name' is associated to several tags separated by spaces.


```python
df = pd.read_csv(path/'train_v2.csv')
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>image_name</th>
      <th>tags</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>train_0</td>
      <td>haze primary</td>
    </tr>
    <tr>
      <th>1</th>
      <td>train_1</td>
      <td>agriculture clear primary water</td>
    </tr>
    <tr>
      <th>2</th>
      <td>train_2</td>
      <td>clear primary</td>
    </tr>
    <tr>
      <th>3</th>
      <td>train_3</td>
      <td>clear primary</td>
    </tr>
    <tr>
      <th>4</th>
      <td>train_4</td>
      <td>agriculture clear habitation primary road</td>
    </tr>
  </tbody>
</table>
</div>



### Creating `DataBunch`

This step is very important, regardless of what kind of dataset we are using, in order to do modelling, we first need to convert the dataset to a `DataBunch` object.

Different types of datasets will use slightly different functions to convert them to `DataBunch`, so its a good idea to check out [data block API](https://docs.fast.ai/data_block.html),

For this case, we need to use `ImageList` (and not `ImageDataBunch`). This will make sure the model created has the proper loss function to deal with the multiple classes.

Pay attention to the specific data augmentations applied here as seen in the parameters of `get_transforms`. `flip_vert=True` because for satellite image the specific orientation should not matter (unlike in a dog classifier where we believe that the photo is always upright, so `flip_vert = False` makes sense). `max_lighting=0.1`, `max_zoom=1.05` and `max_warp = 0.` are parameters we can play around to see what works best.


```python
# assign image transformation process
tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.)
```

We use parentheses around the data block pipeline below, so that we can use a multiline statement without needing to add '\\'.


```python
np.random.seed(42)
src = (ImageList.from_csv(path, 'train_v2.csv', folder='train-jpg', suffix='.jpg')
       # Where to find the data? -> in path 'train-jpg' folder
       .split_by_rand_pct(0.2)
       # How to split in train/valid? -> randomly with the default 20% in valid
       .label_from_df(label_delim=' '))
       # How to label? -> use the second column of the csv file and split the tags by ' '
```


```python
data = (src.transform(tfms, size=128)
       # Data augmentation? -> use tfms with a size of 128        
        .databunch().normalize(imagenet_stats))
       # Use the defaults for conversion to databunch, and normalize it
```

`show_batch` still works, and show us the different labels separated by `;`.


```python
data.show_batch(rows=3, figsize=(12,9))
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/planets/output_33_0.png" alt="">


### Creating `Learner`

Time to create our `Learner`, we will be using resnet50 for the base architecture, but the metrics are a little bit different: we use `accuracy_thresh` instead of `accuracy`, because this a multi-label classification problem, and so each activation can be 0. or 1. `accuracy_thresh` selects the ones that are above a certain threshold (0.5 by default) and compares them to the ground truth.

As for Fbeta, it's the metric that was used by Kaggle on this competition. See [here](https://en.wikipedia.org/wiki/F1_score) for more details.


```python
# model base architecture
arch = models.resnet50
```


```python
acc_02 = partial(accuracy_thresh, thresh=0.2)
f_score = partial(fbeta, thresh=0.2)
learn = cnn_learner(data, arch, metrics=[acc_02, f_score])
```

    Downloading: "https://download.pytorch.org/models/resnet50-19c8e357.pth" to /root/.torch/models/resnet50-19c8e357.pth
    102502400it [00:01, 98455974.34it/s]



```python
data.classes
```




    ['agriculture',
     'artisinal_mine',
     'bare_ground',
     'blooming',
     'blow_down',
     'clear',
     'cloudy',
     'conventional_mine',
     'cultivation',
     'habitation',
     'haze',
     'partly_cloudy',
     'primary',
     'road',
     'selective_logging',
     'slash_burn',
     'water']



**Why are we not using `accuracy` but `accuracy_thresh`?**

Because we are not going to pick 1 of those 17, instead, we are gonna pick n out of the17 classes.
We compare each of probability to some threshold (which we set to be `0.2`)

If something has a higher probability than the threshold, we can assume it has that feature. Feel free to experiment with different threshold that gives better results.

Normal accuracy function cannot do that. It canâ€™t argmax. We have to use a different function called `accuracy_thresh()`. So this one will compare all probabilities with a threshold and return the ones which are higher than the threshold.

**What is `partial()`?**

Itâ€™s common to define a new function that just like an old function but always gonna be called with a particular parameter. In python, this is called the partial function.

The partial function takes some function and keyword and values and creates new function acc_02 i.e exactly same as accuracy_thresh, but it is always gonna get called up with thresh=0.2.

### Choosing Learning Rate

We use the LR Finder to pick a good learning rate.


```python
learn.lr_find()
```





    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.



```python
learn.recorder.plot()
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/planets/output_43_0.png" alt="">


Then we can fit the head of our network.


```python
# we choose this, because its at a point where its going for a sharp gradient descent
lr = 0.01
```


```python
learn.fit_one_cycle(5, slice(lr))
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/planets/output_02_0.png" alt="">



```python
learn.save('stage-1-rn50')
```

...And fine-tune the whole model:


```python
learn.unfreeze()
```


```python
learn.lr_find()
learn.recorder.plot()
```





    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/planets/output_50_2.png" alt="">


```python
# rule of thumb: choose a learning rate before it spikes as the first slice
# and then choose frozen model lr / 5 as the second slice
learn.fit_one_cycle(5, slice(1e-5, lr/5))
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/planets/output_03_0.png" alt="">



```python
learn.save('stage-2-rn50')
```

### Transfer Learning

The model is doing very well, but it can be further improved by doing **transfer learning** .

Which is to create a new DataBunch for the same learner, where the DataBunch is of size 256 X 256 instead. Because the original image data from Kaggle is of size 256 x 256, why not do this the first place?

Because its a quick way to experiment how well can the learner recognize the context of 128 X 128 satellite image tiles, but more importantly, after many training steps, the learner is so well at recognizing 128 X 128 that it could be prone to overfitting, and by alternating the DataBunch size is kinda like creating a whole new dataset and we can evaluate again using the previous learner.




```python
data = (src.transform(tfms, size=256)
        .databunch().normalize(imagenet_stats))

learn.data = data
data.train_ds[0][0].shape
```




    torch.Size([3, 256, 256])




```python
learn.freeze()
```


```python
learn.lr_find()
learn.recorder.plot()
```





    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/planets/output_57_2.png" alt="">


```python
lr=1e-3
```


```python
learn.fit_one_cycle(5, slice(lr))
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/planets/output_04_0.png" alt="">



```python
learn.save('stage-1-256-rn50')
```


```python
learn.unfreeze()
```


```python
learn.lr_find()
learn.recorder.plot()
```





    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/planets/output_62_2.png" alt="">


```python
learn.fit_one_cycle(5, slice(1e-5, lr/5))
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/planets/output_05_0.png" alt="">



```python
learn.recorder.plot_losses()
```

<img src="{{ site.url }}{{ site.baseurl }}/assets/images/planets/output_64_0.png" alt="">


```python
learn.save('stage-2-256-rn50')
```


```python
# export the learner to be used in production
learn.export()
```

We won't really know how we're going until we submit to Kaggle, since the leaderboard isn't using the same subset as we have for training. But as a guide, 50th place (out of 938 teams) on the private leaderboard was a score of `0.930`.

In another word, I think we did pretty well with `0.931` validation f2 score with our subset of training set. Credit goes to [fast.ai](https://www.fast.ai/) for the invaluable learning experience.

Gratitude to you for reading my blogpost, see you soon!

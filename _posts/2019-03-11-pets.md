---
title: "Image classification with resnet34 & resnet50"
date: 2019-03-11
permalink: /pets-1.1/
tags: [fastai, deep learning, resnet34, resnet50, pets]
excerpt: "Meow or Woof?"
mathjax: "true"
---

# What breed is your Meow & Woof?

What if I tell you I can build a model that can classify if an underlying image shows a cat or a dog? Isn't that cool? No?

What if I tell you my model can also classify the breed of your pet? I am a a little bit cooler now huh?

For this blogpost, We are going to use the [Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) by [O. M. Parkhi et al., 2012](http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf) which features **12 cat breeds and 25 dogs breeds**. Our model will need to learn to differentiate between these 37 distinct categories.

According to their paper, **the best accuracy they could get in 2012 was 59.21%**, using a complex model that was specific to pet detection, with separate "Image", "Head", and "Body" models for the pet photos. Let's see how accurate we can be using deep learning!

But, first we gotta set up some configurations...


# Colab Cloud Server VM setup & FastAI Configurations

To learn how to set up a Colab Server which supports fastai library and its applications, [click here](https://course.fast.ai/start_colab.html).  There are many other server options, but basically I chose Colab because issa **FREE**.

NB: This is a free service that may not always be available, and requires extra steps to ensure your work is saved. Be sure to read the docs on the Colab web-site to ensure you understand the limitations of the system.


```python
# Permit collaboratory instance to read/write to Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
root_dir = "/content/gdrive/My Drive/"
base_dir = root_dir + 'fastai-v3/'
```

    Mounted at /content/gdrive


We import all the necessary packages. We are going to work with the [fastai V1 library](http://www.fast.ai/2018/10/02/fastai-ai/) which sits on top of [Pytorch 1.0](https://hackernoon.com/pytorch-1-0-468332ba5163). The fastai library provides many useful functions that enable us to quickly and easily build neural networks and train our models.


```python
from fastai.vision import *
from fastai.metrics import error_rate
```


```python
# this line creates data & models folder and functionalities integration (e.g. untar_data, model.save)  
  !curl -s https://course.fast.ai/setup/colab | bash
```

    Updating fastai...
    Done.


Every notebook starts with the following three lines; they ensure that any edits to libraries you make are reloaded here automatically, and also that any charts or images displayed are shown in this notebook.


```python
%reload_ext autoreload
%autoreload 2
%matplotlib inline
```

If you're using a computer with an unusually small GPU, you may get an out of memory error when running this notebook. If this happens, click Kernel->Restart, uncomment the 2nd line below to use a smaller *batch size* (you'll learn all about what this means during the course), and try again.


```python
bs = 64
# bs = 16   # uncomment this line if you run out of memory even after clicking Kernel->Restart
```

# Understanding the Data

We are going to use the `untar_data` function to which we must pass a URL as an argument and which will download and extract the data.


```python
# let's read a bit about the function
help(untar_data)
```

    Help on function untar_data in module fastai.datasets:

    untar_data(url:str, fname:Union[pathlib.Path, str]=None, dest:Union[pathlib.Path, str]=None, data=True, force_download=False) -> pathlib.Path
        Download `url` to `fname` if it doesn't exist, and un-tgz to folder `dest`.




```python
# this is the url for the data, remember to add .tgz at the back
print(URLs.PETS)
```

    https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet



```python
# This download the data to the 'data' subdirectory
path = untar_data(URLs.PETS)
path
```




    PosixPath('/content/data/oxford-iiit-pet')




```python
# List what's inside the oxford-iiit-pet folder
path.ls()
```




    [PosixPath('/content/data/oxford-iiit-pet/annotations'),
     PosixPath('/content/data/oxford-iiit-pet/images')]




```python
path_anno = path/'annotations'
path_img = path/'images'
```

The first thing we do when we approach a problem is to take a look at the data. We _always_ need to understand very well what the problem is and what the data looks like before we can figure out how to solve it. Taking a look at the data means understanding how the data directories are structured, what the labels are and what some sample images look like.

The main difference between the handling of image classification datasets is the way labels are stored. In this particular dataset, labels are stored in the filenames themselves. We will need to extract them to be able to classify the images into the correct categories. Fortunately, the fastai library has a handy function made exactly for this, `ImageDataBunch.from_name_re` gets the labels from the filenames using a [regular expression](https://docs.python.org/3.6/library/re.html).


```python
fnames = get_image_files(path_img)
fnames[:5]
```




    [PosixPath('/content/data/oxford-iiit-pet/images/newfoundland_44.jpg'),
     PosixPath('/content/data/oxford-iiit-pet/images/boxer_160.jpg'),
     PosixPath('/content/data/oxford-iiit-pet/images/american_bulldog_109.jpg'),
     PosixPath('/content/data/oxford-iiit-pet/images/shiba_inu_19.jpg'),
     PosixPath('/content/data/oxford-iiit-pet/images/staffordshire_bull_terrier_201.jpg')]




```python
np.random.seed(2)
pat = r'/([^/]+)_\d+.jpg$' # regular expression pattern to extract labels
```

# Data Preprocessing

Now the following is fairly straightforward, but there are a few that of notes:

`size = 224`

- Size of images: In this post, the images used are square (i.e. height = width), and of size 224 x 224. Images which are of different shape/size are resized and cropped accordingly. It is one of the drawback of deep learning models that they need images of same size. Variable size images are in scope of part 2 of the course.

`ds_tfms=get_transforms()`

- Transform: Not sure the exact transformation going on here, but basically, it does centre cropping and rescaling on images to convert size of images to 224 x 224.

`bs = bs`

- Batch size: number of instances that will be propagated through the network,  [Here's a good example!](https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network)

`data.normalize(imagenet_stats)`

- Image normalization: Individual pixel values range from 0 to 255 and are normalized to bring the value to mean of 0 and standard deviation of 1. [Why do we need to normalize?](https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network)

`ImageDataBunch.from_name_re` returns a databunch object which contains training, and validation data, sometimes along with test data as well.


```python
data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs)
data.normalize(imagenet_stats)
```




    ImageDataBunch;

    Train: LabelList (5912 items)
    x: ImageList
    Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)
    y: CategoryList
    newfoundland,boxer,american_bulldog,pomeranian,german_shorthaired
    Path: /content/data/oxford-iiit-pet/images;

    Valid: LabelList (1478 items)
    x: ImageList
    Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)
    y: CategoryList
    american_bulldog,British_Shorthair,english_cocker_spaniel,Sphynx,wheaten_terrier
    Path: /content/data/oxford-iiit-pet/images;

    Test: None




```python
data.show_batch(rows=3, figsize=(7,6))
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Pets/output_25_0.png" alt="">



```python
print(data.classes)
len(data.classes),data.c
```

    ['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair', 'Egyptian_Mau', 'Maine_Coon', 'Persian', 'Ragdoll', 'Russian_Blue', 'Siamese', 'Sphynx', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond', 'leonberger', 'miniature_pinscher', 'newfoundland', 'pomeranian', 'pug', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier']





    (37, 37)



# Deep Learning with resnet34

Now we will start training our model. We will use a [convolutional neural network](http://cs231n.github.io/convolutional-networks/) backbone and a fully connected head with a single hidden layer as a classifier. Don't know what these things mean? Not to worry, we will dive deeper in the coming lessons. For the moment you need to know that we are building a model which will take images as input and will output the predicted probability for each of the categories (in this case, it will have 37 outputs).

We will train for 4 epochs (4 cycles through all our data).


```python
learn = create_cnn(data, models.resnet34, metrics = error_rate)
```

    /usr/local/lib/python3.6/dist-packages/fastai/vision/learner.py:105: UserWarning: `create_cnn` is deprecated and is now named `cnn_learner`.
      warn("`create_cnn` is deprecated and is now named `cnn_learner`.")



The first time we run the above code, it was downloading resnet34 pre-trained weights, because this particular model has already been trained on looking at about one and a half million pictures of thousands of different categories of things using an image data set called **ImageNet**.


The idea is that, we do not start off with a model that is totally clueless, this model probably knows something about recognizing images, most probably know the difference between cats and dogs too.


This is called 'Transfer Learning', which takes a pre-trained model that knows how to do a particular task pretty well, and make it so that it can do our task really well, it also greatly reduced model training time.


Also, the `error_rate` metrics that we chose to print in the `create_cnn` actually prints the validation set error rate.


```python
learn.fit_one_cycle(4)
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Pets/output_01_0.png" alt="">


Deep learning with resnet34 gives an validation error rate of 6.63%, that's 93.37% accuracy with a few line of codes. Obviously, if we run this multiple times, we will get slightly different results (about 5-7%), this is known as [model variance](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff), but more on that later.

Although we can further improve the model, but this is already a very decent model! We will explore more on why resnet34 work so well in the future, for now, just embrace the beauty of it.


```python
# save the parameters/tuned weights in the 'models' folder
learn.save(name = 'pets-stage-1', return_path = True)
```




    PosixPath('/content/data/oxford-iiit-pet/images/models/pets-stage-1.pth')




```python
# create a model directory in gdrive
folder = 'models'

path = Path(base_dir)
dest = path/folder
dest.mkdir(parents=True, exist_ok=True)

# use !cp to copy my saved learner over to gdrive
!cp /content/data/oxford-iiit-pet/images/models/pets-stage-1.pth /content/gdrive/My\ Drive/fastai-v3/models
```

## Results

Let's see what results we have got.

We will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not. In this case the mistakes look reasonable (none of the mistakes seems obviously naive). This is an indicator that our classifier is working correctly.

Furthermore, when we plot the confusion matrix, we can see that the distribution is heavily skewed: the model makes the same mistakes over and over again but it rarely confuses other categories. This suggests that it just finds it difficult to distinguish some specific categories between each other; this is normal behaviour.


```python
type(learn) # learner object has data and trained model
```




    fastai.basic_train.Learner




```python
interp = ClassificationInterpretation.from_learner(learn) # who said deep learning was a black box?
```


```python
interp.plot_top_losses(9, figsize=(15,11))
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Pets/output_39_0.png" alt="">



Basically, the title of each image shows: prediction, actual, loss, probability of actual class.

For example, if we look at the top left image, the model predicts that the breed to be a boxer but it turned out to be a pug, with a loss of 7.26, and a 0% prediction probability of actual class. We will talk about loss in the future, for now just know the algorithm predict that the probability of the image showing an american pit bull terrier (actual class) to be 0%.

Visual inspection also tells us that resnet34 does a poor classification job when the image has some kind of weird contrast level.


```python
# This is to visualize actual vs predicted, but harder to see when its a multi-classification
interp.plot_confusion_matrix(figsize=(12,12), dpi=100)
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Pets/output_41_0.png" alt="">


Seems like resnet34 tend to misclassify 'Ragdoll' as 'Birman', any dog expert here?


```python
# this output actual, predicted, and frequency of misclassification
interp.most_confused(min_val=2)
```




    [('Ragdoll', 'Birman', 8),
     ('staffordshire_bull_terrier', 'american_pit_bull_terrier', 6),
     ('basset_hound', 'beagle', 4),
     ('Birman', 'Ragdoll', 3),
     ('american_bulldog', 'american_pit_bull_terrier', 3),
     ('american_pit_bull_terrier', 'american_bulldog', 3),
     ('chihuahua', 'miniature_pinscher', 3),
     ('pug', 'boxer', 3),
     ('staffordshire_bull_terrier', 'american_bulldog', 3),
     ('Abyssinian', 'Maine_Coon', 2),
     ('Bengal', 'Abyssinian', 2),
     ('Bengal', 'Egyptian_Mau', 2),
     ('Birman', 'Persian', 2),
     ('Birman', 'Siamese', 2),
     ('British_Shorthair', 'Russian_Blue', 2),
     ('Maine_Coon', 'Ragdoll', 2),
     ('Siamese', 'Birman', 2),
     ('american_bulldog', 'staffordshire_bull_terrier', 2),
     ('american_pit_bull_terrier', 'staffordshire_bull_terrier', 2),
     ('pomeranian', 'samoyed', 2)]



## Unfreezing, fine-tuning, and learning rates

Since our model is working as we expect it to, we will *unfreeze* our model and train some more.

The basic idea is that when we call `fit_one_cycle`,  the model freeze the early convolutional layers of the network and only train the last few layers which make a prediction, which is a lot faster.


```python
learn.unfreeze()
```


```python
learn.fit_one_cycle(1)
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Pets/output_02_0.png" alt="">


Unexpectedly, when we try to tune more layers of the network, we get a worse result, a validation error rate of 8.53%.  

Different layers in convolutional neural network represents different levels of semantic complexitiy

By default, the model trains all layers at the same speed. In another word, it will update the early convolutional layers which represent like diagonals and gradients (ImageNet) just as much it tries to update the later convolutional layers which represent the exact specifics of how an eyeball look like, and we do not want that to be the case when we know what we are looking for; cat breeds & dog breeds.


```python
# load our previously saved parameters/weights
learn.load('pets-stage-1');
```

We use `lr_find()` to find out what is the fastest learning rate (how quickly are we updating the parameters in the model) we should pick, without causing the divergent behaviours.


```python
learn.lr_find()
```





    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.



```python
learn.recorder.plot()
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Pets/output_52_0.png" alt="">



```python
# train first layer at lr of 1e-6, and last layer at lr of 1e-4, and every other layers in between the range
# good rule of thumb is to make 2nd part of the slice 10 times smaller (i.e. 1e-04) than your first stage (i.e. 1e-03)
# and first slice should be guided by visualizing lr_finder()
learn.unfreeze()
learn.fit_one_cycle(4, max_lr=slice(1e-6, 1e-4))
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Pets/output_03_0.png" alt="">



# Deep Learning with resnet50

Now we will train in the same way as before but with one caveat: instead of using resnet34 as our backbone we will use resnet50 (resnet34 is a 34 layer residual network while resnet50 has 50 layers. we will get into the details later.)

Basically, resnet50 usually performs better because it is a deeper network with more parameters. Let's see if we can achieve a higher performance here. To help it along, let's us use larger images too, since that way the network can see more detail. We reduce the batch size a bit since otherwise this larger network will require more GPU memory.


```python
# convert image to databunch and normalize them
data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(),
                                   size=299, bs=48).normalize(imagenet_stats)
```


```python
# model learning with resnet50
learn = create_cnn(data, models.resnet50, metrics=error_rate)
```

    /usr/local/lib/python3.6/dist-packages/fastai/vision/learner.py:105: UserWarning: `create_cnn` is deprecated and is now named `cnn_learner`.
      warn("`create_cnn` is deprecated and is now named `cnn_learner`.")



```python
learn.fit_one_cycle(5)
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Pets/output_04_0.png" alt="">




```python
learn.save('stage-1-res50')
```

Seems like adding additional hidden layers werent enough to improve the results, lets try to tune the learning rate and see if we can get a better results.


```python
# visualize loss against learning rate
learn.lr_find()
learn.recorder.plot()
```





    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Pets/output_61_2.png" alt="">



```python
# Select learning rate
learn.fit_one_cycle(3, max_lr=slice(1e-6,1e-4))
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Pets/output_05_0.png" alt="">




```python
# In case your previous model is better, uncomment the following which had better results
# learn.load('stage-1-50')
# learn.load('stage-1')
```

Looks like we manage to get the model to 95.2% of validation accuracy, and this is also an improvement as compared to res34. It's astonishing how we can achieve such accuracy with the amount of work we put in.


```python
interp = ClassificationInterpretation.from_learner(learn)
```


```python
# This is to visualize actual vs predicted, but harder to see when its a multi-classification
interp.plot_confusion_matrix(figsize=(12,12), dpi=100)
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Pets/output_66_0.png" alt="">



```python
interp.most_confused(min_val=2)
```




    [('Ragdoll', 'Birman', 8),
     ('american_pit_bull_terrier', 'staffordshire_bull_terrier', 5),
     ('staffordshire_bull_terrier', 'american_bulldog', 5),
     ('Bengal', 'Maine_Coon', 4),
     ('Bengal', 'Egyptian_Mau', 3),
     ('Egyptian_Mau', 'Bengal', 3),
     ('Russian_Blue', 'British_Shorthair', 3),
     ('american_bulldog', 'staffordshire_bull_terrier', 3),
     ('Egyptian_Mau', 'Maine_Coon', 2),
     ('Persian', 'Ragdoll', 2),
     ('miniature_pinscher', 'american_pit_bull_terrier', 2)]



# Predicting my dog's breed with resnet50

As shown in the confusion matrix of resnet50, out of all 39 predictions for 'pomeranian', one was misclassified and was actually 'chihuahua', I wouldn't really be harsh on the algorithm because even human could make these misclassifications.

Anyway, I've uploaded some photos of my dog on google drive, she's a pomeranian, how well can the algorithm predict the breed of my dog? :)

Let's see!


```python
# create your path for your own custom images
img_path = Path("/content/gdrive/My Drive/fastai-v3/pets_data/")
```


```python
fnames = get_image_files(img_path)
fnames[:4]
```




    [PosixPath('/content/gdrive/My Drive/fastai-v3/pets_data/pomeranian_0.jpeg'),
     PosixPath('/content/gdrive/My Drive/fastai-v3/pets_data/pomeranian_3.jpeg'),
     PosixPath('/content/gdrive/My Drive/fastai-v3/pets_data/pomeranian_2.jpeg'),
     PosixPath('/content/gdrive/My Drive/fastai-v3/pets_data/pomeranian_1.jpeg')]




```python
# Have not figured out an elegant way to do single image prediction, so for now I am using custom code
@dataclass
class ConvPredictor:

    learner: create_cnn
    mean: FloatTensor
    std: FloatTensor

    def __post_init__(self):
        device = self.learner.data.device
        self.mean, self.std = [torch.tensor(x).to(device) for x in (self.mean, self.std)]

    def predict(self, x):
        out = self.predict_logits(x)
        best_index = F.softmax(out).argmax()
        return self.learner.data.classes[best_index]

    def predict_logits(self, x):
        x = x.to(self.learner.data.device)
        x = normalize(x, self.mean, self.std)
        out = self.learner.model(x[None])
        return out

    def predict_from_file(self, filename):
        data = open_image(filename).data
        return self.predict(x)

```


```python
# long haired pomeranian in a jersey
img0 = open_image(fnames[0])
img0
```




<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Pets/output_73_0.png" alt="">




```python
predictor = ConvPredictor(learn, *imagenet_stats)
predictor.predict(img0.data)
```

    /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.






    'pomeranian'



The algorithm correctly classify the breed of my dog, despite her wearing a jersey. 1-0 resnet50.


```python
# short haired pomeranian
img1 = open_image(fnames[1])
img1
```




<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Pets/output_76_0.png" alt="">




```python
predictor.predict(img1.data)
```

    /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.






    'pomeranian'



Dang, this was supposed to be a hard one, because Poms typically have ruff of fur on their neck, just like the previous photo. Fine, 2-0 resnet50.


```python
# a really good looking man and a pomeranian
img2 = open_image(fnames[2])
img2
```




<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Pets/output_79_0.png" alt="">




```python
predictor.predict(img2.data)
```

    /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.






    'Abyssinian'



Abyssinian? That's a cat, terrible terrible misclassifcation, can't really blame the algorithm because it is probably picking the patterns from my face as well.


```python
# short haired pomeranian with different camera angle shoot
img3 = open_image(fnames[3])
img3
```




<img src="{{ site.url }}{{ site.baseurl }}/assets/images/Pets/output_82_0.png" alt="">




```python
predictor.predict(img3.data)
```

    /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.






    'pomeranian'



Wow, even when she's not looking at the camera, the algorithm can still pick up enough distinct patterns to classify her correctly. Brilliant!

That my friend, is known as 'Deep Learning'. In all seriousness, we didn't really cover the maths and theories behind the algorithm, but that's really because I am following through a course by [fast.ai](https://www.fast.ai/) and its following a somewhat unusual learning approach in being *top-down* rather than bottom-up. So rather than starting with theory, and only getting to practical applications later, instead we start with practical applications, and then gradually dig deeper and deeper in to them, learning the theory as needed.

Another thing that i'd like to point out, all the work that has been done in this project, all the environmental variables, packages, and files that have been downloaded, will not persist across different sessions. Consequently, we will have to rerun the code when we come back to this project, but that's okay for now since this is a very tiny project to get us started.

*There will be a lot of knowledge gaps, but I assure you that we will be filling these gaps later on.*

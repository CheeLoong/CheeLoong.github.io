---
title: "Understanding Neural Network on Fashion-MNIST dataset with PyTorch"
date: 2019-07-03
permalink: /fmnist-cnn/
tags: [pytorch, cnn, fashion mnist, deep learning]
excerpt: "Building a simple CNN with PyTorch library from scratch"
mathjax: "true"
---


If you have been reading my previous blogposts, you'd realize that I am mostly using fast.ai library, which can be seen as another layer built on top of PyTorch which gives new functionalities around building a neural network.

Fun fact is, I have not learnt PyTorch before I learn fast.ai, occasionally I scratch my head when I see PyTorch code in fast.ai implementation, so here we are, me trying to build a simple CNN on Fashion-MNIST dataset, which is supposed to be the harder version of the traditional digit MNIST dataset.

Note that this blogpost is not about achieving state-of-the-art accuracy classifier for this particular dataset, but rather understanding what each line of code in PyTorch means as we build them.


```python
import torch
import torch.nn as nn
import torch.functional as F
import torch.optim as optim

import torchvision
import torchvision.transforms as transforms

import numpy as np
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings("ignore")

torch.set_printoptions(linewidth = 120) # display options for output
torch.set_grad_enabled(True) # this is True by default
```




    <torch.autograd.grad_mode.set_grad_enabled at 0x7fb4f7d6c518>



## Prepare the Data

### Extract & Transform
Get the Fashion-MNIST dataset from the source, and transform the data into `tensor`, data type should be in `torchvision.datasets` after transformations.


```python
train_set = torchvision.datasets.FashionMNIST(root = './data/FashionMNIST',
                                             train = True,
                                             download = True,
                                             transform = transforms.Compose([transforms.ToTensor()]))
```


```python
type(train_set)
```




    torchvision.datasets.mnist.FashionMNIST



### Load
Put data into dataloader that can be easily accessible.


```python
train_loader = torch.utils.data.DataLoader(train_set, batch_size = 16)
```

### Training Set Exploration


```python
len(train_set)
```




    60000




```python
train_set.train_labels
```




    tensor([9, 0, 0,  ..., 3, 0, 5])




```python
# train data is class-balanced!
train_set.train_labels.bincount()
```




    tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])




```python
sample = next(iter(train_set))
```


```python
# tuple containing image dataset and the corresponding label
print(type(sample), len(sample))
```

    <class 'tuple'> 2



```python
# sequence unpacking
image, label = sample
```


```python
print(image.shape)
print(label)
```

    torch.Size([1, 28, 28])
    9



```python
plt.imshow(image.squeeze(), cmap = 'gray')
print('label:', label)
```

    label: 9



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/fmnist-cnn/output_16_1.png" alt="">



Now, let's do this for a batch.


```python
batch = next(iter(train_loader))
```


```python
# tuple containing image datasets and the corresponding labels
print(type(batch), len(batch))
```

    <class 'list'> 2



```python
# sequence unpacking
images, labels = batch
```


```python
print(images.shape)
print(labels)
```

    torch.Size([16, 1, 28, 28])
    tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5, 0, 9, 5, 5, 7, 9])



```python
grid = torchvision.utils.make_grid(images, nrow = 8)

plt.figure(figsize = (15,15))
plt.imshow(np.transpose(grid, (1,2,0)))

print('label:', label)
```

    label: 9



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/fmnist-cnn/output_22_1.png" alt="">


## Build the Model

### Network Architecture

We will create a 6 layers network; an input layer, 2 x convolution layers, 2 x linear layers, and an output layer.


```python
import torch.nn as nn
import torch.nn.functional as F
```


```python
class Network(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5)
    self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5)

    self.fc1 = nn.Linear(in_features = 12*4*4, out_features = 120)
    self.fc2 = nn.Linear(in_features = 120, out_features = 60)
    self.out = nn.Linear(in_features = 60, out_features = 10)

  def forward(self, t):
    # implement forward pass
    return t
```


```python
network = Network()
network
```




    Network(
      (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
      (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))
      (fc1): Linear(in_features=192, out_features=120, bias=True)
      (fc2): Linear(in_features=120, out_features=60, bias=True)
      (out): Linear(in_features=60, out_features=10, bias=True)
    )




```python
# conv1 weights
network.conv1.weight
```




    Parameter containing:
    tensor([[[[ 0.0877,  0.0452, -0.0610, -0.0898,  0.1692],
              [-0.1225,  0.0032, -0.1845,  0.1101, -0.0158],
              [-0.0575, -0.0969, -0.1704,  0.0098, -0.0344],
              [ 0.0939,  0.0010,  0.0057,  0.0452,  0.0521],
              [ 0.1607,  0.1492, -0.0816,  0.0104, -0.0932]]],


            [[[ 0.0124, -0.1305,  0.1463,  0.0406,  0.0982],
              [ 0.0494,  0.0166,  0.1170, -0.0163, -0.0165],
              [-0.0465, -0.1710, -0.0277, -0.1644,  0.0746],
              [-0.0684, -0.0112, -0.0860, -0.0885,  0.1287],
              [-0.0014, -0.1647,  0.1379,  0.1751,  0.0879]]],


            [[[ 0.0776,  0.1957,  0.0260, -0.0778,  0.0641],
              [ 0.1857,  0.0411, -0.0940,  0.1869,  0.1644],
              [-0.0516,  0.0652, -0.1025,  0.0447,  0.1764],
              [-0.1325,  0.1893, -0.1780, -0.1526, -0.0585],
              [-0.1779,  0.0632, -0.1801,  0.0729, -0.0875]]],


            [[[-0.1489, -0.0531,  0.1190,  0.1800,  0.0293],
              [-0.0992,  0.1321,  0.1582,  0.1103,  0.0783],
              [ 0.0403, -0.0866,  0.1975,  0.1250, -0.1758],
              [-0.0459, -0.0467,  0.0280, -0.0381,  0.0703],
              [ 0.1005,  0.1277,  0.1529, -0.0050, -0.0824]]],


            [[[-0.0737, -0.1788,  0.1797, -0.1960,  0.0217],
              [-0.0494, -0.1379, -0.0750, -0.0103, -0.1383],
              [-0.0622,  0.0953,  0.0073, -0.1855, -0.0432],
              [-0.1787,  0.1334,  0.1900,  0.0503,  0.1420],
              [ 0.1535, -0.0447, -0.0071, -0.0387,  0.0568]]],


            [[[ 0.0939,  0.1766, -0.1146, -0.1969,  0.0439],
              [ 0.1734,  0.0498, -0.1399, -0.1410,  0.0218],
              [-0.1920, -0.0869, -0.1811, -0.1987,  0.0875],
              [ 0.0057, -0.0302,  0.0706,  0.1781, -0.1437],
              [ 0.0135, -0.0707,  0.0989,  0.0705, -0.0626]]]], requires_grad=True)



These weights are learnable parameters, which means that as the network is trained, the weights will update accordingly. In addition, these weight tensors are also instances of `Parameter(torch.Tensor)` class.


```python
network.conv1.weight.shape
```




    torch.Size([6, 1, 5, 5])




```python
network.conv2.weight.shape
```




    torch.Size([12, 6, 5, 5])




```python
for name, param in network.named_parameters():
  print(name,'\t\t', param.shape)
```

    conv1.weight 		 torch.Size([6, 1, 5, 5])
    conv1.bias 		 torch.Size([6])
    conv2.weight 		 torch.Size([12, 6, 5, 5])
    conv2.bias 		 torch.Size([12])
    fc1.weight 		 torch.Size([120, 192])
    fc1.bias 		 torch.Size([120])
    fc2.weight 		 torch.Size([60, 120])
    fc2.bias 		 torch.Size([60])
    out.weight 		 torch.Size([10, 60])
    out.bias 		 torch.Size([10])


In `conv1`, we take in 1 input channel (grayscale), and output 6 channels (# of filters), with filter size of 5 * 5, which is why the shape is `[6,1,5,5]`.

In `conv2` we take have 6 input channels (matching 6 output channels from previous layer), with filter size of 5 * 5, and this time outputting 12 channels, resulting shape is `[12,6,5,5]`.



```python
print(network)
print(network.fc1.weight.shape)
print(network.fc2.weight.shape)
```

    Network(
      (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
      (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))
      (fc1): Linear(in_features=192, out_features=120, bias=True)
      (fc2): Linear(in_features=120, out_features=60, bias=True)
      (out): Linear(in_features=60, out_features=10, bias=True)
    )
    torch.Size([120, 192])
    torch.Size([60, 120])


With our linear / fully connect layers (e.g. `fc1`, `fc2`), we have flattened rank 1 tensor as input and as output, the transformation from `in_features` to `out_features` is using a weight matrix with length of height that is same as the # of the desired output features, and with length of width that is the same as the # of the desired input features.

### Understanding Linear Layer


the `Linear()` is really doing matrix multiplication, let's show this with a simplified example.


```python
# flattened rank 1 tensor input
in_features = torch.tensor([1,2,3,4], dtype=torch.float32)
in_features.shape
```




    torch.Size([4])




```python
weight_matrix = torch.tensor([
    [1,2,3,4],
    [2,3,4,5],
    [3,4,5,6]
], dtype=torch.float32)
weight_matrix.shape
```




    torch.Size([3, 4])




```python
weight_matrix.matmul(in_features)
```




    tensor([30., 40., 50.])




```python
# let's try using PyTorch fully connected/linear layet to do the above operation
fc = nn.Linear(in_features = 4, out_features = 3)
```


```python
fc(in_features)
```




    tensor([ 0.0680,  1.9663, -1.0057], grad_fn=<AddBackward0>)



The result is different from above, because PyTorch initializes random weights to do the `matmul` with `in_features`, but we can specify the weight matrix if we want to.


```python
# set weight matrix
fc.weight = nn.Parameter(weight_matrix)
```


```python
fc(in_features)
```




    tensor([29.8064, 40.1985, 50.4821], grad_fn=<AddBackward0>)



Notice how we called the object instance as if it were a function (i.e. `fc(in_features))`, because PyTorch implemented a special call method `__call__(in_features)`, so instead of calling `forward(in_features)`, we call using the object instance instead, therefore, `fc(in_features)` invokes `__call__(in_features)` which invokes `forward(in_features)`.

Notice that the output value has a slight difference due to bias terms, let's turn the bias terms.


```python
fc = nn.Linear(in_features = 4, out_features = 3, bias = False)
fc.weight = nn.Parameter(weight_matrix)
```


```python
fc(in_features)
```




    tensor([30., 40., 50.], grad_fn=<SqueezeBackward3>)



### Forward Method


```python
class Network(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5)
    self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5)

    self.fc1 = nn.Linear(in_features = 12*4*4, out_features = 120)
    self.fc2 = nn.Linear(in_features = 120, out_features = 60)
    self.out = nn.Linear(in_features = 60, out_features = 10)

  def forward(self, t):
    # (1) input layer
    t = t

    # (2) hidden conv layer
    t = self.conv1(t)
    t = F.relu(t)
    t = F.max_pool2d(t, kernel_size = 2, stride = 2)

    # (3) hidden conv layer
    t = self.conv2(t)
    t = F.relu(t)
    t = F.max_pool2d(t, kernel_size = 2, stride = 2)

    return t
```

It's important to know that in this network, we have a bunch of layers, and *each of these layers is comprising of collections of weights and operations.*

The weights are encapsulated inside the neural network module layer class instances (i.e. `self.conv1(t)`), both the `relu` and `max_pool2d` are pure operations, which is why they don't contain weights and we call them directly from `torch.nn.functional`API.

Although it's quite common to hear an activation layer, of max pooling layer, its good to think about layers without weights as operations. For example, if we look at our first hidden conv layer in the code above, we will say **the second layer in our network is a convolutional layer that contains a collection of weights (i.e. `self.conv1(t)`), and perform 3 operations; a convolution operation, a ReLu operation, and a max pooling operation.**


```python
class Network(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5)
    self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5)

    self.fc1 = nn.Linear(in_features = 12*4*4, out_features = 120)
    self.fc2 = nn.Linear(in_features = 120, out_features = 60)
    self.out = nn.Linear(in_features = 60, out_features = 10)

  def forward(self, t):
    # (1) input layer
    t = t

    # (2) hidden conv layer
    t = self.conv1(t)
    t = F.relu(t)
    t = F.max_pool2d(t, kernel_size = 2, stride = 2)

    # (3) hidden conv layer
    t = self.conv2(t)
    t = F.relu(t)
    t = F.max_pool2d(t, kernel_size = 2, stride = 2)

    # (4) hidden linear layer
    t = t.reshape(-1, 12, 4 * 4)
    t = self.fc1(t)
    t = F.relu(t)

    # (5) hidden linear layer
    t = self.fc2(t)
    t = F.relu(t)

    # (6) output layer
    t = self.out(t)
    #t = F.softmax(t, dim = 1)


    return t
```

Before we pass our input to the first hidden linear layer, we must reshape or flatten our tensors, this will be the case any time we passing output from the conv layer as input to another linear layer. Since the fourth layer is the first linear layer, we will include shaping operation as part of the 4th layer.

We won't use softmax because the loss function we will be using is cross entropy loss function from `nn.Functional` which implicitly perform a softmax operations on its input.

### Forward propagation with a batchsize of 1

Let's also shorten the previous code a little bit.


```python
class Network(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5)
    self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 12, kernel_size = 5)

    self.fc1 = nn.Linear(in_features = 12*4*4, out_features = 120)
    self.fc2 = nn.Linear(in_features = 120, out_features = 60)
    self.out = nn.Linear(in_features = 60, out_features = 10)

  def forward(self, t):
    # (1) input layer
    t = t

    # (2) hidden conv layer
    t = F.relu(self.conv1(t))
    t = F.max_pool2d(t, kernel_size = 2, stride = 2)

    # (3) hidden conv layer
    t = F.relu(self.conv2(t))
    t = F.max_pool2d(t, kernel_size = 2, stride = 2)

    # (4) hidden linear layer
    t = F.relu(self.fc1(t.reshape(-1, 12 * 4 * 4)))

    # (5) hidden linear layer
    t = F.relu(self.fc2(t))

    # (6) output layer
    t = self.out(t)
    #t = F.softmax(t, dim = 1)


    return t
```

Here's the *Operations* and *Output shape* of the network on each layer

(1) input layer:

Identity function --> torch.Size([1, 1, 28, 28])

(2) hidden conv layer:

Convolution (5 x 5)	--> torch.Size([1, 6, 24, 24])

Max pooling (2 x 2) -->	torch.Size([1, 6, 12, 12])

(3) hidden conv layer:

Convolution (5 x 5)	torch.Size([1, 12, 8, 8])

Max pooling (2 x 2)	torch.Size([1, 12, 4, 4])

(4) hidden linear layer:

Flatten (reshape)	torch.Size([1, 192])

Linear transformation	torch.Size([1, 120])

(5) hidden linear layer:

Linear transformation	torch.Size([1, 60])

(6) output layer

Linear transformation	torch.Size([1, 10])

Stop the computation graph tracking because we are not training the network yet.


```python
torch.set_grad_enabled(False)
```




    <torch.autograd.grad_mode.set_grad_enabled at 0x7fb4f4ec2cf8>




```python
network = Network()
```


```python
sample = next(iter(train_set))
```


```python
image, label = sample
image.shape
```




    torch.Size([1, 28, 28])




```python
import matplotlib.pyplot as plt
```


```python
plt.imshow(image.squeeze(), cmap = 'gray')
print('label:', label)
```

    label: 9



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/fmnist-cnn/output_63_1.png" alt="">


We want to propagate this image through our network, we need to package this a rank 4 tensor because `forward` method in Pytorch module expects that. We can simply do this by unsqueezing.


```python
image.unsqueeze(0).shape # this gives us a batchsize of 1
```




    torch.Size([1, 1, 28, 28])




```python
# get the network prediction
pred = network(image.unsqueeze(0))
```


```python
pred.shape
```




    torch.Size([1, 10])




```python
pred
```




    tensor([[-0.0754, -0.0463, -0.0748, -0.1625, -0.0700,  0.0998, -0.0886,  0.0339,  0.0248, -0.0028]])




```python
pred.argmax(dim = 1)
```




    tensor([5])



For each class, we have a prediction value, if we wanted the probability, we can use `softmax`.


```python
F.softmax(pred, dim = 1)
```




    tensor([[0.0959, 0.0987, 0.0960, 0.0879, 0.0964, 0.1143, 0.0946, 0.1070, 0.1060, 0.1031]])



We can see that when we used `pred.argmax` we found that the class associated with highest prediction value was indexed at 1, which represent a trouser, but the actual value should be 9, which is an ankle boot, the prediction is incorrect, but that's expected because the weights were randomly assigned and the network hasn't started training.

### Forward Propagation with a batchsize of 10


```python
data_loader = torch.utils.data.DataLoader(train_set, batch_size = 10)
```


```python
batch = next(iter(data_loader))
```


```python
images, labels = batch
```


```python
print(images.shape)
print(labels.shape)
```

    torch.Size([10, 1, 28, 28])
    torch.Size([10])



```python
preds = network(images)
```


```python
preds.shape
```




    torch.Size([10, 10])




```python
preds
```




    tensor([[-0.0754, -0.0463, -0.0748, -0.1625, -0.0700,  0.0998, -0.0886,  0.0339,  0.0248, -0.0028],
            [-0.0730, -0.0552, -0.0670, -0.1671, -0.0793,  0.1035, -0.0940,  0.0297,  0.0191, -0.0048],
            [-0.0787, -0.0473, -0.0855, -0.1593, -0.0729,  0.0939, -0.0811,  0.0372,  0.0159, -0.0050],
            [-0.0754, -0.0502, -0.0803, -0.1642, -0.0772,  0.0970, -0.0854,  0.0311,  0.0140, -0.0007],
            [-0.0746, -0.0557, -0.0733, -0.1703, -0.0750,  0.1048, -0.0916,  0.0235,  0.0190, -0.0015],
            [-0.0697, -0.0552, -0.0735, -0.1680, -0.0795,  0.1005, -0.0901,  0.0281,  0.0162, -0.0036],
            [-0.0757, -0.0463, -0.0814, -0.1596, -0.0722,  0.0983, -0.0853,  0.0377,  0.0179, -0.0061],
            [-0.0704, -0.0558, -0.0729, -0.1712, -0.0805,  0.1028, -0.0911,  0.0262,  0.0195, -0.0023],
            [-0.0773, -0.0401, -0.0867, -0.1557, -0.0678,  0.0945, -0.0835,  0.0360,  0.0184, -0.0032],
            [-0.0710, -0.0441, -0.0837, -0.1571, -0.0709,  0.0982, -0.0850,  0.0337,  0.0146, -0.0016]])




```python
preds.argmax(dim = 1)
```




    tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5])




```python
labels
```




    tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])




```python
# simple function showing amount correct predictions

def get_correct_preds(preds, labels):
  return preds.argmax(dim = 1).eq(labels).sum().item()
```


```python
get_correct_preds(preds, labels)
```




    2



## Calculate Loss, Gradient & Weights Update


```python
# turn computation graph tracking on
torch.set_grad_enabled(True)
```




    <torch.autograd.grad_mode.set_grad_enabled at 0x7fb4f4e97518>




```python
train_loader = torch.utils.data.DataLoader(train_set, batch_size = 100)
batch = next(iter(train_loader))
images, labels = batch
```

### Calculate loss


```python
# calculating the loss
preds = network(images)
loss = F.cross_entropy(preds, labels)
loss.item()
```




    2.3150975704193115



### Calculate gradient through backpropagation


```python
# sanity check on gradient on first layer before back-prop
print(network.conv1.weight.grad)
```

    None



```python
# calculating the gradient
loss.backward()
```


```python
# gradients are being calculated and updated
network.conv1.weight.grad.shape
```




    torch.Size([6, 1, 5, 5])



Understand that for each of the parameters in the weight tensor, there is a corresponding gradient for that particular parameter.

### Update weights using ADAM optimizer


```python
optimizer = optim.Adam(network.parameters(), lr = 0.01)
```


```python
# check our loss again
loss.item()
```




    2.3150975704193115




```python
# check number of correct predictions
get_correct_preds(preds, labels)
```




    11




```python
# update the weights
optimizer.step()
```


```python
# pass same batch of images to network again
preds = network(images)
loss = F.cross_entropy(preds, labels)
```


```python
loss.item()
```




    2.2999227046966553




```python
get_correct_preds(preds, labels)
```




    11



We have achieved a lower loss and a higher number of correct predictions for 1 batch after updating the weights.

### Training with a single batch

Let's summarize the training of a single batch in one block of code.


```python
network = Network()

train_loader = torch.utils.data.DataLoader(train_set, batch_size = 100)
optimizer = optim.Adam(network.parameters(), lr = 0.01)

batch = next(iter(train_loader)) # get batch
images, labels = batch

preds = network(images) # pass batch
loss = F.cross_entropy(preds, labels) # calculate loss

loss.backward() # calculate gradients
optimizer.step() # update weights

#-------------------------------------------------------

print('current loss:', loss.item())
preds = network(images)
loss = F.cross_entropy(preds, labels)
print('updated loss:', loss.item())
```

    current loss: 2.312145233154297
    updated loss: 2.2928755283355713


### Training with all the batches in 1 epoch

To train all the batches, we need to create a training loop, and also keeping track of the loss after every iterations and amount of correct predictions, we also use `optimizer_zero_grad()` to zero out the gradients from the previous iteration on every iteration.


```python
network = Network()

train_loader = torch.utils.data.DataLoader(train_set, batch_size = 100)
optimizer = optim.Adam(network.parameters(), lr = 0.01)

total_loss = 0
total_correct = 0

for batch in train_loader:
    images, labels = batch

    preds = network(images) # pass batch
    loss = F.cross_entropy(preds, labels) # calculate loss

    optimizer.zero_grad() # zero out gradients held in grad attributes
    loss.backward() # calculate gradients
    optimizer.step() # update weights

    total_loss += loss.item()
    total_correct += get_correct_preds(preds, labels)

print("epoch:", 0, "total_correct:", total_correct, "loss:", total_loss)
```

    epoch: 0 total_correct: 45374 loss: 383.05339550971985


Since we have specified a batch size of 100, and the train set is consist of 60,000 images, that means we will have 60,000/100 = 600 iterations, that also means our weights are being updated 600 times, so if we think about it from a gradient descent point of view, that would be 600 steps towards the loss function minimum, they key takeaway is that if we change the batch size, we would change the magnitude of steps we are taking in gradient descent.


```python
print(total_correct / len(train_set))
```

    0.7562333333333333


Thats close to 77% accuracy after 600 iterations (a.k.a 1 epoch).

### Training with multiple epochs
If we want to train all the batches in multiple epochs, just add a loop, let's say we want to do 5 epochs.


```python
network = Network()

train_loader = torch.utils.data.DataLoader(train_set, batch_size = 100)
optimizer = optim.Adam(network.parameters(), lr = 0.01)

for epoch in range(5):

    total_loss = 0
    total_correct = 0

    for batch in train_loader:
        images, labels = batch

        preds = network(images) # pass batch
        loss = F.cross_entropy(preds, labels) # calculate loss

        optimizer.zero_grad() # zero out gradients held in grad attributes
        loss.backward() # calculate gradients
        optimizer.step() # update weights

        total_loss += loss.item()
        total_correct += get_correct_preds(preds, labels)

    print("epoch:", epoch, "total_correct:", total_correct, "loss:", total_loss)
```

    epoch: 0 total_correct: 46946 loss: 342.4444157779217
    epoch: 1 total_correct: 51189 loss: 237.18022437393665
    epoch: 2 total_correct: 51831 loss: 218.74542425572872
    epoch: 3 total_correct: 52269 loss: 207.52790862321854
    epoch: 4 total_correct: 52547 loss: 201.7267366796732


Wow, now my brain can actually visualize the tensor flowing from input to the output and backpropagate through the network, calculating the gradient, updating the weights.

I am pretty sure I have a solid understanding of what is going on even before this, but right now the picture is just so much clearer.

That is all for the blogpost, I will see you soon!



In the previous blogpost, I've built a CNN with PyTorch using Fashion MNIST dataset to learn more about PyTorch in general. In this one, I will emphasize on the model accuracy and I will be using fast.ai library as well, this dataset is made to replace the digit recognizer MNIST dataset, so they hold a lot in common, they both are in csv format with 1 column specifying the labels, and the rest of the columns are the pixel values of the image.

However, in my previous blogpost on Digit Recognizer MNIST dataset, I was using fast.ai processed dataset, which are in image files, so it was easy to put that into `ImageDataBunch` and conveniently perform model training, but this time, I downloaded the data straight off from Kaggle, and it was in `csv` form, the challenge was to convert that to `ImageDataBunch`.

First import libraries and load the codes as usual.

```python
%reload_ext autoreload
%autoreload 2
%matplotlib inline
```


```python
!curl -s https://course.fast.ai/setup/colab | bash
```

    Updating fastai...
    Done.



```python
import numpy as np
import matplotlib.pyplot as plt

from fastai.vision import *

import warnings
warnings.filterwarnings("ignore")

torch.set_printoptions(linewidth = 120) # display options for output
torch.set_grad_enabled(True) # this is True by default
```




    <torch.autograd.grad_mode.set_grad_enabled at 0x7fc725e12f98>



## Getting the data from Kaggle

All ready to get the data from Kaggle.

```python
# ! pip3 install kaggle --upgrade
```


```python
## uncomment these two lines after uploading kaggle.json
! mkdir -p ~/.kaggle/
! mv kaggle.json ~/.kaggle/
```


```python
path = Config.data_path()/'fashion-mnist'
path.mkdir(parents=True, exist_ok=True)
path
```




    PosixPath('/root/.fastai/data/fashion-mnist')




```python
# download fashion mnist dataset from Kaggle to `path`
! kaggle datasets download -d zalando-research/fashionmnist -f fashion-mnist_train.csv -p {path}
! kaggle datasets download -d zalando-research/fashionmnist -f fashion-mnist_test.csv -p {path}
```

    Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'
    Downloading fashion-mnist_train.csv.zip to /root/.fastai/data/fashion-mnist
     72% 24.0M/33.6M [00:00<00:00, 19.8MB/s]
    100% 33.6M/33.6M [00:00<00:00, 68.3MB/s]
    Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'
    Downloading fashion-mnist_test.csv.zip to /root/.fastai/data/fashion-mnist
     89% 5.00M/5.59M [00:00<00:00, 22.2MB/s]
    100% 5.59M/5.59M [00:00<00:00, 22.1MB/s]



```python
# unzip the file
! unzip -q -n {path}/fashion-mnist_train.csv -d {path}
! unzip -q -n {path}/fashion-mnist_test.csv -d {path}
```


```python
# remove the zip files
# !rm {path}/fashion-mnist_train.csv.zip
# !rm {path}/fashion-mnist_test.csv.zip
```


```python
path.ls()
```




    [PosixPath('/root/.fastai/data/fashion-mnist/fashion-mnist_train.csv'),
     PosixPath('/root/.fastai/data/fashion-mnist/fashion-mnist_test.csv'),
     PosixPath('/root/.fastai/data/fashion-mnist/fashion-mnist_train.csv.zip'),
     PosixPath('/root/.fastai/data/fashion-mnist/fashion-mnist_test.csv.zip')]




```python
df_train = pd.read_csv(path/'fashion-mnist_train.csv')
df_train.head(2)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>pixel1</th>
      <th>pixel2</th>
      <th>pixel3</th>
      <th>pixel4</th>
      <th>pixel5</th>
      <th>pixel6</th>
      <th>pixel7</th>
      <th>pixel8</th>
      <th>pixel9</th>
      <th>pixel10</th>
      <th>pixel11</th>
      <th>pixel12</th>
      <th>pixel13</th>
      <th>pixel14</th>
      <th>pixel15</th>
      <th>pixel16</th>
      <th>pixel17</th>
      <th>pixel18</th>
      <th>pixel19</th>
      <th>pixel20</th>
      <th>pixel21</th>
      <th>pixel22</th>
      <th>pixel23</th>
      <th>pixel24</th>
      <th>pixel25</th>
      <th>pixel26</th>
      <th>pixel27</th>
      <th>pixel28</th>
      <th>pixel29</th>
      <th>pixel30</th>
      <th>pixel31</th>
      <th>pixel32</th>
      <th>pixel33</th>
      <th>pixel34</th>
      <th>pixel35</th>
      <th>pixel36</th>
      <th>pixel37</th>
      <th>pixel38</th>
      <th>pixel39</th>
      <th>...</th>
      <th>pixel745</th>
      <th>pixel746</th>
      <th>pixel747</th>
      <th>pixel748</th>
      <th>pixel749</th>
      <th>pixel750</th>
      <th>pixel751</th>
      <th>pixel752</th>
      <th>pixel753</th>
      <th>pixel754</th>
      <th>pixel755</th>
      <th>pixel756</th>
      <th>pixel757</th>
      <th>pixel758</th>
      <th>pixel759</th>
      <th>pixel760</th>
      <th>pixel761</th>
      <th>pixel762</th>
      <th>pixel763</th>
      <th>pixel764</th>
      <th>pixel765</th>
      <th>pixel766</th>
      <th>pixel767</th>
      <th>pixel768</th>
      <th>pixel769</th>
      <th>pixel770</th>
      <th>pixel771</th>
      <th>pixel772</th>
      <th>pixel773</th>
      <th>pixel774</th>
      <th>pixel775</th>
      <th>pixel776</th>
      <th>pixel777</th>
      <th>pixel778</th>
      <th>pixel779</th>
      <th>pixel780</th>
      <th>pixel781</th>
      <th>pixel782</th>
      <th>pixel783</th>
      <th>pixel784</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>9</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>2 rows × 785 columns</p>
</div>




```python
df_test = pd.read_csv(path/'fashion-mnist_test.csv')
df_test.head(2)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>pixel1</th>
      <th>pixel2</th>
      <th>pixel3</th>
      <th>pixel4</th>
      <th>pixel5</th>
      <th>pixel6</th>
      <th>pixel7</th>
      <th>pixel8</th>
      <th>pixel9</th>
      <th>pixel10</th>
      <th>pixel11</th>
      <th>pixel12</th>
      <th>pixel13</th>
      <th>pixel14</th>
      <th>pixel15</th>
      <th>pixel16</th>
      <th>pixel17</th>
      <th>pixel18</th>
      <th>pixel19</th>
      <th>pixel20</th>
      <th>pixel21</th>
      <th>pixel22</th>
      <th>pixel23</th>
      <th>pixel24</th>
      <th>pixel25</th>
      <th>pixel26</th>
      <th>pixel27</th>
      <th>pixel28</th>
      <th>pixel29</th>
      <th>pixel30</th>
      <th>pixel31</th>
      <th>pixel32</th>
      <th>pixel33</th>
      <th>pixel34</th>
      <th>pixel35</th>
      <th>pixel36</th>
      <th>pixel37</th>
      <th>pixel38</th>
      <th>pixel39</th>
      <th>...</th>
      <th>pixel745</th>
      <th>pixel746</th>
      <th>pixel747</th>
      <th>pixel748</th>
      <th>pixel749</th>
      <th>pixel750</th>
      <th>pixel751</th>
      <th>pixel752</th>
      <th>pixel753</th>
      <th>pixel754</th>
      <th>pixel755</th>
      <th>pixel756</th>
      <th>pixel757</th>
      <th>pixel758</th>
      <th>pixel759</th>
      <th>pixel760</th>
      <th>pixel761</th>
      <th>pixel762</th>
      <th>pixel763</th>
      <th>pixel764</th>
      <th>pixel765</th>
      <th>pixel766</th>
      <th>pixel767</th>
      <th>pixel768</th>
      <th>pixel769</th>
      <th>pixel770</th>
      <th>pixel771</th>
      <th>pixel772</th>
      <th>pixel773</th>
      <th>pixel774</th>
      <th>pixel775</th>
      <th>pixel776</th>
      <th>pixel777</th>
      <th>pixel778</th>
      <th>pixel779</th>
      <th>pixel780</th>
      <th>pixel781</th>
      <th>pixel782</th>
      <th>pixel783</th>
      <th>pixel784</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>9</td>
      <td>8</td>
      <td>0</td>
      <td>0</td>
      <td>34</td>
      <td>29</td>
      <td>7</td>
      <td>0</td>
      <td>11</td>
      <td>24</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>3</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>242</td>
      <td>245</td>
      <td>224</td>
      <td>245</td>
      <td>234</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>37</td>
      <td>69</td>
      <td>94</td>
      <td>123</td>
      <td>127</td>
      <td>138</td>
      <td>138</td>
      <td>142</td>
      <td>145</td>
      <td>135</td>
      <td>125</td>
      <td>103</td>
      <td>87</td>
      <td>56</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>209</td>
      <td>190</td>
      <td>181</td>
      <td>150</td>
      <td>170</td>
      <td>193</td>
      <td>180</td>
      <td>219</td>
      <td>5</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>24</td>
      <td>...</td>
      <td>248</td>
      <td>238</td>
      <td>80</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>174</td>
      <td>233</td>
      <td>155</td>
      <td>0</td>
      <td>65</td>
      <td>235</td>
      <td>216</td>
      <td>34</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>2 rows × 785 columns</p>
</div>



First column is the label of the image, and the other 784 columns are basically the pixel of the images, which are 28 * 28 pixel images, which explains the 784 columns. However, this is not how we look at image data.

Both train and test set are labeled, which is why I assigned `fashion-mnist_test.csv` as  `df_valid`.


```python
print('train shape:     ', df_train.shape)
print('validation shape:', df_test.shape)
```

    train shape:      (60000, 785)
    validation shape: (10000, 785)


## Data Preprocessing

A small road block that I face in this project is that the dataset is not image files but in `csv` with label columns and pixel columns, in my previous blogpost, the MNIST digit recognizer dataset was in image files, so we can't exactly copy and paste the code from there.

However, we know that we got to put our dataset into `ImageDataBunch` eventually, we already learnt how to do it if we have image files, but now we have `csv` files with the first column as labels and the rest as the pixel values, how do we go about doing it? We got to write some custom functions.


```python
class CustomImageList(ImageList):
    def open(self, fn):
        # Skip label (first column if available) and stack the other columns into a 28x28 image
        img = fn[1:].reshape(28, 28) if len(fn) > 784 else fn.reshape(28, 28)
        img = np.stack((img,) * 3, axis=-1) # convert to 3 channels

        x = pil2tensor(img, dtype=np.float32) # convert ndarray to tensor
        x.div_(255) # Range should be 0.0 to 1.0 for pixel values
        return Image(x)

    @classmethod
    def from_csv_custom(cls, path:PathOrStr, csv_name:str, imgIdx=1, header:str='infer', **kwargs) -> 'ItemLists':
        df = pd.read_csv(Path(path)/csv_name, header=header)

        # Set correct labels for fashion mnist (for readability)
        df["label"] = df["label"].astype('category')
        df["label"].cat.categories = [
            "T-shirt/top", "Trouser", "Pullover",
            "Dress", "Coat", "Sandal", "Shirt",
            "Sneaker", "Bag", "Ankle boot"]

        res = super().from_df(df, path=path, **kwargs) # inherits from ItemList.from_df
        res.items = df.iloc[:, imgIdx:].values # make itemlist.items show label AND PIXEL VALUES
        return res
```

The instance method `open` basically open up a single ndarray from the `ItemList` and convert that to `tensor` and show it as an `Image`.

The class method `from_csv_custom` basically reads `csv`, put it in `DataFrame`, convert labels to `string` for readability, and converting the dataframe into an `ItemList` and assiging pixel values from the dataframe into the `ItemList.items`.


```python
arch = models.resnet34
sz = 28
bs = 64
tfms = ([*rand_pad(padding=3, size=sz, mode='zeros')], [])
```


```python
test = CustomImageList.from_csv_custom(path=path, csv_name='fashion-mnist_test.csv', imgIdx=None)
train = CustomImageList.from_csv_custom(path=path, csv_name='fashion-mnist_train.csv')

data = (CustomImageList.from_csv_custom(path=path, csv_name='fashion-mnist_train.csv')
                       .split_by_list(train = train, valid = test)
                       .label_from_df(cols='label')
                       .transform(tfms)
                       .databunch(bs=bs, num_workers=4)
                       .normalize(imagenet_stats))
```

Why am I using the test set as validation set? Because the test set has labels, I mean we can drop the labels and pretend it as test set, but I would like to maximize the performance of the model, so I am going to use the test set as the validation set, in practice its better to split the original train set to train set and validation set, and use the test set as test set.


```python
data
```




    ImageDataBunch;

    Train: LabelList (60000 items)
    x: CustomImageList
    Image (3, 28, 28),Image (3, 28, 28),Image (3, 28, 28),Image (3, 28, 28),Image (3, 28, 28)
    y: CategoryList
    Pullover,Ankle boot,Shirt,T-shirt/top,Dress
    Path: /root/.fastai/data/fashion-mnist;

    Valid: LabelList (10000 items)
    x: CustomImageList
    Image (3, 28, 28),Image (3, 28, 28),Image (3, 28, 28),Image (3, 28, 28),Image (3, 28, 28)
    y: CategoryList
    T-shirt/top,Trouser,Pullover,Pullover,Dress
    Path: /root/.fastai/data/fashion-mnist;

    Test: None




```python
data.show_batch(rows=3, figsize=(5,5))
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/fmnist-cnn2/output_23_0.png" alt="">




```python
def _plot(i,j,ax): data.train_ds[0][0].show(ax, cmap='gray')
plot_multi(_plot, 3, 3, figsize=(5,5))
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/fmnist-cnn2/output_24_0.png" alt="">

The function `_plot` and `plot_multi` basically plot the image of the selected training data and we can see that they are slightly different due to the tranformations (i.e. `tfms`) everytime the image is grabbed from the disk.

## Model Training

### Resnet34 Base Architecture




```python
learn = create_cnn(data, base_arch = arch, metrics = accuracy)
```

    Downloading: "https://download.pytorch.org/models/resnet34-333f7ec4.pth" to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth
    100%|██████████| 87306240/87306240 [00:00<00:00, 116820718.68it/s]



```python
# print(learn.summary())
```


```python
learn.lr_find()
```





    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.



```python
learn.recorder.plot()
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/fmnist-cnn2/output_32_0.png" alt="">



```python
learn.fit_one_cycle(4, max_lr=slice(1e-3,1e-2))
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.640581</td>
      <td>0.482474</td>
      <td>0.813800</td>
      <td>01:04</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.467268</td>
      <td>0.416098</td>
      <td>0.848700</td>
      <td>01:04</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.411352</td>
      <td>0.300628</td>
      <td>0.888800</td>
      <td>01:04</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.349716</td>
      <td>0.293075</td>
      <td>0.889000</td>
      <td>01:04</td>
    </tr>
  </tbody>
</table>



```python
learn.save('stage-1')
```

**Unfreeze and train again**


```python
# learn = create_cnn(data, base_arch = arch, metrics = accuracy)
# learn.load('stage-1')
```


```python
# unfreeze the model
learn.unfreeze()
```


```python
learn.lr_find()
```





    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.



```python
learn.recorder.plot()
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/fmnist-cnn2/output_39_0.png" alt="">



```python
learn.fit_one_cycle(15, max_lr=slice(1e-5,1e-3))
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.346195</td>
      <td>0.285307</td>
      <td>0.893100</td>
      <td>01:31</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.360822</td>
      <td>0.277174</td>
      <td>0.895900</td>
      <td>01:31</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.327483</td>
      <td>0.266927</td>
      <td>0.899900</td>
      <td>01:31</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.320952</td>
      <td>0.253655</td>
      <td>0.900000</td>
      <td>01:31</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.308548</td>
      <td>0.240804</td>
      <td>0.908400</td>
      <td>01:32</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.280608</td>
      <td>0.229278</td>
      <td>0.910600</td>
      <td>01:31</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.248924</td>
      <td>0.225329</td>
      <td>0.912900</td>
      <td>01:32</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.246759</td>
      <td>0.213709</td>
      <td>0.920600</td>
      <td>01:31</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.227157</td>
      <td>0.216023</td>
      <td>0.916500</td>
      <td>01:32</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.228934</td>
      <td>0.201364</td>
      <td>0.921700</td>
      <td>01:32</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.216050</td>
      <td>0.198530</td>
      <td>0.924600</td>
      <td>01:32</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.195642</td>
      <td>0.197770</td>
      <td>0.924400</td>
      <td>01:32</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.183322</td>
      <td>0.192798</td>
      <td>0.928200</td>
      <td>01:31</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.193675</td>
      <td>0.193725</td>
      <td>0.928900</td>
      <td>01:31</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.190245</td>
      <td>0.194233</td>
      <td>0.927800</td>
      <td>01:31</td>
    </tr>
  </tbody>
</table>


The best accuracy we ahieved is approximately 92.9%, that's pretty good, could consider tuning learn rate further or changing base architecture to resnet50 for better accuracy.


```python
interp = ClassificationInterpretation.from_learner(learn)
```


```python
interp.plot_top_losses(9, figsize=(10,10))
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/fmnist-cnn2/output_43_0.png" alt="">



```python
interp.plot_confusion_matrix(figsize = (8, 8))
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/fmnist-cnn2/output_44_0.png" alt="">



```python
interp.most_confused(min_val=2)
```




    [('Shirt', 'T-shirt/top', 100),
     ('T-shirt/top', 'Shirt', 88),
     ('Pullover', 'Coat', 52),
     ('Pullover', 'Shirt', 52),
     ('Shirt', 'Coat', 46),
     ('Coat', 'Pullover', 44),
     ('Coat', 'Shirt', 44),
     ('Shirt', 'Pullover', 38),
     ('Ankle boot', 'Sneaker', 30),
     ('Shirt', 'Dress', 26),
     ('Coat', 'Dress', 19),
     ('Dress', 'Shirt', 18),
     ('Dress', 'Coat', 16),
     ('Dress', 'T-shirt/top', 16),
     ('Sandal', 'Sneaker', 16),
     ('Sneaker', 'Ankle boot', 16),
     ('T-shirt/top', 'Dress', 15),
     ('Pullover', 'T-shirt/top', 13),
     ('T-shirt/top', 'Pullover', 13),
     ('Pullover', 'Dress', 12),
     ('Dress', 'Trouser', 9),
     ('Sneaker', 'Sandal', 7),
     ('Dress', 'Pullover', 4),
     ('Trouser', 'Dress', 4),
     ('Bag', 'Shirt', 3),
     ('Sandal', 'Ankle boot', 3),
     ('Shirt', 'Bag', 3),
     ('T-shirt/top', 'Coat', 3),
     ('Ankle boot', 'Sandal', 2)]



Seems like most misclassifications happen on shirt & T-shirt/top, it's understandable since they can look very identical sometimes.

Let's try a Resnet50 CNN and see how much improvement we can get.

### Resnet50 Base Architecture


```python
del learn
arch = models.resnet50
learn = create_cnn(data, base_arch = arch, metrics = accuracy)
```

    Downloading: "https://download.pytorch.org/models/resnet50-19c8e357.pth" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth
    100%|██████████| 102502400/102502400 [00:00<00:00, 106152946.31it/s]



```python
learn.lr_find()
```





    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.



```python
learn.recorder.plot()
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/fmnist-cnn2/output_50_0.png" alt="">



```python
learn.fit_one_cycle(4, max_lr=slice(1e-3,1e-2))
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.606110</td>
      <td>0.474249</td>
      <td>0.827400</td>
      <td>01:32</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.412902</td>
      <td>0.343599</td>
      <td>0.873700</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.350043</td>
      <td>0.262666</td>
      <td>0.900300</td>
      <td>01:30</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.306530</td>
      <td>0.250169</td>
      <td>0.904100</td>
      <td>01:30</td>
    </tr>
  </tbody>
</table>



```python
learn.save('stage-1-res50')
```


```python
# learn = create_cnn(data, base_arch = arch, metrics = accuracy)
# learn.load('stage-1-res50')
```

**Unfreeze Model**


```python
# unfreeze the model
learn.unfreeze()
```


```python
learn.lr_find()
```





    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.



```python
learn.recorder.plot()
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/fmnist-cnn2/output_57_0.png" alt="">



```python
learn.fit_one_cycle(15, max_lr=slice(1e-5,1e-3))
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.325524</td>
      <td>0.239069</td>
      <td>0.908500</td>
      <td>02:04</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.326735</td>
      <td>0.248483</td>
      <td>0.904900</td>
      <td>02:04</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.284597</td>
      <td>0.240992</td>
      <td>0.908800</td>
      <td>02:04</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.281502</td>
      <td>0.231625</td>
      <td>0.914500</td>
      <td>02:04</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.252360</td>
      <td>0.292156</td>
      <td>0.909400</td>
      <td>02:04</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.242619</td>
      <td>0.268902</td>
      <td>0.920700</td>
      <td>02:04</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.231148</td>
      <td>0.241668</td>
      <td>0.921300</td>
      <td>02:04</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.236667</td>
      <td>0.192701</td>
      <td>0.928300</td>
      <td>02:04</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.198332</td>
      <td>0.198730</td>
      <td>0.930800</td>
      <td>02:05</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.185650</td>
      <td>0.183410</td>
      <td>0.931100</td>
      <td>02:05</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.176017</td>
      <td>0.178165</td>
      <td>0.934500</td>
      <td>02:05</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.157185</td>
      <td>0.175511</td>
      <td>0.935800</td>
      <td>02:04</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.150230</td>
      <td>0.175014</td>
      <td>0.935000</td>
      <td>02:04</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.146324</td>
      <td>0.175356</td>
      <td>0.935700</td>
      <td>02:04</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.137921</td>
      <td>0.182035</td>
      <td>0.935000</td>
      <td>02:04</td>
    </tr>
  </tbody>
</table>



```python
learn.save('stage-1-res50-uf')
```


```python
interp.most_confused(min_val=2)
```




    [('Shirt', 'T-shirt/top', 100),
     ('T-shirt/top', 'Shirt', 88),
     ('Pullover', 'Coat', 52),
     ('Pullover', 'Shirt', 52),
     ('Shirt', 'Coat', 46),
     ('Coat', 'Pullover', 44),
     ('Coat', 'Shirt', 44),
     ('Shirt', 'Pullover', 38),
     ('Ankle boot', 'Sneaker', 30),
     ('Shirt', 'Dress', 26),
     ('Coat', 'Dress', 19),
     ('Dress', 'Shirt', 18),
     ('Dress', 'Coat', 16),
     ('Dress', 'T-shirt/top', 16),
     ('Sandal', 'Sneaker', 16),
     ('Sneaker', 'Ankle boot', 16),
     ('T-shirt/top', 'Dress', 15),
     ('Pullover', 'T-shirt/top', 13),
     ('T-shirt/top', 'Pullover', 13),
     ('Pullover', 'Dress', 12),
     ('Dress', 'Trouser', 9),
     ('Sneaker', 'Sandal', 7),
     ('Dress', 'Pullover', 4),
     ('Trouser', 'Dress', 4),
     ('Bag', 'Shirt', 3),
     ('Sandal', 'Ankle boot', 3),
     ('Shirt', 'Bag', 3),
     ('T-shirt/top', 'Coat', 3),
     ('Ankle boot', 'Sandal', 2)]



93.58% validation accuracy, that's better than our resnet34 base architecture, but it seems like even resnet50 CNN could not figure out how to improve on the classificaiton of shirt vs T-shirt/top.

That is all for the blogpost, the most important thing I was trying to learn from this dataset is how to transform with images with pixel data stored as `csv` into an `ImageDataBunch`.

---
title: "Image Restoration with U-Net & Generative Adversarial Networks on Oxford Pets dataset"
date: 2019-05-30
permalink: /pets-gan/
tags: [fastai, pytorch, u-net, gan, pets, deep learning]
excerpt: "Building a U-Net & GAN for Image restoration task with fastai library"
mathjax: "true"
---

In this blogpost, we will explore how U-Net can help with **Image restoration** on oxford pets dataset that we have used before in our blogpost, basically we want to transform lower resolution photo with text writing on top of the image into a higher resolution photo with the text removed. In additional, we also build a **Generative Adversarial Network (GAN)** to improve the performance of image restoration task.

## Pretrained GAN


```python
import fastai
from fastai.vision import *
from fastai.callbacks import *
from fastai.vision.gan import *
```


```python
# this line creates data & models folder and functionalities integration (e.g. untar_data, model.save)  
  !curl -s https://course.fast.ai/setup/colab | bash
```

    Updating fastai...
    Done.



```python
path = untar_data(URLs.PETS)
path_hr = path/'images'
path_lr = path/'crappy'
```

In order to build U-Net that would transform bad image to good image, we first need a dataset consist of bad images and good images, from the pets dataset, we have got the good bit of images, we have to make a copy of bad images from it.


```python
!ps -aux|grep python
```

    root          24  0.6  0.8 421576 118580 ?       Sl   07:46   1:12 /usr/bin/python2 /usr/local/bin/jupyter-notebook --ip="172.28.0.2" --port=9000 --FileContentsManager.root_dir="/" --MappingKernelManager.root_dir="/content"
    root        1899  0.8  1.8 40048024 250716 ?     Ssl  10:41   0:02 /usr/bin/python3 -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-b7e991dc-40f0-4bb8-a150-ad7bf2445ca8.json
    root        2027  0.0  0.0  39196  6540 ?        S    10:46   0:00 /bin/bash -c ps -aux|grep python
    root        2029  0.0  0.0  38572  4940 ?        S    10:46   0:00 grep python


## Crappified data

Prepare the input data by crappifying images.


```python
from PIL import Image, ImageDraw, ImageFont
```


```python
def crappifier(fn, i):
  dest = path_lr/fn.relative_to(path_hr)    
  dest.parent.mkdir(parents=True, exist_ok=True)
  img = PIL.Image.open(fn)
  targ_sz = resize_to(img, 96, use_min=True)
  img = img.resize(targ_sz, resample=PIL.Image.BILINEAR).convert('RGB')
  w,h = img.size
  q = random.randint(10,70)
  ImageDraw.Draw(img).text((random.randint(0,w//2),random.randint(0,h//2)), str(q), fill=(255,255,255))
  img.save(dest, quality=q)
```

Let's breakdown this code..

first we open the image from the filename

```
img = PIL.Image.open(fn)
```

we resize the image to be smaller (96 x 96)

```
targ_sz = resize_to(img, 96, use_min = True)
```

the method of resizing is bilinear interpolation

```
img = img.resize(targ_sz, resample=PIL.Image.BILINEAR).convert('RGB')
```

pick a number between 10 to 70, and draw it on a random coordinate of the image
```
q = random.randint(10,70)
ImageDraw.Draw(img).text((random.randint(0,w//2),random.randint(0,h//2)), str(q), fill=(255,255,255))
  ```
save this image with JPEG quality of the random number picked in previous step, the lower the worse quality

```
img.save(dest, quality=q)
```

One thing worth of mentioning is that, there are many types of bad images, so if we want to fix a specific type of bad images, we have to make sure that in our `crappifier` code we turn our good images to that specific type of bad images, otherwise the model will not see it as bad images.

`parallel` is a fastai function that does multi-processing, which we pass a function name and a list of items to run the functions, since the crappification process can take a while.


```python
# Uncomment below the first time we run this notebook
# il = ImageList.from_folder(path_hr)
# parallel(crappifier, il.items)
```



    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='7390' class='' max='7390', style='width:300px; height:20px; vertical-align: middle;'></progress>
      100.00% [7390/7390 00:32<00:00]
    </div>



Here's a preview of what happened to one of our image that has been crappified.



```python
print(ImageList.from_folder(path_lr)[123].show(title='crappfied - low res'), il[123].show(title='original - high res'))
```

    None None



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/pets-gan/output_15_1.png" alt="">



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/pets-gan/output_15_2.png" alt="">


For gradual resizing we can change the commented line here.


```python
bs,size=32, 128
# bs,size = 24,160
#bs,size = 8,256
arch = models.resnet34
```

## Pre-train Generator with U-Net

Now let's pretrain the generator.


```python
arch = models.resnet34
src = ImageImageList.from_folder(path_lr).split_by_rand_pct(0.1, seed=42)
```


```python
src
```




    ItemLists;

    Train: ImageImageList (6651 items)
    Image (3, 96, 128),Image (3, 128, 96),Image (3, 96, 128),Image (3, 96, 136),Image (3, 96, 128)
    Path: /root/.fastai/data/oxford-iiit-pet/crappy;

    Valid: ImageImageList (739 items)
    Image (3, 96, 145),Image (3, 96, 128),Image (3, 96, 128),Image (3, 96, 128),Image (3, 96, 127)
    Path: /root/.fastai/data/oxford-iiit-pet/crappy;

    Test: None




```python
# to understand what this line of code does to src, uncomment below
# src.label_from_func(lambda x: path_hr/x.name)
```

We will be using `resnet34` encoder pretrained for our U-Net and `ImageImageList` was used instead of `ImageList` because `ImageImageList` item list is suited for Image to Image tasks.

We then define a function to get labels for our crappy images which are the good quality images from `path_hr`, transform it, turn it into a databunch and normalize it with `imagenet_stats` because we are using a pretrained model.


```python
def get_data(bs,size):
    data = (src.label_from_func(lambda x: path_hr/x.name)
           .transform(get_transforms(max_zoom=2.), size=size, tfm_y=True)
           .databunch(bs=bs).normalize(imagenet_stats, do_y=True))

    data.c = 3
    return data
```


```python
data_gen = get_data(bs,size)
```


```python
data_gen
```




    ImageDataBunch;

    Train: LabelList (6651 items)
    x: ImageImageList
    Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)
    y: ImageList
    Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)
    Path: /root/.fastai/data/oxford-iiit-pet/crappy;

    Valid: LabelList (739 items)
    x: ImageImageList
    Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)
    y: ImageList
    Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)
    Path: /root/.fastai/data/oxford-iiit-pet/crappy;

    Test: None




```python
data_gen.show_batch(4)
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/pets-gan/output_27_0.png" alt="">


To clarify, we now have `ImageDataBunch` with training set which include the bad images (X) and the good images (y), and also validation set of it.


```python
wd = 1e-3
```


```python
y_range = (-3.,3.)
```


```python
loss_gen = MSELossFlat()
```

We're using MSE loss, so in other words what's the mean squared error between the actual pixel value that it should be in the pixel value that we predicted. MSE loss normally expects two vectors. In our case, we have two images so in fastai library, we use this `MSElossflat` which simply flattens out those images into a vector.

*(Note: there's never any reason not to use this, even if you do have a vector, it works fine, if you don't have a vector, it'll also work fine.)*

This whole thing, We are calling a "generator", because the model is generating something, there isn't really a formal definition, but think of it this way, the output is an object which in this case, an image, rather than a value.


```python
def create_gen_learner():
    return unet_learner(data_gen, arch, wd=wd, blur=True, norm_type=NormType.Weight,
                         self_attention=True, y_range=y_range, loss_func=loss_gen)
```

So we created our U-Net with that data, the architecture is ResNet 34. `blur, norm_type, self_attention` will be discussed in  future blogpost, for now, we would always include them when building U-Net for this kind of problem.


```python
learn_gen = create_gen_learner()
```

    Downloading: "https://download.pytorch.org/models/resnet34-333f7ec4.pth" to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth
    100%|██████████| 87306240/87306240 [00:04<00:00, 21576011.68it/s]



```python
learn_gen.fit_one_cycle(2, pct_start=0.8)
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.064175</td>
      <td>0.057696</td>
      <td>01:58</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.052079</td>
      <td>0.050140</td>
      <td>01:51</td>
    </tr>
  </tbody>
</table>


As discussed in older blogpost, when we do transfer learning, by default fastai will freeze the the pre-trained part which for U-Net is the downsampling part. That's where the `resnet34` is.

So let's unfreeze and have a go again!


```python
learn_gen.unfreeze()
```


```python
learn_gen.fit_one_cycle(3, slice(1e-6,1e-3))
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.051302</td>
      <td>0.049894</td>
      <td>01:55</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.049144</td>
      <td>0.047027</td>
      <td>01:55</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.047413</td>
      <td>0.045845</td>
      <td>01:55</td>
    </tr>
  </tbody>
</table>



```python
learn_gen.show_results(rows=4)
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/pets-gan/output_40_0.png" alt="">



```python
learn_gen.save('gen-pre2')
```

And we have got ourself a watermark removal model because the model seemed to removed all the numbers on top of the image, probably not the best at upsampling, and sometimes when it removes a number, it might leave a little bit of JPEG artifact, but overall, great job!

The question now is, can we improve the upsampling? That is, can the predictions be more like the targets because we can obviously tell that the predictions have a resolution that is much lower than the target.

## Understanding Generative Adversarial Network (GAN)

Looking back at our `Input/Prediction/Target` output again, the main reason why we are not progressing as much as we would hope for in terms of the pixel resolution, is because we were using **Pixel MSE** as the loss function, and in fact the mean squared errors between the pixel in our prediction and target is actually very small, which is why we can see the colour of each pixel is almost always on point, that tells us that the model does not think that the prediction is that far off from the target as much as we do.

Paying closer attention to the images, we can see that the model is missing out certain features like texture of furs, details of background, etc. Thus, we need to use a different loss function that can actually distinguish that target images are of good quality, and our input images are of bad quality.

<br>

A **Generative Adversarial Network or GAN** tries to solve this problem by calling another model, and to ease understanding, here's a diagram from fast.ai lesson.

<img src="https://i.imgur.com/MjUo3dg.png" width="800">

Let's walk through this flow diagram, we have got our crappy images, and we passed it through the generator that we have created, which gives us prediction of images, we then take the prediction of the images to compare it with Hi-res image with the loss function **Pixel MSE**, so far so good.

Here's where it gets interesting, we create another **Discriminator / Critic** model (let's just call it critic), that is essentially a binary classification model that takes all the pairs of the generated image and the real high-res image, and learn to classify which image is in which category. Therefore, a critic is just a regular binary cross-entropy classifier

Therefore, rather than using **Pixel MSE** as the loss, we would be using the **Critic** as the loss, that is, how good are we at generating prediction of images that the critic thinks are real? To minimize loss means fooling the critics, and that means to generate a set of prediction of images that the critic would classify it as real image.

We first train generator to do as good images as plausible using **Pixel MSE**, then we train critic to recognize which is generated using **Critic**, when the critic is smart enough to distinguish between the real and the generated, we train generator again with the **Trained Critic** as the loss function, and we continue this loop until model produces good results. This is fastai library version of **GAN**, the takeaway is that both loss functions offer a different kind of optimization and using alternately will yield better results than just using one.

*note: fastai library version of **GAN** also has pretrained model for the generator and pretrained critic*




## Save generated images

Since we are using fastai library to create a binary classifier, we need to prepare 2 folders; one folder containing the high-res images, and the other containing generated images, we already have the high-res images folder, now we have to save the generated images to a new folder.


```python
learn_gen.load('gen-pre2');
```


```python
name_gen = 'image_gen'
path_gen = path/name_gen
```


```python
# shutil.rmtree(path_gen)
```


```python
path_gen.mkdir(exist_ok=True)
```


```python
def save_preds(dl):
    i=0
    names = dl.dataset.items

    for b in dl:
        preds = learn_gen.pred_batch(batch=b, reconstruct=True)
        for o in preds:
            o.save(path_gen/names[i].name)
            i += 1
```

We define `save_preds` that takes on a data loader, we grab the filenames with `dl.dataset.items`. For each batch in the data loader, grab a batch of predictions for that batch, `reconstruct = True` to create fastai image object for each of the prediction in the batch. For each of the predictions save them using the name of the original file that we've stored earlier.


```python
save_preds(data_gen.fix_dl)
```

Let's randomly pick an image form the generated image folder, bam.


```python
PIL.Image.open(path_gen.ls()[0])
```




<img src="{{ site.url }}{{ site.baseurl }}/assets/images/pets-gan/output_55_0.png" alt="">



## Training Critic

Now we are ready to train the critic, but before we do that, let's reclaim some GPU memory without having to restart the Jupyter Notebook runtime, we know that the learner we created earlier `learn_gen` is taking up quite some GPU memory, so we can set it to `None` and do memory garbage collection with `gc.collect()`


```python
learn_gen=None
gc.collect()
```




    20



First, we have to make sure the data is a `DataBunch`, so as usual, we get the `ImageList` from the folder, `classes` is a parameter which allows us to specify which folders to include in the `ImageList`, we then get the `LabelList` with `label_from_folder`, some transformations, and converting it to `databunch` and normalize with `imagenet_stats`.


```python
def get_crit_data(classes, bs, size):
    src = ImageList.from_folder(path, include=classes).split_by_rand_pct(0.1, seed=42)
    ll = src.label_from_folder(classes=classes)
    data = (ll.transform(get_transforms(max_zoom=2.), size=size)
           .databunch(bs=bs).normalize(imagenet_stats))
    data.c = 3
    return data
```


```python
data_crit = get_crit_data([name_gen, 'images'], bs=bs, size=size)
```


```python
data_crit
```




    ImageDataBunch;

    Train: LabelList (12637 items)
    x: ImageList
    Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)
    y: CategoryList
    image_gen,image_gen,image_gen,image_gen,image_gen
    Path: /root/.fastai/data/oxford-iiit-pet;

    Valid: LabelList (1404 items)
    x: ImageList
    Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)
    y: CategoryList
    images,images,images,image_gen,images
    Path: /root/.fastai/data/oxford-iiit-pet;

    Test: None




```python
data_crit.show_batch(rows=3, ds_type=DatasetType.Train, imgsize=3)
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/pets-gan/output_63_0.png" alt="">


Alright, we got our `DataBunch` set up, let's build the critic model!


```python
loss_critic = AdaptiveLoss(nn.BCEWithLogitsLoss())
```

The reason why we wrap the Binary Cross Entropy loss `nn.BCEWithLogitsLoss()` with `AdaptiveLoss()` is because `gan_critic()` has a slightly way of averaging different part of the losses from images, so we have to remember to wrap it when are doing **GAN**.


```python
def create_critic_learner(data, metrics):
    return Learner(data, gan_critic(), metrics=metrics, loss_func=loss_critic, wd=wd)
```

Notice that we did not use `resnet` as the model, it has something to do with spectral normalization which is beyond the scope of this blogpost, for now, just know that we can call `gan_critc()` and fastai libary will provide a suitable binary classifier for **GAN**.


```python
learn_critic = create_critic_learner(data_crit, accuracy_thresh_expand)
```

We use `accuracy_thresh_expand` because this is the **GAN** equivalent of accuracy in fastai, since we are using a slightly different model and loss function.


```python
learn_critic.fit_one_cycle(6, 1e-3)
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy_thresh_expand</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.632402</td>
      <td>0.628435</td>
      <td>0.669459</td>
      <td>03:28</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.264103</td>
      <td>0.217744</td>
      <td>0.921994</td>
      <td>03:31</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.123434</td>
      <td>0.606294</td>
      <td>0.704103</td>
      <td>03:32</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.075148</td>
      <td>0.117710</td>
      <td>0.960969</td>
      <td>03:32</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.041826</td>
      <td>0.104471</td>
      <td>0.966439</td>
      <td>03:32</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.039979</td>
      <td>0.083247</td>
      <td>0.974587</td>
      <td>03:33</td>
    </tr>
  </tbody>
</table>


As we can see, our model is at approximately 97.4% accurate in recognizing which images are real and which are generated.



```python
learn_critic.save('critic-pre2')
```

## Building GAN

<img src="https://i.imgur.com/MjUo3dg.png" width="800">

Let's look at this flowchart again and do an update on our progress, we now have a pre-trained generator, and a pre-trained critic, and we will use those pretrained model in a **GAN**.


```python
learn_crit=None
learn_gen=None
gc.collect()
```




    15774



Get the data for our critics again, and load our pre-trained **Critic** model.


```python
data_crit = get_crit_data(['crappy', 'images'], bs=bs, size=size)
```


```python
learn_crit = create_critic_learner(data_crit, metrics=None).load('critic-pre2')
```

load the pre-trained **Generator** model.


```python
learn_gen = create_gen_learner().load('gen-pre2')
```

To define a GAN Learner, we just have to specify the learner objects foor the generator and the critic. The switcher is a callback that decides when to switch from discriminator to generator and vice versa. Here we do as many iterations of the discriminator as needed to get its loss back < 0.5 then one iteration of the generator.

The loss of the critic is given by `learn_crit.loss_func`. We take the average of this loss function on the batch of real predictions (target 1) and the batch of fake predicitions (target 0).

The loss of the generator is weighted sum (weights in `weights_gen`) of `learn_crit.loss_func` on the batch of fake (passed throught the critic to become predictions) with a target of 1, and the `learn_gen.loss_func` applied to the output (batch of fake) and the target (corresponding batch of superres images).


```python
switcher = partial(AdaptiveGANSwitcher, critic_thresh=0.65)
learn = GANLearner.from_learners(learn_gen, learn_crit, weights_gen=(1.,50.), show_img=False, switcher=switcher,
                                 opt_func=partial(optim.Adam, betas=(0.,0.99)), wd=wd)
learn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.))
```



The code is a little daunting, but all we have to know for now are as follows:

- `GANLearner` takes on a pre-trained generator and pre-trained critic and does the iterative loop where we train generator, then critic, then generator (with the new critic as loss), then critic again and so forth.  
- `weights_gen` this multiply the **Pixel MSE** loss by 50, since it's on a different scale from **Critic** loss, and yes both losses are added and used together because if only critic were used, the prediction might look like a real image, but look very different from the original image.
- `betas=(0.,...)` GAN does not work well with momentum, and hence when we use `Adam` optimizer, we will need to set momentum to be 0.

*TL;DR: Just use the hyperparameters in the code and it should work just fine.*





```python
lr = 1e-4
```


```python
learn.fit(40,lr)
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>gen_loss</th>
      <th>disc_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>2.182588</td>
      <td>1.948150</td>
      <td>03:30</td>
    </tr>
    <tr>
      <td>1</td>
      <td>2.033133</td>
      <td>1.269898</td>
      <td>03:30</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.972908</td>
      <td>2.235838</td>
      <td>03:31</td>
    </tr>
    <tr>
      <td>3</td>
      <td>2.102590</td>
      <td>1.941931</td>
      <td>03:28</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.985073</td>
      <td>1.763344</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>5</td>
      <td>2.035091</td>
      <td>1.814058</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>6</td>
      <td>2.056103</td>
      <td>2.230835</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>7</td>
      <td>2.036760</td>
      <td>1.883716</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>8</td>
      <td>2.093678</td>
      <td>1.836209</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>9</td>
      <td>1.982500</td>
      <td>1.848118</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>10</td>
      <td>2.060856</td>
      <td>1.920196</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>11</td>
      <td>2.090670</td>
      <td>1.354180</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>12</td>
      <td>2.038133</td>
      <td>1.877020</td>
      <td>03:30</td>
    </tr>
    <tr>
      <td>13</td>
      <td>2.062729</td>
      <td>1.992053</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>14</td>
      <td>1.988933</td>
      <td>1.874720</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>15</td>
      <td>2.076017</td>
      <td>1.827738</td>
      <td>03:28</td>
    </tr>
    <tr>
      <td>16</td>
      <td>2.034793</td>
      <td>2.173032</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>17</td>
      <td>2.022686</td>
      <td>2.030897</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>18</td>
      <td>2.015309</td>
      <td>1.854405</td>
      <td>03:28</td>
    </tr>
    <tr>
      <td>19</td>
      <td>1.987776</td>
      <td>2.060545</td>
      <td>03:30</td>
    </tr>
    <tr>
      <td>20</td>
      <td>2.051640</td>
      <td>1.979111</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>21</td>
      <td>2.098156</td>
      <td>1.842055</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>22</td>
      <td>1.961414</td>
      <td>2.048688</td>
      <td>03:30</td>
    </tr>
    <tr>
      <td>23</td>
      <td>2.066474</td>
      <td>2.150644</td>
      <td>03:28</td>
    </tr>
    <tr>
      <td>24</td>
      <td>2.039973</td>
      <td>1.952134</td>
      <td>03:30</td>
    </tr>
    <tr>
      <td>25</td>
      <td>2.023382</td>
      <td>2.178393</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>26</td>
      <td>2.075311</td>
      <td>2.043443</td>
      <td>03:30</td>
    </tr>
    <tr>
      <td>27</td>
      <td>1.999097</td>
      <td>2.346807</td>
      <td>03:30</td>
    </tr>
    <tr>
      <td>28</td>
      <td>2.069972</td>
      <td>1.995587</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>29</td>
      <td>1.988542</td>
      <td>1.790482</td>
      <td>03:30</td>
    </tr>
    <tr>
      <td>30</td>
      <td>2.044225</td>
      <td>2.094155</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>31</td>
      <td>2.046011</td>
      <td>1.834003</td>
      <td>03:30</td>
    </tr>
    <tr>
      <td>32</td>
      <td>1.985295</td>
      <td>1.761712</td>
      <td>03:30</td>
    </tr>
    <tr>
      <td>33</td>
      <td>2.073957</td>
      <td>1.811233</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>34</td>
      <td>2.080958</td>
      <td>1.921561</td>
      <td>03:28</td>
    </tr>
    <tr>
      <td>35</td>
      <td>1.989938</td>
      <td>2.155618</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>36</td>
      <td>2.066681</td>
      <td>1.817710</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>37</td>
      <td>2.045019</td>
      <td>1.884791</td>
      <td>03:28</td>
    </tr>
    <tr>
      <td>38</td>
      <td>2.113732</td>
      <td>2.247601</td>
      <td>03:29</td>
    </tr>
    <tr>
      <td>39</td>
      <td>2.032242</td>
      <td>2.048030</td>
      <td>03:29</td>
    </tr>
  </tbody>
</table>


The thing about all these loss is that they are not important, because when the loss for generator is low, the loss for critic will be high, and vice versa, so really, it's hard to put a number to evaluate how good the the **GAN** model is, so it's better to see the prediction output and compare it with the real images.


```python
learn.save('gan-1c')
```


```python
learn.data=get_data(16,192)
```


```python
learn.fit(10,lr/2)
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>gen_loss</th>
      <th>disc_loss</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>2.658895</td>
      <td>2.717649</td>
      <td>08:48</td>
    </tr>
    <tr>
      <td>1</td>
      <td>2.607832</td>
      <td>2.779973</td>
      <td>08:35</td>
    </tr>
    <tr>
      <td>2</td>
      <td>2.696339</td>
      <td>2.837824</td>
      <td>08:34</td>
    </tr>
    <tr>
      <td>3</td>
      <td>2.704093</td>
      <td>2.735073</td>
      <td>08:35</td>
    </tr>
    <tr>
      <td>4</td>
      <td>2.649220</td>
      <td>2.418239</td>
      <td>08:36</td>
    </tr>
    <tr>
      <td>5</td>
      <td>2.570500</td>
      <td>2.804809</td>
      <td>08:36</td>
    </tr>
    <tr>
      <td>6</td>
      <td>2.769950</td>
      <td>2.512039</td>
      <td>08:32</td>
    </tr>
    <tr>
      <td>7</td>
      <td>2.684983</td>
      <td>2.612129</td>
      <td>08:33</td>
    </tr>
    <tr>
      <td>8</td>
      <td>2.673535</td>
      <td>2.444972</td>
      <td>08:35</td>
    </tr>
    <tr>
      <td>9</td>
      <td>2.679748</td>
      <td>2.644453</td>
      <td>08:35</td>
    </tr>
  </tbody>
</table>



```python
learn.show_results(rows=16)
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/pets-gan/output_91_0.png" alt="">



```python
learn.save('gan-1c')
```

We can see that the generated images are sharpening up quite nicely, it is not always great, because if the input is of very very bad quality, it would be hard to really sharpen that up, but this is definitely a lot better than what we had earlier using only **Pixel MSE**.

However, there is still a discernible pattern here, the generated images are pretty bad at outlining the eyeball of the pets, that is because the critic does not know that eyeballs are particularly important feature, it is easy for us to tell what is not right about the generated images, it's definitely a shortcoming of this approach, on the next blogpost, we will talk about how we can try to overcome this issue.

Later!

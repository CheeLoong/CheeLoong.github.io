---
title: "86% AUPRC Credit Card Fraud dataset with fully connected neural network"
date: 2019-08-03
permalink: /ccfraud-nn
tags: [fastai, credit card fraud, deep learning]
excerpt: "Using fastai's TabularLearner to build predictive model on cc fraud dataset"
mathjax: "true"
---


```python
!curl -s https://course.fast.ai/setup/colab | bash
```

    Updating fastai...
    Done.



```python
%reload_ext autoreload
%autoreload 2
%matplotlib inline
```


```python
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score

from fastai.tabular import *
```

## Look at data


```python
d_path = "/content/data/"
folder = "cc-fraud"

path = Path(d_path + folder)
path.mkdir(parents=True, exist_ok=True)
```


```python
df = pd.read_csv(path/'creditcard.csv')
```


```python
df.shape
```




    (284807, 31)




```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Time</th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>V10</th>
      <th>V11</th>
      <th>V12</th>
      <th>V13</th>
      <th>V14</th>
      <th>V15</th>
      <th>V16</th>
      <th>V17</th>
      <th>V18</th>
      <th>V19</th>
      <th>V20</th>
      <th>V21</th>
      <th>V22</th>
      <th>V23</th>
      <th>V24</th>
      <th>V25</th>
      <th>V26</th>
      <th>V27</th>
      <th>V28</th>
      <th>Amount</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>-1.359807</td>
      <td>-0.072781</td>
      <td>2.536347</td>
      <td>1.378155</td>
      <td>-0.338321</td>
      <td>0.462388</td>
      <td>0.239599</td>
      <td>0.098698</td>
      <td>0.363787</td>
      <td>0.090794</td>
      <td>-0.551600</td>
      <td>-0.617801</td>
      <td>-0.991390</td>
      <td>-0.311169</td>
      <td>1.468177</td>
      <td>-0.470401</td>
      <td>0.207971</td>
      <td>0.025791</td>
      <td>0.403993</td>
      <td>0.251412</td>
      <td>-0.018307</td>
      <td>0.277838</td>
      <td>-0.110474</td>
      <td>0.066928</td>
      <td>0.128539</td>
      <td>-0.189115</td>
      <td>0.133558</td>
      <td>-0.021053</td>
      <td>149.62</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>1.191857</td>
      <td>0.266151</td>
      <td>0.166480</td>
      <td>0.448154</td>
      <td>0.060018</td>
      <td>-0.082361</td>
      <td>-0.078803</td>
      <td>0.085102</td>
      <td>-0.255425</td>
      <td>-0.166974</td>
      <td>1.612727</td>
      <td>1.065235</td>
      <td>0.489095</td>
      <td>-0.143772</td>
      <td>0.635558</td>
      <td>0.463917</td>
      <td>-0.114805</td>
      <td>-0.183361</td>
      <td>-0.145783</td>
      <td>-0.069083</td>
      <td>-0.225775</td>
      <td>-0.638672</td>
      <td>0.101288</td>
      <td>-0.339846</td>
      <td>0.167170</td>
      <td>0.125895</td>
      <td>-0.008983</td>
      <td>0.014724</td>
      <td>2.69</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>-1.358354</td>
      <td>-1.340163</td>
      <td>1.773209</td>
      <td>0.379780</td>
      <td>-0.503198</td>
      <td>1.800499</td>
      <td>0.791461</td>
      <td>0.247676</td>
      <td>-1.514654</td>
      <td>0.207643</td>
      <td>0.624501</td>
      <td>0.066084</td>
      <td>0.717293</td>
      <td>-0.165946</td>
      <td>2.345865</td>
      <td>-2.890083</td>
      <td>1.109969</td>
      <td>-0.121359</td>
      <td>-2.261857</td>
      <td>0.524980</td>
      <td>0.247998</td>
      <td>0.771679</td>
      <td>0.909412</td>
      <td>-0.689281</td>
      <td>-0.327642</td>
      <td>-0.139097</td>
      <td>-0.055353</td>
      <td>-0.059752</td>
      <td>378.66</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.0</td>
      <td>-0.966272</td>
      <td>-0.185226</td>
      <td>1.792993</td>
      <td>-0.863291</td>
      <td>-0.010309</td>
      <td>1.247203</td>
      <td>0.237609</td>
      <td>0.377436</td>
      <td>-1.387024</td>
      <td>-0.054952</td>
      <td>-0.226487</td>
      <td>0.178228</td>
      <td>0.507757</td>
      <td>-0.287924</td>
      <td>-0.631418</td>
      <td>-1.059647</td>
      <td>-0.684093</td>
      <td>1.965775</td>
      <td>-1.232622</td>
      <td>-0.208038</td>
      <td>-0.108300</td>
      <td>0.005274</td>
      <td>-0.190321</td>
      <td>-1.175575</td>
      <td>0.647376</td>
      <td>-0.221929</td>
      <td>0.062723</td>
      <td>0.061458</td>
      <td>123.50</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2.0</td>
      <td>-1.158233</td>
      <td>0.877737</td>
      <td>1.548718</td>
      <td>0.403034</td>
      <td>-0.407193</td>
      <td>0.095921</td>
      <td>0.592941</td>
      <td>-0.270533</td>
      <td>0.817739</td>
      <td>0.753074</td>
      <td>-0.822843</td>
      <td>0.538196</td>
      <td>1.345852</td>
      <td>-1.119670</td>
      <td>0.175121</td>
      <td>-0.451449</td>
      <td>-0.237033</td>
      <td>-0.038195</td>
      <td>0.803487</td>
      <td>0.408542</td>
      <td>-0.009431</td>
      <td>0.798278</td>
      <td>-0.137458</td>
      <td>0.141267</td>
      <td>-0.206010</td>
      <td>0.502292</td>
      <td>0.219422</td>
      <td>0.215153</td>
      <td>69.99</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



#### Missing Data


```python
# check for missing value
df.isnull().mean() * 100
```




    Time      0.0
    V1        0.0
    V2        0.0
    V3        0.0
    V4        0.0
    V5        0.0
    V6        0.0
    V7        0.0
    V8        0.0
    V9        0.0
    V10       0.0
    V11       0.0
    V12       0.0
    V13       0.0
    V14       0.0
    V15       0.0
    V16       0.0
    V17       0.0
    V18       0.0
    V19       0.0
    V20       0.0
    V21       0.0
    V22       0.0
    V23       0.0
    V24       0.0
    V25       0.0
    V26       0.0
    V27       0.0
    V28       0.0
    Amount    0.0
    Class     0.0
    dtype: float64



no missing value, awesome!

#### Class Distributions


```python
# Graph
my_pal = {0: 'deepskyblue', 1: 'deeppink'}

plt.figure(figsize = (8, 5))
ax = sns.countplot(x = df['Class'], palette = my_pal)
plt.title('Class Distribution')
plt.show()

# Count and %
Count_Normal_transacation = len(df[df['Class']==0])
Count_Fraud_transacation = len(df[df['Class']==1])

Percentage_of_Normal_transacation = Count_Normal_transacation/(Count_Normal_transacation+Count_Fraud_transacation)
print('Percentage of normal transacation       :', "{0:.2f}%".format(Percentage_of_Normal_transacation*100))
print('Number of normal transaction            :', Count_Normal_transacation)

Percentage_of_Fraud_transacation= Count_Fraud_transacation/(Count_Normal_transacation+Count_Fraud_transacation)
print('Percentage of fraud transacation        :', "{0:.2f}%".format(Percentage_of_Fraud_transacation*100))
print('Number of fraud transaction             :', Count_Fraud_transacation)
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/cc-fraud/output_12_0.png" alt="">


    Percentage of normal transacation       : 99.83%
    Number of normal transaction            : 284315
    Percentage of fraud transacation        : 0.17%
    Number of fraud transaction             : 492


#### Class vs Amount


```python
print ("Fraud")
print (df.Amount[df.Class == 1].describe())
print ()
print ("Normal")
print (df.Amount[df.Class == 0].describe())
```

    Fraud
    count     492.000000
    mean      122.211321
    std       256.683288
    min         0.000000
    25%         1.000000
    50%         9.250000
    75%       105.890000
    max      2125.870000
    Name: Amount, dtype: float64

    Normal
    count    284315.000000
    mean         88.291022
    std         250.105092
    min           0.000000
    25%           5.650000
    50%          22.000000
    75%          77.050000
    max       25691.160000
    Name: Amount, dtype: float64



```python
f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,8))

bins = 30

ax1.hist(df.Amount[df.Class == 1], bins = bins, color = 'deeppink')
ax1.set_title('Fraud')

ax2.hist(df.Amount[df.Class == 0], bins = bins, color = 'deepskyblue')
ax2.set_title('Normal')

plt.xlabel('Transaction Amount ($)')
plt.ylabel('Number of Transactions')
plt.yscale('log')
plt.show()
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/cc-fraud/output_15_0.png" alt="">


Looking at this, the fraud transactions seem to be at a smaller scale in comparison with normal transactions.

Fraudulent transactions have a maximum value far less than the maximum value for  normal transactions, \$2,125.87 vs \$25,691.16.

#### Class vs Time


```python
f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,8))

bins = 50

ax1.hist(df.Time[df.Class == 1], bins = bins, color = 'deeppink')
ax1.set_title('Fraud')

ax2.hist(df.Time[df.Class == 0], bins = bins, color = 'deepskyblue')
ax2.set_title('Normal')

plt.xlabel('Time (in Seconds)')
plt.ylabel('Number of Transactions')
plt.show()
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/cc-fraud/output_18_0.png" alt="">


There's not really a clear pattern in the fraud transactions, but in the normal transactions we can see that transactions happened more frequently in certain period of time, its safe to assume the troughs are when the customers are resting (most probably midnight), which explains less transactions.

We will drop this feature.

## Loss Function

This is a binary classification problem, that means we use `CrossEntropyLoss` as the loss function. Although some would argue because this dataset is highly class-imbalanced, it'd be better to use `Dice` as the loss function, because in this case, we are not optimizing for higher accuracy, but a higher AUROC.

**Why aren't we optimizing for a higher accuracy but a higher AUROC?**

This dataset has only 0.17% fraud transactions, when we build a model to optimize for accuracy, the model would be motivated to predict every transaciton as non-fraud, returning a high accuracy of 99.83%, the problem is, we still could not identify the fraud transactions, that would lead to high False Negative (model predicts normal transaction but ground truth is fraud transaction), and low True Positive (since we did not predict any fraud), which results in an extremely bad Recall metric.

Practically speaking, we would want to optimize Area Under Receiver operating characteristic (AUROC), and if we can get a high AUROC, that would mean the model achieve a low False Positive Rate (FPR) and a high True Positive Rate (TPR).

However, we would still stick with the good ol' `CrossEntropyLoss` as the loss function, and if it fails to optimize for a high AUROC, we would try a different loss function.

By default, fast.ai library computes metrics for each batch and then take the average across all the batches which makes sense for most metrics, just not AUROC, because it's required to be computed on the entire dataset.

Here's how to implement a `Callback` function to compute the AUROC.


```python
def auroc_score(input, target):
    input, target = input.cpu().numpy()[:,1], target.cpu().numpy()
    return roc_auc_score(target, input)

class AUROC(Callback):
    _order = -20 #Needs to run before the recorder

    def __init__(self, learn, **kwargs): self.learn = learn
    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['AUROC'])
    def on_epoch_begin(self, **kwargs): self.output, self.target = [], []

    def on_batch_end(self, last_target, last_output, train, **kwargs):
        if not train:
            self.output.append(last_output)
            self.target.append(last_target)

    def on_epoch_end(self, last_metrics, **kwargs):
        if len(self.output) > 0:
            output = torch.cat(self.output)
            target = torch.cat(self.target)
            preds = F.softmax(output, dim=1)
            metric = auroc_score(preds, target)
            return add_metrics(last_metrics, [metric])
```


```python
def auprc_score(input, target):
    input, target = input.cpu().numpy()[:,1], target.cpu().numpy()
    return average_precision_score(target, input)

class AUPRC(Callback):
    _order = -20 #Needs to run before the recorder

    def __init__(self, learn, **kwargs): self.learn = learn
    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['AUPRC'])
    def on_epoch_begin(self, **kwargs): self.output, self.target = [], []

    def on_batch_end(self, last_target, last_output, train, **kwargs):
        if not train:
            self.output.append(last_output)
            self.target.append(last_target)

    def on_epoch_end(self, last_metrics, **kwargs):
        if len(self.output) > 0:
            output = torch.cat(self.output)
            target = torch.cat(self.target)
            preds = F.softmax(output, dim=1)
            metric = auprc_score(preds, target)
            return add_metrics(last_metrics, [metric])
```

## Preparing `TabularDataBunch`

I know that we can do data split with `TabularList.split_by_rand_pct()`, but since I will be doing SMOTE-oversampling on the train set after the first stage of modelling, I would like to access the train set in `pd.dataframe` easily.

This is basically because I can't figure out how to convert `TabularLabelList` to `pd.dataframe`, so as a temporary fix, I will be doing the split with `TabularList.split_by_idx()`.


```python
from sklearn.model_selection import train_test_split
```


```python
# create a copy of original df
df_ori = df.copy()

# drop unwanted feature
df = df.drop('Time', axis = 1)
```


```python
train, valid = train_test_split(df, test_size = 0.2, random_state = 69)
```


```python
train.shape
```




    (227845, 30)




```python
# these are the indices for validation set
valid_idx = valid.index
```


```python
# assign continuous variable names
cont_vars = []

for i in df.columns:
  if i not in ('Time', 'Class') :
    cont_vars.append(i)
```


```python
# assign dependant variable name
dep_var = 'Class'

# assign preprocessors that will be used
procs=[Normalize]
```


```python
data = (TabularList.from_df(df, path=path, cont_names=cont_vars, procs=procs)
                .split_by_idx(valid_idx) # split with idx
                .label_from_df(cols=dep_var) # label with dependant variable
                .databunch())
```


```python
data
```




    TabularDataBunch;

    Train: LabelList (227845 items)
    x: TabularList
    V1 -0.6947; V2 -0.0440; V3 1.6693; V4 0.9741; V5 -0.2477; V6 0.3504; V7 0.1962; V8 0.0824; V9 0.3320; V10 0.0845; V11 -0.5398; V12 -0.6189; V13 -0.9966; V14 -0.3232; V15 1.6037; V16 -0.5350; V17 0.2428; V18 0.0330; V19 0.4956; V20 0.3287; V21 -0.0239; V22 0.3830; V23 -0.1761; V24 0.1107; V25 0.2451; V26 -0.3939; V27 0.3357; V28 -0.0646; Amount 0.2509; ,V1 0.6086; V2 0.1618; V3 0.1088; V4 0.3165; V5 0.0448; V6 -0.0611; V7 -0.0640; V8 0.0710; V9 -0.2315; V10 -0.1518; V11 1.5806; V12 1.0644; V13 0.4938; V14 -0.1488; V15 0.6943; V16 0.5303; V17 -0.1351; V18 -0.2159; V19 -0.1796; V20 -0.0913; V21 -0.3076; V22 -0.8808; V23 0.1630; V24 -0.5607; V25 0.3192; V26 0.2586; V27 -0.0219; V28 0.0453; Amount -0.3501; ,V1 -0.6939; V2 -0.8137; V3 1.1668; V4 0.2682; V5 -0.3688; V6 1.3611; V7 0.6473; V8 0.2073; V9 -1.3774; V10 0.1916; V11 0.6124; V12 0.0651; V13 0.7235; V14 -0.1719; V15 2.5624; V16 -3.2940; V17 1.2989; V18 -0.1421; V19 -2.7786; V20 0.6873; V21 0.3403; V22 1.0640; V23 1.4573; V24 -1.1373; V25 -0.6293; V26 -0.2903; V27 -0.1382; V28 -0.1835; Amount 1.1877; ,V1 -0.4937; V2 -0.1123; V3 1.1799; V4 -0.6108; V5 -0.0068; V6 0.9432; V7 0.1946; V8 0.3161; V9 -1.2613; V10 -0.0491; V11 -0.2213; V12 0.1772; V13 0.5126; V14 -0.2990; V15 -0.6896; V16 -1.2069; V17 -0.8016; V18 2.3420; V19 -1.5145; V20 -0.2734; V21 -0.1470; V22 0.0072; V23 -0.3040; V24 -1.9399; V25 1.2397; V26 -0.4618; V27 0.1580; V28 0.1889; Amount 0.1441; ,V1 -0.5917; V2 0.5333; V3 1.0190; V4 0.2846; V5 -0.2983; V6 0.0736; V7 0.4850; V8 -0.2271; V9 0.7451; V10 0.6917; V11 -0.8056; V12 0.5372; V13 1.3563; V14 -1.1658; V15 0.1913; V16 -0.5134; V17 -0.2782; V18 -0.0431; V19 0.9863; V20 0.5347; V21 -0.0118; V22 1.1007; V23 -0.2193; V24 0.2333; V25 -0.3961; V26 1.0382; V27 0.5511; V28 0.6612; Amount -0.0748;
    y: CategoryList
    0,0,0,0,0
    Path: /content/data/cc-fraud;

    Valid: LabelList (56962 items)
    x: TabularList
    V1 0.9228; V2 -0.1635; V3 -0.8202; V4 0.5154; V5 -0.1648; V6 -0.5922; V7 -0.0880; V8 -0.0471; V9 0.4973; V10 -0.2481; V11 1.2209; V12 0.5560; V13 -0.6759; V14 -0.8217; V15 -0.2466; V16 0.7090; V17 0.4776; V18 0.7146; V19 -0.1830; V20 -0.0788; V21 -0.0070; V22 -0.1038; V23 0.2416; V24 -0.1304; V25 -0.6024; V26 0.3998; V27 -0.1060; V28 -0.0714; Amount -0.0170; ,V1 -4.5882; V2 -7.4286; V3 0.1306; V4 2.1969; V5 7.9789; V6 -5.4794; V7 -6.7136; V8 -0.3714; V9 2.1204; V10 1.2868; V11 -1.1452; V12 1.0960; V13 0.8204; V14 -0.1920; V15 0.9390; V16 0.4448; V17 -0.5792; V18 0.0108; V19 0.6726; V20 -2.8780; V21 0.6182; V22 1.0050; V23 1.3027; V24 1.5440; V25 -3.2568; V26 1.0150; V27 5.4197; V28 -1.1852; Amount -0.2435; ,V1 0.4530; V2 -0.0417; V3 0.0460; V4 0.9805; V5 -0.1611; V6 -0.5249; V7 0.4339; V8 -0.1795; V9 -0.2452; V10 -0.0389; V11 0.1496; V12 0.4797; V13 0.1076; V14 0.5386; V15 1.0538; V16 -0.4722; V17 0.0199; V18 -1.0210; V19 -1.0666; V20 0.1747; V21 0.1576; V22 0.0818; V23 -0.2552; V24 0.6945; V25 1.0628; V26 -0.7274; V27 -0.0223; V28 0.1358; Amount 0.2889; ,V1 0.6257; V2 -1.0262; V3 0.8001; V4 -0.8176; V5 -1.6045; V6 0.2268; V7 -1.4328; V8 0.3019; V9 -0.9981; V10 1.2748; V11 0.9494; V12 -0.5794; V13 -1.1454; V14 -0.4384; V15 -0.5790; V16 -0.6407; V17 1.0711; V18 0.2641; V19 -0.2932; V20 -0.4282; V21 -0.1725; V22 -0.0187; V23 0.1109; V24 0.2904; V25 0.0842; V26 -0.4111; V27 0.1772; V28 0.0898; Amount -0.0420; ,V1 -0.7202; V2 -0.1832; V3 0.6976; V4 0.8371; V5 -0.3475; V6 0.6727; V7 1.1540; V8 -0.6880; V9 0.8452; V10 1.3212; V11 1.4006; V12 0.1412; V13 -0.7904; V14 -0.7679; V15 0.3766; V16 -0.5589; V17 -0.5807; V18 0.0648; V19 0.4658; V20 -0.7160; V21 -0.2182; V22 1.0078; V23 -0.8646; V24 0.1347; V25 -1.8977; V26 -1.2328; V27 -3.3788; V28 -3.1721; Amount 0.6127;
    y: CategoryList
    0,0,0,0,0
    Path: /content/data/cc-fraud;

    Test: None




```python
data.show_batch()
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>V10</th>
      <th>V11</th>
      <th>V12</th>
      <th>V13</th>
      <th>V14</th>
      <th>V15</th>
      <th>V16</th>
      <th>V17</th>
      <th>V18</th>
      <th>V19</th>
      <th>V20</th>
      <th>V21</th>
      <th>V22</th>
      <th>V23</th>
      <th>V24</th>
      <th>V25</th>
      <th>V26</th>
      <th>V27</th>
      <th>V28</th>
      <th>Amount</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>-0.3956</td>
      <td>-0.6241</td>
      <td>1.0200</td>
      <td>-0.1749</td>
      <td>0.7337</td>
      <td>0.0886</td>
      <td>-0.5342</td>
      <td>0.1367</td>
      <td>1.0532</td>
      <td>-0.4630</td>
      <td>-1.7133</td>
      <td>-0.1998</td>
      <td>0.4504</td>
      <td>-0.7074</td>
      <td>1.1026</td>
      <td>0.5611</td>
      <td>-1.2675</td>
      <td>1.2994</td>
      <td>0.1971</td>
      <td>0.5739</td>
      <td>0.5398</td>
      <td>1.4577</td>
      <td>0.3163</td>
      <td>0.0665</td>
      <td>-1.7079</td>
      <td>-0.7118</td>
      <td>0.1935</td>
      <td>0.1522</td>
      <td>-0.1166</td>
      <td>0</td>
    </tr>
    <tr>
      <td>-0.6321</td>
      <td>0.8145</td>
      <td>-0.2279</td>
      <td>-1.4262</td>
      <td>-0.2638</td>
      <td>-0.8205</td>
      <td>0.1855</td>
      <td>0.6602</td>
      <td>0.4768</td>
      <td>-1.2312</td>
      <td>0.4232</td>
      <td>1.3861</td>
      <td>-0.2318</td>
      <td>0.9396</td>
      <td>-0.9079</td>
      <td>-0.5026</td>
      <td>-0.2471</td>
      <td>0.5516</td>
      <td>0.6464</td>
      <td>-0.2016</td>
      <td>0.2270</td>
      <td>0.8001</td>
      <td>-0.2277</td>
      <td>0.0435</td>
      <td>0.0778</td>
      <td>-1.6745</td>
      <td>0.7413</td>
      <td>0.4987</td>
      <td>-0.3304</td>
      <td>0</td>
    </tr>
    <tr>
      <td>-0.2100</td>
      <td>0.5658</td>
      <td>0.8920</td>
      <td>0.0781</td>
      <td>-0.1032</td>
      <td>-0.5652</td>
      <td>0.4078</td>
      <td>0.1401</td>
      <td>-0.0833</td>
      <td>-0.4511</td>
      <td>-0.0673</td>
      <td>-1.0437</td>
      <td>-2.1448</td>
      <td>0.0544</td>
      <td>1.4860</td>
      <td>0.1122</td>
      <td>0.6191</td>
      <td>-0.5056</td>
      <td>-0.6858</td>
      <td>-0.0860</td>
      <td>-0.3257</td>
      <td>-0.9556</td>
      <td>0.1353</td>
      <td>0.4776</td>
      <td>-0.6720</td>
      <td>0.1814</td>
      <td>0.6226</td>
      <td>0.2877</td>
      <td>-0.3427</td>
      <td>0</td>
    </tr>
    <tr>
      <td>-0.3934</td>
      <td>0.4163</td>
      <td>0.9341</td>
      <td>-0.6758</td>
      <td>-0.1787</td>
      <td>-0.2763</td>
      <td>0.1851</td>
      <td>-0.6427</td>
      <td>0.4703</td>
      <td>-0.7754</td>
      <td>-0.5089</td>
      <td>0.4297</td>
      <td>-0.4024</td>
      <td>-0.3285</td>
      <td>-0.9595</td>
      <td>-0.5857</td>
      <td>0.3406</td>
      <td>-1.3914</td>
      <td>-0.9432</td>
      <td>-0.5378</td>
      <td>1.1369</td>
      <td>0.1521</td>
      <td>0.1647</td>
      <td>0.7880</td>
      <td>-1.3431</td>
      <td>1.6535</td>
      <td>0.0051</td>
      <td>0.5208</td>
      <td>-0.3202</td>
      <td>0</td>
    </tr>
    <tr>
      <td>0.6340</td>
      <td>-0.0313</td>
      <td>0.3811</td>
      <td>-0.0820</td>
      <td>-0.4248</td>
      <td>-0.4131</td>
      <td>-0.2200</td>
      <td>-0.0348</td>
      <td>0.3224</td>
      <td>-0.2407</td>
      <td>0.0381</td>
      <td>0.4423</td>
      <td>0.3671</td>
      <td>0.1131</td>
      <td>1.6120</td>
      <td>0.1172</td>
      <td>-0.1177</td>
      <td>-1.0678</td>
      <td>-0.4078</td>
      <td>-0.1085</td>
      <td>-0.1409</td>
      <td>-0.3277</td>
      <td>0.2000</td>
      <td>0.2373</td>
      <td>0.1015</td>
      <td>1.9310</td>
      <td>-0.1310</td>
      <td>0.0200</td>
      <td>-0.3548</td>
      <td>0</td>
    </tr>
  </tbody>
</table>


## Modelling with `TabularLearner`

Alright I am going to use a for loop to look at what number of layers would provide a good looking learning rate curve, based on my experience, we want a learning rate curve where it has a sharp decrease on the loss.


```python
layerlist = [[100,50], [200,100], [400,200], [600,300], [800,400], [1000,500]]
```


```python
for layer in layerlist:
  learn = tabular_learner(data, layers = layer,  metrics = [accuracy, Precision(), Recall(), FBeta(beta=1)], callback_fns = [AUROC,AUPRC])
  learn.lr_find()
  learn.recorder.plot()
```

    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/cc-fraud/output_39_12.png" alt="">



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/cc-fraud/output_39_13.png" alt="">



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/cc-fraud/output_39_14.png" alt="">



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/cc-fraud/output_39_15.png" alt="">



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/cc-fraud/output_39_16.png" alt="">



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/cc-fraud/output_39_17.png" alt="">


After looking at these graphs, I have decided to use `5e-03` as my max learning rate.


```python
selected_layer = [[600,300], [800,400], [1000,500]]

for layer in selected_layer:
  learn = tabular_learner(data, layers = layer,  metrics = [accuracy, Precision(), Recall(), FBeta(beta=1)], callback_fns = [AUROC,AUPRC])
  learn.fit_one_cycle(10, 5e-03)
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>precision</th>
      <th>recall</th>
      <th>f_beta</th>
      <th>AUROC</th>
      <th>AUPRC</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.003598</td>
      <td>0.004439</td>
      <td>0.999596</td>
      <td>0.864198</td>
      <td>0.853659</td>
      <td>0.858896</td>
      <td>0.984209</td>
      <td>0.775629</td>
      <td>00:43</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.005307</td>
      <td>0.007891</td>
      <td>0.999192</td>
      <td>0.666667</td>
      <td>0.878049</td>
      <td>0.757895</td>
      <td>0.978987</td>
      <td>0.761501</td>
      <td>00:44</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.001893</td>
      <td>0.008436</td>
      <td>0.999491</td>
      <td>0.797753</td>
      <td>0.865854</td>
      <td>0.830409</td>
      <td>0.977078</td>
      <td>0.765808</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.003067</td>
      <td>0.002717</td>
      <td>0.999579</td>
      <td>0.881579</td>
      <td>0.817073</td>
      <td>0.848101</td>
      <td>0.975550</td>
      <td>0.838252</td>
      <td>00:44</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.003770</td>
      <td>0.005000</td>
      <td>0.999544</td>
      <td>0.825581</td>
      <td>0.865854</td>
      <td>0.845238</td>
      <td>0.974346</td>
      <td>0.743949</td>
      <td>00:44</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.002645</td>
      <td>0.003442</td>
      <td>0.999456</td>
      <td>0.780220</td>
      <td>0.865854</td>
      <td>0.820809</td>
      <td>0.982137</td>
      <td>0.825189</td>
      <td>00:44</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.002687</td>
      <td>0.018474</td>
      <td>0.999315</td>
      <td>0.717172</td>
      <td>0.865854</td>
      <td>0.784530</td>
      <td>0.978218</td>
      <td>0.777765</td>
      <td>00:44</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.001946</td>
      <td>0.006552</td>
      <td>0.999491</td>
      <td>0.797753</td>
      <td>0.865854</td>
      <td>0.830409</td>
      <td>0.984833</td>
      <td>0.776976</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.003688</td>
      <td>0.003928</td>
      <td>0.999526</td>
      <td>0.823529</td>
      <td>0.853659</td>
      <td>0.838323</td>
      <td>0.985346</td>
      <td>0.818316</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.001122</td>
      <td>0.004679</td>
      <td>0.999350</td>
      <td>0.747253</td>
      <td>0.829268</td>
      <td>0.786127</td>
      <td>0.979770</td>
      <td>0.790147</td>
      <td>00:44</td>
    </tr>
  </tbody>
</table>



<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>precision</th>
      <th>recall</th>
      <th>f_beta</th>
      <th>AUROC</th>
      <th>AUPRC</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.003439</td>
      <td>0.006582</td>
      <td>0.999544</td>
      <td>0.878378</td>
      <td>0.792683</td>
      <td>0.833333</td>
      <td>0.913929</td>
      <td>0.720606</td>
      <td>00:44</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.003292</td>
      <td>0.003187</td>
      <td>0.999596</td>
      <td>0.873418</td>
      <td>0.841463</td>
      <td>0.857143</td>
      <td>0.974743</td>
      <td>0.807881</td>
      <td>00:44</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.002966</td>
      <td>0.002695</td>
      <td>0.999561</td>
      <td>0.880000</td>
      <td>0.804878</td>
      <td>0.840764</td>
      <td>0.973683</td>
      <td>0.809673</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.005244</td>
      <td>0.004779</td>
      <td>0.999544</td>
      <td>0.825581</td>
      <td>0.865854</td>
      <td>0.845238</td>
      <td>0.983684</td>
      <td>0.804423</td>
      <td>00:44</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.004992</td>
      <td>0.003564</td>
      <td>0.999508</td>
      <td>0.806818</td>
      <td>0.865854</td>
      <td>0.835294</td>
      <td>0.982031</td>
      <td>0.836296</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.007164</td>
      <td>0.002676</td>
      <td>0.999579</td>
      <td>0.853659</td>
      <td>0.853659</td>
      <td>0.853659</td>
      <td>0.981667</td>
      <td>0.843839</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.002751</td>
      <td>0.002569</td>
      <td>0.999561</td>
      <td>0.851852</td>
      <td>0.841463</td>
      <td>0.846626</td>
      <td>0.979856</td>
      <td>0.844384</td>
      <td>00:44</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.005364</td>
      <td>0.002615</td>
      <td>0.999596</td>
      <td>0.864198</td>
      <td>0.853659</td>
      <td>0.858896</td>
      <td>0.983533</td>
      <td>0.858920</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.004878</td>
      <td>0.003315</td>
      <td>0.999508</td>
      <td>0.806818</td>
      <td>0.865854</td>
      <td>0.835294</td>
      <td>0.985405</td>
      <td>0.853140</td>
      <td>00:44</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.001753</td>
      <td>0.002849</td>
      <td>0.999579</td>
      <td>0.845238</td>
      <td>0.865854</td>
      <td>0.855422</td>
      <td>0.984126</td>
      <td>0.856853</td>
      <td>00:44</td>
    </tr>
  </tbody>
</table>



<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>precision</th>
      <th>recall</th>
      <th>f_beta</th>
      <th>AUROC</th>
      <th>AUPRC</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.008685</td>
      <td>0.008210</td>
      <td>0.999105</td>
      <td>0.637168</td>
      <td>0.878049</td>
      <td>0.738462</td>
      <td>0.988117</td>
      <td>0.749804</td>
      <td>00:44</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.007913</td>
      <td>0.006445</td>
      <td>0.999403</td>
      <td>0.750000</td>
      <td>0.878049</td>
      <td>0.808989</td>
      <td>0.984282</td>
      <td>0.772664</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.002646</td>
      <td>0.004408</td>
      <td>0.999544</td>
      <td>0.825581</td>
      <td>0.865854</td>
      <td>0.845238</td>
      <td>0.977243</td>
      <td>0.800446</td>
      <td>00:44</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.003896</td>
      <td>0.009129</td>
      <td>0.999333</td>
      <td>0.720000</td>
      <td>0.878049</td>
      <td>0.791209</td>
      <td>0.970474</td>
      <td>0.778278</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.009305</td>
      <td>1.617505</td>
      <td>0.997174</td>
      <td>0.311005</td>
      <td>0.792683</td>
      <td>0.446735</td>
      <td>0.977084</td>
      <td>0.211361</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.001253</td>
      <td>0.008271</td>
      <td>0.999263</td>
      <td>0.704082</td>
      <td>0.841463</td>
      <td>0.766667</td>
      <td>0.977742</td>
      <td>0.628094</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.002069</td>
      <td>0.025055</td>
      <td>0.999105</td>
      <td>0.659794</td>
      <td>0.780488</td>
      <td>0.715084</td>
      <td>0.979752</td>
      <td>0.526179</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.000480</td>
      <td>0.003182</td>
      <td>0.999596</td>
      <td>0.855422</td>
      <td>0.865854</td>
      <td>0.860606</td>
      <td>0.977341</td>
      <td>0.853228</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.001179</td>
      <td>0.002783</td>
      <td>0.999456</td>
      <td>0.780220</td>
      <td>0.865854</td>
      <td>0.820809</td>
      <td>0.980888</td>
      <td>0.870442</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.004084</td>
      <td>0.008425</td>
      <td>0.999438</td>
      <td>0.784091</td>
      <td>0.841463</td>
      <td>0.811765</td>
      <td>0.978811</td>
      <td>0.686384</td>
      <td>00:45</td>
    </tr>
  </tbody>
</table>


Out of all these layers, I see that `[800,400]` has a consistently high f1 and AUPRC, thus I will use that as the argument for the layers hyperparameter.


```python
learn = tabular_learner(data, layers = [800,400], ps=[0.001,0.01], metrics = [accuracy, Precision(), Recall(), FBeta(beta=1)], callback_fns = [AUROC,AUPRC])
learn.fit_one_cycle(8, 5e-03)
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>precision</th>
      <th>recall</th>
      <th>f_beta</th>
      <th>AUROC</th>
      <th>AUPRC</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.010900</td>
      <td>0.009469</td>
      <td>0.998912</td>
      <td>0.578125</td>
      <td>0.902439</td>
      <td>0.704762</td>
      <td>0.981869</td>
      <td>0.760759</td>
      <td>00:44</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.005955</td>
      <td>0.004094</td>
      <td>0.999544</td>
      <td>0.841463</td>
      <td>0.841463</td>
      <td>0.841463</td>
      <td>0.976766</td>
      <td>0.764774</td>
      <td>00:44</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.005444</td>
      <td>0.039538</td>
      <td>0.998402</td>
      <td>0.469799</td>
      <td>0.853659</td>
      <td>0.606061</td>
      <td>0.975324</td>
      <td>0.371037</td>
      <td>00:44</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.001903</td>
      <td>0.483022</td>
      <td>0.996700</td>
      <td>0.280992</td>
      <td>0.829268</td>
      <td>0.419753</td>
      <td>0.978598</td>
      <td>0.173835</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.004199</td>
      <td>0.002593</td>
      <td>0.999614</td>
      <td>0.875000</td>
      <td>0.853659</td>
      <td>0.864198</td>
      <td>0.976707</td>
      <td>0.848231</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.003883</td>
      <td>0.002478</td>
      <td>0.999579</td>
      <td>0.862500</td>
      <td>0.841463</td>
      <td>0.851852</td>
      <td>0.979984</td>
      <td>0.859510</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.002205</td>
      <td>0.002696</td>
      <td>0.999579</td>
      <td>0.853659</td>
      <td>0.853659</td>
      <td>0.853659</td>
      <td>0.976566</td>
      <td>0.860672</td>
      <td>00:45</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.004052</td>
      <td>0.002595</td>
      <td>0.999614</td>
      <td>0.865854</td>
      <td>0.865854</td>
      <td>0.865854</td>
      <td>0.977514</td>
      <td>0.864230</td>
      <td>00:45</td>
    </tr>
  </tbody>
</table>



```python
preds,y,losses = learn.get_preds(with_loss=True)
interp = ClassificationInterpretation(learn, preds, y, losses)
```


```python
interp.plot_confusion_matrix()
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/cc-fraud/output_45_0.png" alt="">


Considering the fact that is dataset is highly imbalanced, a precision of `0.8659` and recall of `0.8659` is actually very impressive, this gives an f1 score of `0.8659` and a AUPRC is `0.8642`

Question now is, can sampling technique help increase the performance of the model? Let's try SMOTE oversampling technique.

## SMOTE OverSampling

Let's try to upsample the minority class and see if we can get better model performance


```python
from imblearn.over_sampling import SMOTE
```

    /usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).
      "(https://pypi.org/project/six/).", DeprecationWarning)


Here's how SMOTE upsampling work, in our labels in the training set , we have 227,435 normal transactions and 410 fraud transactions, the upsampling will basically upsize the minority class to match the majority class, so there 410 fraud transactions will be increased up to 227,435 size as well, which result in a 454,870 length of data set with equal class distribution.


```python
X_train = train.drop('Class', axis = 1)
y_train = train['Class']

smote = SMOTE(ratio='minority', random_state = 0)
X_train_sm, y_train_sm = smote.fit_sample(X_train, y_train)

# convert arrays to dataframes
X_train_sm = pd.DataFrame(X_train_sm, columns = X_train.columns)
y_train_sm = pd.DataFrame(y_train_sm, columns = ['Class'])

# concat X and y
df_sm = pd.concat([X_train_sm, y_train_sm], axis=1)

# adding a column of boolean column to specify valid set
df_sm['is_valid'] = False
valid['is_valid'] = True
print('shape of SMOTE df:', df_sm.shape)
print('shape of valid df:', valid.shape)

# concat df_sm and valid
df_sm = pd.concat([df_sm, valid], axis=0)
print('shape of SMOTE df after concat with valid df:', df_sm.shape, '\n')
```

    shape of SMOTE df: (454870, 31)
    shape of valid df: (56962, 31)
    shape of SMOTE df after concat with valid df: (511832, 31)



    /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: SettingWithCopyWarning:
    A value is trying to be set on a copy of a slice from a DataFrame.
    Try using .loc[row_indexer,col_indexer] = value instead

    See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
      app.launch_new_instance()


Now because the SMOTE-ed dataset will be used to train, the valid set will still be the same as before, just to clarify again, the SMOTE-ed dataset has equal class distribution, but the valid set is still very much class-imbalanced.

To do what we proposed, we need to create a column of `is_valid` for `TabularList` to read.


```python
data_sm = (TabularList.from_df(df_sm, path=path, cont_names=cont_vars, procs=procs)
                .split_from_df(col = "is_valid") # split with idx
                .label_from_df(cols=dep_var) # label with dependant variable
                .databunch())
```


```python
data_sm
```




    TabularDataBunch;

    Train: LabelList (454870 items)
    x: TabularList
    V1 0.5684; V2 -0.8834; V3 0.4930; V4 -0.8337; V5 0.1320; V6 0.1514; V7 0.4680; V8 -0.1203; V9 0.1222; V10 0.7769; V11 -0.4716; V12 0.7534; V13 -0.1788; V14 0.8339; V15 -0.6539; V16 0.1683; V17 0.5534; V18 1.0363; V19 -0.3689; V20 -0.1271; V21 -0.2179; V22 -0.8011; V23 -0.2701; V24 0.2077; V25 0.4682; V26 2.3114; V27 -0.2193; V28 -0.0108; Amount 0.7885; ,V1 0.1217; V2 -0.9889; V3 0.9097; V4 -1.4037; V5 0.0033; V6 0.1093; V7 0.4082; V8 -0.0298; V9 -0.4313; V10 0.8129; V11 -0.2964; V12 0.5085; V13 -0.0796; V14 0.6716; V15 -0.2197; V16 0.6963; V17 0.5491; V18 0.9159; V19 -0.9285; V20 0.3670; V21 -0.0588; V22 0.0268; V23 0.5820; V24 0.9129; V25 0.4440; V26 -0.7657; V27 0.1105; V28 0.2897; Amount 0.6926; ,V1 -0.1048; V2 -0.0231; V3 0.6785; V4 -1.0306; V5 0.0818; V6 1.6204; V7 -0.2242; V8 -1.6282; V9 0.5024; V10 0.0663; V11 -0.7350; V12 0.8863; V13 -2.5896; V14 1.0552; V15 -1.5862; V16 0.8288; V17 0.6344; V18 0.4555; V19 -0.7890; V20 1.3886; V21 -1.7375; V22 0.8325; V23 0.4488; V24 -0.5423; V25 -0.7491; V26 1.7026; V27 0.0228; V28 0.1341; Amount -0.3568; ,V1 0.7742; V2 -0.5987; V3 0.5946; V4 -0.5202; V5 0.1277; V6 0.1423; V7 0.3003; V8 -0.0964; V9 1.7265; V10 0.5388; V11 -0.6471; V12 0.2231; V13 2.2201; V14 0.9919; V15 0.0348; V16 0.7774; V17 0.5731; V18 0.5884; V19 -0.6859; V20 -0.4014; V21 -0.1600; V22 0.0641; V23 0.3342; V24 0.1316; V25 -1.0718; V26 0.9220; V27 -0.1267; V28 -0.1909; Amount -0.3624; ,V1 -0.1515; V2 0.2414; V3 -0.0513; V4 -1.2599; V5 0.6034; V6 2.3054; V7 0.1858; V8 0.6589; V9 0.5092; V10 0.6111; V11 -1.0176; V12 0.8928; V13 0.0796; V14 1.1289; V15 -0.0512; V16 0.7133; V17 0.6219; V18 0.1632; V19 -0.5417; V20 -0.3061; V21 -0.1802; V22 -0.8334; V23 0.4167; V24 1.3388; V25 -0.3390; V26 0.2559; V27 -0.5733; V28 -0.8009; Amount -0.3789;
    y: CategoryList
    0,0,0,0,0
    Path: /content/data/cc-fraud;

    Valid: LabelList (56962 items)
    x: TabularList
    V1 0.7459; V2 -0.5597; V3 0.3506; V4 -0.4718; V5 0.3208; V6 -0.0517; V7 0.4459; V8 -0.0925; V9 0.8047; V10 0.5667; V11 -0.2289; V12 0.8062; V13 -0.6014; V14 0.5833; V15 -0.2136; V16 0.7777; V17 0.6281; V18 0.7301; V19 -0.3781; V20 -0.2678; V21 -0.1170; V22 -0.0892; V23 0.1678; V24 -0.0491; V25 -0.5757; V26 0.3793; V27 -0.1306; V28 -0.1458; Amount -0.0823; ,V1 -1.2091; V2 -3.8393; V3 0.5818; V4 0.2884; V5 2.9988; V6 -4.0162; V7 -0.9505; V8 -0.1811; V9 1.5912; V10 0.9399; V11 -1.1232; V12 0.9273; V13 0.9386; V14 0.7162; V15 0.9554; V16 0.7093; V17 0.4722; V18 0.4764; V19 0.1960; V20 -2.3899; V21 0.0723; V22 0.6748; V23 0.8008; V24 1.8235; V25 -2.8321; V26 1.0379; V27 2.0890; V28 -1.0014; Amount -0.3098; ,V1 0.5793; V2 -0.5047; V3 0.5612; V4 -0.2615; V5 0.3220; V6 0.0029; V7 0.5559; V8 -0.1287; V9 0.4450; V10 0.6176; V11 -0.6338; V12 0.7891; V13 0.2050; V14 0.8704; V15 1.0686; V16 0.4720; V17 0.5606; V18 0.1045; V19 -0.9710; V20 -0.0756; V21 -0.0672; V22 0.0386; V23 -0.1287; V24 0.8734; V25 0.8398; V26 -0.8274; V27 -0.0970; V28 0.0135; Amount 0.2251; ,V1 0.6405; V2 -0.9491; V3 0.7445; V4 -1.0745; V5 -0.1526; V6 0.6127; V7 0.1624; V8 0.0028; V9 0.0802; V10 0.9370; V11 -0.3315; V12 0.5515; V13 -1.0846; V14 0.6642; V15 -0.5414; V16 0.4284; V17 0.7156; V18 0.5677; V19 -0.4521; V20 -0.5327; V21 -0.1672; V22 -0.0306; V23 0.0898; V24 0.4215; V25 0.0080; V26 -0.4888; V27 -0.0169; V28 -0.0219; Amount -0.1074; ,V1 0.1631; V2 -0.5686; V3 0.7196; V4 -0.3264; V5 0.2607; V6 0.9743; V7 0.7076; V8 -0.2676; V9 0.9733; V10 0.9483; V11 -0.1609; V12 0.7131; V13 -0.7193; V14 0.5946; V15 0.4008; V16 0.4496; V17 0.4720; V18 0.4959; V19 0.0572; V20 -0.7509; V21 -0.1810; V22 0.6767; V23 -0.4922; V24 0.2473; V25 -1.6767; V26 -1.3684; V27 -1.4452; V28 -2.5279; Amount 0.5505;
    y: CategoryList
    0,0,0,0,0
    Path: /content/data/cc-fraud;

    Test: None



The size of dataset has increased after SMOTE oversampling, so we might consider a different layer and learning rate for the learner, let's run a for-loop to estimate an appropriate learning rate.


```python
learn_sm = tabular_learner(data_sm, layers = [800,400], ps=[0.001,0.01], metrics = [accuracy, Precision(), Recall(), FBeta(beta=1)], callback_fns = [AUROC,AUPRC])
learn_sm.lr_find()
learn_sm.recorder.plot(suggestion=True)
```





    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
    Min numerical gradient: 8.32E-04
    Min loss divided by 10: 1.74E-02



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/cc-fraud/output_56_2.png" alt="">


Let's test different learning rate and see how they perform on this model with SMOTE dataset.


```python
lr_sm = [[5e-05], [1e-04], [1e-03], [1.74e-02]]

for lr in lr_sm:
  learn_sm = tabular_learner(data_sm, layers = [800,400], ps=[0.001,0.01], metrics = [accuracy, Precision(), Recall(), FBeta(beta=1)], callback_fns = [AUROC,AUPRC])
  learn_sm.fit_one_cycle(8, lr)
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>precision</th>
      <th>recall</th>
      <th>f_beta</th>
      <th>AUROC</th>
      <th>AUPRC</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.066161</td>
      <td>0.043475</td>
      <td>0.991380</td>
      <td>0.135472</td>
      <td>0.926829</td>
      <td>0.236392</td>
      <td>0.981277</td>
      <td>0.753550</td>
      <td>01:22</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.025506</td>
      <td>0.030293</td>
      <td>0.990731</td>
      <td>0.124579</td>
      <td>0.902439</td>
      <td>0.218935</td>
      <td>0.973920</td>
      <td>0.781787</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.014233</td>
      <td>0.012327</td>
      <td>0.996647</td>
      <td>0.286275</td>
      <td>0.890244</td>
      <td>0.433234</td>
      <td>0.978755</td>
      <td>0.758546</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.007463</td>
      <td>0.011990</td>
      <td>0.996594</td>
      <td>0.282946</td>
      <td>0.890244</td>
      <td>0.429412</td>
      <td>0.974280</td>
      <td>0.797175</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.006606</td>
      <td>0.009559</td>
      <td>0.997367</td>
      <td>0.341121</td>
      <td>0.890244</td>
      <td>0.493243</td>
      <td>0.970869</td>
      <td>0.830025</td>
      <td>01:24</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.002344</td>
      <td>0.009299</td>
      <td>0.997261</td>
      <td>0.330275</td>
      <td>0.878049</td>
      <td>0.480000</td>
      <td>0.972298</td>
      <td>0.841059</td>
      <td>01:24</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.002687</td>
      <td>0.008408</td>
      <td>0.997700</td>
      <td>0.375635</td>
      <td>0.902439</td>
      <td>0.530466</td>
      <td>0.974207</td>
      <td>0.847326</td>
      <td>01:24</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.002498</td>
      <td>0.009071</td>
      <td>0.997454</td>
      <td>0.350711</td>
      <td>0.902439</td>
      <td>0.505119</td>
      <td>0.970198</td>
      <td>0.846877</td>
      <td>01:23</td>
    </tr>
  </tbody>
</table>



<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>precision</th>
      <th>recall</th>
      <th>f_beta</th>
      <th>AUROC</th>
      <th>AUPRC</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.042225</td>
      <td>0.033201</td>
      <td>0.992416</td>
      <td>0.148594</td>
      <td>0.902439</td>
      <td>0.255172</td>
      <td>0.984232</td>
      <td>0.750801</td>
      <td>01:22</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.013014</td>
      <td>0.013545</td>
      <td>0.996559</td>
      <td>0.284091</td>
      <td>0.914634</td>
      <td>0.433526</td>
      <td>0.974468</td>
      <td>0.822629</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.008177</td>
      <td>0.010781</td>
      <td>0.997384</td>
      <td>0.342723</td>
      <td>0.890244</td>
      <td>0.494915</td>
      <td>0.961301</td>
      <td>0.778945</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.004573</td>
      <td>0.008818</td>
      <td>0.998069</td>
      <td>0.420455</td>
      <td>0.902439</td>
      <td>0.573643</td>
      <td>0.962778</td>
      <td>0.790235</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.006150</td>
      <td>0.008109</td>
      <td>0.997753</td>
      <td>0.378947</td>
      <td>0.878049</td>
      <td>0.529412</td>
      <td>0.962064</td>
      <td>0.782973</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.002995</td>
      <td>0.006181</td>
      <td>0.998473</td>
      <td>0.483444</td>
      <td>0.890244</td>
      <td>0.626609</td>
      <td>0.965548</td>
      <td>0.827365</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.002302</td>
      <td>0.005963</td>
      <td>0.998683</td>
      <td>0.525547</td>
      <td>0.878049</td>
      <td>0.657534</td>
      <td>0.959182</td>
      <td>0.843475</td>
      <td>01:24</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.001857</td>
      <td>0.005669</td>
      <td>0.998754</td>
      <td>0.541353</td>
      <td>0.878049</td>
      <td>0.669767</td>
      <td>0.960461</td>
      <td>0.847402</td>
      <td>01:23</td>
    </tr>
  </tbody>
</table>



<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>precision</th>
      <th>recall</th>
      <th>f_beta</th>
      <th>AUROC</th>
      <th>AUPRC</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.024313</td>
      <td>0.036078</td>
      <td>0.987746</td>
      <td>0.096859</td>
      <td>0.902439</td>
      <td>0.174941</td>
      <td>0.973095</td>
      <td>0.798328</td>
      <td>01:22</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.007655</td>
      <td>0.015701</td>
      <td>0.995471</td>
      <td>0.228395</td>
      <td>0.902439</td>
      <td>0.364532</td>
      <td>0.972315</td>
      <td>0.687230</td>
      <td>01:22</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.008293</td>
      <td>0.006951</td>
      <td>0.998209</td>
      <td>0.439759</td>
      <td>0.890244</td>
      <td>0.588710</td>
      <td>0.973958</td>
      <td>0.807763</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.008018</td>
      <td>0.009739</td>
      <td>0.997525</td>
      <td>0.356098</td>
      <td>0.890244</td>
      <td>0.508711</td>
      <td>0.967031</td>
      <td>0.742780</td>
      <td>01:22</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.003202</td>
      <td>0.013550</td>
      <td>0.996032</td>
      <td>0.251724</td>
      <td>0.890244</td>
      <td>0.392473</td>
      <td>0.975881</td>
      <td>0.804713</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.001795</td>
      <td>0.006375</td>
      <td>0.998718</td>
      <td>0.533835</td>
      <td>0.865854</td>
      <td>0.660465</td>
      <td>0.969377</td>
      <td>0.849361</td>
      <td>01:22</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.001803</td>
      <td>0.006529</td>
      <td>0.998894</td>
      <td>0.577236</td>
      <td>0.865854</td>
      <td>0.692683</td>
      <td>0.965197</td>
      <td>0.839080</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.001577</td>
      <td>0.006158</td>
      <td>0.998912</td>
      <td>0.581967</td>
      <td>0.865854</td>
      <td>0.696078</td>
      <td>0.971294</td>
      <td>0.850546</td>
      <td>01:23</td>
    </tr>
  </tbody>
</table>



<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>precision</th>
      <th>recall</th>
      <th>f_beta</th>
      <th>AUROC</th>
      <th>AUPRC</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.027668</td>
      <td>0.013583</td>
      <td>0.997490</td>
      <td>0.356808</td>
      <td>0.926829</td>
      <td>0.515254</td>
      <td>0.980851</td>
      <td>0.540572</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.036390</td>
      <td>0.034030</td>
      <td>0.988835</td>
      <td>0.107649</td>
      <td>0.926829</td>
      <td>0.192893</td>
      <td>0.984126</td>
      <td>0.598335</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.027285</td>
      <td>0.209877</td>
      <td>0.940118</td>
      <td>0.021833</td>
      <td>0.926829</td>
      <td>0.042661</td>
      <td>0.971532</td>
      <td>0.248883</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.023372</td>
      <td>2.806553</td>
      <td>0.993311</td>
      <td>0.162528</td>
      <td>0.878049</td>
      <td>0.274286</td>
      <td>0.975606</td>
      <td>0.120687</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.014034</td>
      <td>0.019330</td>
      <td>0.994189</td>
      <td>0.186398</td>
      <td>0.902439</td>
      <td>0.308977</td>
      <td>0.983225</td>
      <td>0.824631</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.010645</td>
      <td>0.010720</td>
      <td>0.997279</td>
      <td>0.334842</td>
      <td>0.902439</td>
      <td>0.488449</td>
      <td>0.970846</td>
      <td>0.634225</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.003551</td>
      <td>0.008991</td>
      <td>0.997981</td>
      <td>0.409836</td>
      <td>0.914634</td>
      <td>0.566038</td>
      <td>0.979261</td>
      <td>0.849560</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.001284</td>
      <td>0.007149</td>
      <td>0.998262</td>
      <td>0.447853</td>
      <td>0.890244</td>
      <td>0.595918</td>
      <td>0.981268</td>
      <td>0.849798</td>
      <td>01:23</td>
    </tr>
  </tbody>
</table>



```python
  learn_sm = tabular_learner(data_sm, layers = [800,400], ps=[0.001,0.01], metrics = [accuracy, Precision(), Recall(), FBeta(beta=1)], callback_fns = [AUROC,AUPRC])
  learn_sm.fit_one_cycle(16, 5e-05)
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>precision</th>
      <th>recall</th>
      <th>f_beta</th>
      <th>AUROC</th>
      <th>AUPRC</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.088555</td>
      <td>0.067375</td>
      <td>0.985745</td>
      <td>0.087104</td>
      <td>0.939024</td>
      <td>0.159420</td>
      <td>0.983176</td>
      <td>0.811722</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.036362</td>
      <td>0.042116</td>
      <td>0.988255</td>
      <td>0.103914</td>
      <td>0.939024</td>
      <td>0.187120</td>
      <td>0.976087</td>
      <td>0.791831</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.014910</td>
      <td>0.020043</td>
      <td>0.994505</td>
      <td>0.195251</td>
      <td>0.902439</td>
      <td>0.321041</td>
      <td>0.964062</td>
      <td>0.826507</td>
      <td>01:24</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.007601</td>
      <td>0.010932</td>
      <td>0.996910</td>
      <td>0.304167</td>
      <td>0.890244</td>
      <td>0.453416</td>
      <td>0.968102</td>
      <td>0.827991</td>
      <td>01:24</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.006250</td>
      <td>0.012884</td>
      <td>0.996015</td>
      <td>0.250859</td>
      <td>0.890244</td>
      <td>0.391421</td>
      <td>0.972084</td>
      <td>0.795826</td>
      <td>01:23</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.004021</td>
      <td>0.009401</td>
      <td>0.997384</td>
      <td>0.339713</td>
      <td>0.865854</td>
      <td>0.487972</td>
      <td>0.972292</td>
      <td>0.818229</td>
      <td>01:25</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.004183</td>
      <td>0.007771</td>
      <td>0.997788</td>
      <td>0.382979</td>
      <td>0.878049</td>
      <td>0.533333</td>
      <td>0.978051</td>
      <td>0.861032</td>
      <td>01:26</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.002430</td>
      <td>0.009922</td>
      <td>0.997261</td>
      <td>0.330275</td>
      <td>0.878049</td>
      <td>0.480000</td>
      <td>0.973563</td>
      <td>0.841364</td>
      <td>01:25</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.002237</td>
      <td>0.006831</td>
      <td>0.998209</td>
      <td>0.438272</td>
      <td>0.865854</td>
      <td>0.581967</td>
      <td>0.978875</td>
      <td>0.857985</td>
      <td>01:24</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.002345</td>
      <td>0.005806</td>
      <td>0.998473</td>
      <td>0.482993</td>
      <td>0.865854</td>
      <td>0.620087</td>
      <td>0.981940</td>
      <td>0.846667</td>
      <td>01:24</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.002369</td>
      <td>0.004864</td>
      <td>0.998876</td>
      <td>0.572581</td>
      <td>0.865854</td>
      <td>0.689320</td>
      <td>0.979501</td>
      <td>0.858246</td>
      <td>01:26</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.003230</td>
      <td>0.004717</td>
      <td>0.998859</td>
      <td>0.568000</td>
      <td>0.865854</td>
      <td>0.685990</td>
      <td>0.984255</td>
      <td>0.858265</td>
      <td>01:25</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.001725</td>
      <td>0.004681</td>
      <td>0.998876</td>
      <td>0.572581</td>
      <td>0.865854</td>
      <td>0.689320</td>
      <td>0.987698</td>
      <td>0.855999</td>
      <td>01:25</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.000789</td>
      <td>0.004637</td>
      <td>0.998947</td>
      <td>0.591667</td>
      <td>0.865854</td>
      <td>0.702970</td>
      <td>0.984567</td>
      <td>0.856234</td>
      <td>01:26</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.004702</td>
      <td>0.004483</td>
      <td>0.999034</td>
      <td>0.619469</td>
      <td>0.853659</td>
      <td>0.717949</td>
      <td>0.982343</td>
      <td>0.851635</td>
      <td>01:26</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.000560</td>
      <td>0.004238</td>
      <td>0.999070</td>
      <td>0.630631</td>
      <td>0.853659</td>
      <td>0.725389</td>
      <td>0.984040</td>
      <td>0.857053</td>
      <td>01:26</td>
    </tr>
  </tbody>
</table>



```python
preds_sm,y_sm,losses_sm = learn_sm.get_preds(with_loss=True)
interp_sm = ClassificationInterpretation(learn_sm, preds_sm, y_sm, losses_sm)
```


```python
# smote oversampling model
interp_sm.plot_confusion_matrix()
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/cc-fraud/output_61_0.png" alt="">


SMOTE oversampling have very similar AUPRC with the non-sampling original dataset, but it seems like the use of SMOTE sampling causes the precision to drop.

Let's revert back to the pre-sampling dataset.


```python
# no sampling model
interp.plot_confusion_matrix()
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/cc-fraud/output_63_0.png" alt="">



```python
learn.export()
```

This will create a file named export.pkl in the directory where we were working that contains everything we need to deploy our model (the model, the weights but also some metadata like the classes or the transforms/normalization used).

## Putting model in production (Optional)


```python
# defaults.device = torch.device('cpu')
# pred_row = df.iloc[20,:]
# learn = load_learner(path)
# pred_class,pred_idx,outputs = learn.predict(pred_row)
# print(pred_class)
```

That is all for this blogpost, the neural net achieved `0.8659` f1-score, `0.98` AUROC and `0.86` AUPRC out of the box. We tried SMOTE oversampling, unfortunately it didn't help.

Thank you for reading and I hope to see you in the next blogpost!

---
title: "The nitty-gritty of Neural Network and Accelerated SGD"
date: 2019-05-03
permalink: /sgd-mnist/
tags: [fastai, sgd, neural net in Excel, adam]
excerpt: "weight decay, building neural net from scratch, momentum, rmsprop, adam, cross entropy loss and softmax"
mathjax: "true"
---

# Weight Decay

As an econometrics graduate, I learnt that when fitting a model, using less parameters is a good idea so that the model will generalize well in the prediction.

Now I realize that it is a convenient fiction for the real truth which is not wanting the function to be too complex, and I was taught that having less parameters make it less complex.

Although it has some truth in it, real life data more often or not have high non-linearities, that is why we need more parameters in the model, but not too many that we would overfit the data, the way we do it is by penalizing the model complexity.

**How do we penalize complexity?**

One way to penalize the complexity is, to sum up the square of the parameters. Then we just add that number to the loss.

But there is the problem that sum can be so big that it is better for the model to just set all parameters into zero. That is why we multiply the sum with some small hyperparameter known as `wd` (weight decay), which generally should be `1e-1`.

And if we put in math notations, it look something like this..

![wd](https://i.imgur.com/SICXf0E.png)

# Colab Cloud Server VM setup & FastAI Configurations

First we need to use GPU for deep learning applications, so instead of buying, we can rent it from cloud servers. There are many other server options, but basically I chose Colab because issa FREE.

To learn how to set up a Colab Server which supports fastai library and its applications, click [here](https://course.fast.ai/start_colab.html).

NB: This is a free service that may not always be available, and requires extra steps to ensure your work is saved. Be sure to read the docs on the Colab web-site to ensure you understand the limitations of the system.


```python
# Permit collaboratory instance to read/write to Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
root_dir = "/content/gdrive/My Drive/"
base_dir = root_dir + 'fastai-v3/'
```

    Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code

    Enter your authorization code:
    ··········
    Mounted at /content/gdrive



```python
  !curl -s https://course.fast.ai/setup/colab | bash
```

    Updating fastai...
    Done.



```python
%reload_ext autoreload
%autoreload 2
%matplotlib inline
```


```python
from fastai.basics import *
```

# MNIST SGD

Get the 'pickled' MNIST dataset from http://deeplearning.net/data/mnist/mnist.pkl.gz. We're going to treat it as a standard flat dataset with fully connected layers, rather than using a CNN.


```python
path = Config().data_path()
```


```python
path.ls()
```




    [PosixPath('/root/.fastai/data/mnist.pkl.gz')]




```python
with gzip.open(path/'mnist.pkl.gz', 'rb') as f:
    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')
```


```python
plt.imshow(x_train[0].reshape((28,28)), cmap="gray")
x_train.shape
```




    (50000, 784)




<img src="{{ site.url }}{{ site.baseurl }}/assets/images/sgd-mnist/output_12_1.png" alt="">

We use `map` to convert the flat numpy arrays to torch.tensor.


```python
# without map function
# x_train = torch.tensor(x_train)
# y_train = torch.tensor(y_train)
# x_valid = torch.tensor(x_valid)
# y_valid = torch.tensor(y_valid)

# with map function
x_train,y_train,x_valid,y_valid = map(torch.tensor, (x_train,y_train,x_valid,y_valid))

n,c = x_train.shape
x_train.shape, y_train.min(), y_train.max()
```




    (torch.Size([50000, 784]), tensor(0), tensor(9))



## Creating dataset and dataloader in PyTorch

In the previous [stochastic gradient descent blogpost](https://cheeloong.github.io/sgd/),  we did these things ourselves:

```python
x = torch.ones(n,2)
def mse(y_hat, y): return ((y_hat-y)**2).mean()
y_hat = x@a
```

Now instead we'll use PyTorch's functions to do it for us, and also to handle mini-batches (which we didn't do last time, since our dataset was so small).

In PyTorch we transfer data into mini-batches by first creating dataset.


```python
# set batchsize
bs=64

# create training TensorDataset with 2 tensors
train_ds = TensorDataset(x_train, y_train)

# create validation TensorDataset with 2 tensors
valid_ds = TensorDataset(x_valid, y_valid)
```

Dataset is just something where x and y values are mapped to each other so we can get $n^{th}$ x and y value using index.

After we get the data in `TensorDataset` we create dataloader with `DataBunch.create`.


```python
# create a data loader which gives batchsize of data
data = DataBunch.create(train_ds, valid_ds, bs=bs)
```

Now our data is in mini batches and we can iterate our data one batchsize at a time.


```python
# next of the iterator of data.train_dl
x,y = next(iter(data.train_dl))
x.shape,y.shape
```




    (torch.Size([64, 784]), torch.Size([64]))



`x` represents the input activation in the train set, 64 is the batchsize that we selected, and 784 is the pixels that are flattened out, `y` is our output activation which has 64 outputs because we set our batchsize `bs = 64`.

## Logistic Regression from scratch


```python
# subclass nn.Module
class Mnist_Logistic(nn.Module):
    def __init__(self):
        super().__init__()
        self.lin = nn.Linear(784, 10, bias=True)

    def forward(self, xb): return self.lin(xb)
```

The code above is subclassing `nn.Module` and adding an attribute in the class wihch contains linear layer `nn.Linear()` which basically does `a@x + b`

`def forward` in this case pass `xb` (i.e. batch of x)  to `self.lin` and return the result of `a@x + b` on this mini batch.

All this creates a Logistic Regression model, which is a one layer neural net without hidden non-linearity layers.


```python
# Put the subclassed to the GPU manually as we are using our own model
model = Mnist_Logistic().cuda()
```


```python
model
```




    Mnist_Logistic(
      (lin): Linear(in_features=784, out_features=10, bias=True)
    )




```python
model.lin
```




    Linear(in_features=784, out_features=10, bias=True)




```python
model(x).shape
```




    torch.Size([64, 10])




```python
[p.shape for p in model.parameters()]
```




    [torch.Size([10, 784]), torch.Size([10])]



`model.parameters()` contains the parameters in our neural net.

`torch.Size([10, 784])` is the parameter layer that would take in 784 dimensional inputs and spits out 10 dimensional outputs, and the `torch.Size([10])` is of vector of length 10 which represent the biases we want to add to our activation.

All this really just follow the first diagram in the [previous blogpost](https://cheeloong.github.io/collabnn/#), it is exactly the layers that were highlighted in yellow.


```python
# specify learning rate
lr=2e-2
```


```python
# this is a classification task, MSE don't make sense
loss_func = nn.CrossEntropyLoss()
```


```python
# define gradient descent with a regularized loss
def update(x,y,lr):
    wd = 1e-5
    y_hat = model(x)
    # weight decay
    w2 = 0.
    for p in model.parameters(): w2 += (p**2).sum()
    # add to regular loss
    loss = loss_func(y_hat, y) + w2*wd
    loss.backward()
    with torch.no_grad():
        for p in model.parameters():
            p.sub_(lr * p.grad)
            p.grad.zero_()
    # .item() turn scalar tensor to python number so we can plot
    return loss.item()
```

The above code resemble a gradient descdent update function, which we have discussed in [this blogpost](https://cheeloong.github.io/sgd/). They look very similar except a few differences.

- Previously, we create our own `y_hat` by typing `y_hat = x@a`, but since we have assigned our `Mnist_Logistic()` to the variable name `model`, we can now type `y_hat = model(x)`.

- Previously, we didn't a for loop because we only have 1 set of parameters, now we have multiple stored in `model.parameters()` and that calls for a for loop!

- Previously we use MSE as the loss, but now we are using Cross Entropy Loss, because this is a classification task.

- Notice that the loss have this regularization term `w2*wd`, this is where we penalize the complexity of the model as explained previously.


```python
losses = [update(x,y,lr) for x,y in data.train_dl]
```


```python
plt.plot(losses);
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/sgd-mnist/output_37_0.png" alt="">

As we can see, the losses bounce more and more towards the minimum loss, this is most probably the loss function is flat in weights space when its reaching the minimum, and so it diverge slightly with the constant learning rate that is too high for the surface, which is why learning rate annealing is a good idea.

In our gradient descent update function, it looks like this:

\begin{align}
w_t = w_{t-1} - lr * \frac{\delta L}{\delta w_{t}}
\end{align}



Now because our loss have included a regularization term:

\begin{align}
L(X, w) = mse(M(X, w), y) + wd * w^2
\end{align}

if we take the gradient of $wd * w^2$ with respect to w:

\begin{align}
\frac{\delta(wd * w^2)}{\delta w} =  2wd * w
\end{align}


removing the constant value 2 without loss of generality and we get:

\begin{align}
wd * w
\end{align}

**Some terminology**

1. When the regularization term is in this form $wd * w^2$ which we add to our loss function. We call this **L2 Regularization**.

2. When the regularization term is in this form $wd * w$ which we subtract from our gradient and multiplied by a learning rate in the gradient descent update function. We call this **weight decay**.

Since we have defined our gradient update function and built a Logistic Regression from scratch, we can also build a Neural Net from scratch.

## Neural Net from scratch

We need 2 linear layers to build this neural net (that's why there's `lin1` and `lin2` in the code below)




```python
class Mnist_NN(nn.Module):
    def __init__(self):
        super().__init__()
        self.lin1 = nn.Linear(784, 50, bias=True)
        self.lin2 = nn.Linear(50, 10, bias=True)

    def forward(self, xb):
        x = self.lin1(xb)
        x = F.relu(x)
        return self.lin2(x)
```


```python
model = Mnist_NN().cuda()
```


```python
losses = [update(x,y,lr) for x,y in data.train_dl]
```


```python
plt.plot(losses);
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/sgd-mnist/output_44_0.png" alt="">

So really, as long as we have the gradient descent update function defined, we can really create any type of model we want.

Now let's learn a new trick to shorten our code:

Previous, our `update` code looks like this:



```
# define gradient descent with a regularized loss
def update(x,y,lr):
    wd = 1e-5
    y_hat = model(x)
    # weight decay
    w2 = 0.
    for p in model.parameters(): w2 += (p**2).sum()
    # add to regular loss
    loss = loss_func(y_hat, y) + w2*wd
    loss.backward()
    with torch.no_grad():
        for p in model.parameters():
            p.sub_(lr * p.grad)
            p.grad.zero_()
    # .item() turn scalar tensor to python number so we can plot
    return loss.item()
```

We can shorten in to the following version:





```python
def update(x,y,lr):
    opt = optim.SGD(model.parameters(), lr, weight_decay = 1e-5)
    y_hat = model(x)
    loss = loss_func(y_hat, y)
    loss.backward()
    opt.step() # this is where the for loop for gradient update occurs
    opt.zero_grad()
    return loss.item()
```

Let's see if it works!


```python
model = Mnist_NN().cuda()
```


```python
losses = [update(x,y,lr) for x,y in data.train_dl]
```


```python
plt.plot(losses);
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/sgd-mnist/output_51_0.png" alt="">

Sweet! Another thing we can do is to optimize the weights using other optimizers such as `Adam` by simplying changing the instance variable `optim.SGD` to `optim.Adam` so it looks like this:


```python
def update(x,y,lr):
    opt = optim.Adam(model.parameters(), lr, weight_decay = 1e-5)
    y_hat = model(x)
    loss = loss_func(y_hat, y)
    loss.backward()
    opt.step() # this is where the for loop for gradient update occurs
    opt.zero_grad()
    return loss.item()
```


```python
model = Mnist_NN().cuda()
```


```python
losses = [update(x,y,lr) for x,y in data.train_dl]
```


```python
plt.plot(losses);
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/sgd-mnist/output_56_0.png" alt="">

Hold up, this does not look like its working properly, because we got to use a different learning rate for `Adam` to optimize, different optimizer, different rules!


```python
model = Mnist_NN().cuda()
```


```python
losses = [update(x,y,1e-3) for x,y in data.train_dl]
```


```python
plt.plot(losses);
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/sgd-mnist/output_60_0.png" alt="">

Hey it works properly now, but more importantly, the loss drops below 0.5 after just 200 epochs, whereas SGD optimizer took approximately 700 epochs.

Fantabulous performance, but what is `optim.Adam` and why does the loss function reduce so quickly than the one in `optim.SGD`?

We will use Microsoft Excel to demonstrate this, click [this link](https://github.com/fastai/course-v3/blob/master/files/xl/graddesc.xlsm) to download the Excel file from  fastai Github account.



# Accelerated SGD in Microsoft Excel



Let's start by simulating some data, suppose that the ground truth value of the intercept $b$ is $30$, and the slope $a$ is $2$. In another word,

$$\begin{align}
y &= ax + b
&= 2x + 30
\end{align}$$


So here we've got random generated $X's$ that are between 1 to 100, we can sub in the $X's$ and get the $y$ value.

(Oh yea, so this is to demonstrate how gradient descent work, in practice, we don't know the ground truth values of our parameters.)


![rand_data](https://i.imgur.com/uUA3HiH.png =800x)



## Online SGD

Stochastic gradient descent is basically gradient descent with a batchsize and online gradient descent is basically stochastic gradient descent with a batchsize of 1.

Thus, each row in the Excel below constitute 1 batch.

![basic](https://i.imgur.com/r1LUFrr.png =800x)

Too..many...NuMbeRs..Let's learn what each of the column means!

We already know that the ground truth values of intercept and slope are 30 and 2 respectively, but let's assume we do not know that and we want to find it out using gradient descent with the available data ($X$ and $y$)

Since we do not know the ground truths, we arbitrarily set intercept and slope to be both equal to 1 ($a = 1$, $b = 1$), and so our prediction with our random guesses would be

\begin{align}
y_{pred} = ax + b
\end{align}

Btw, I know that I should put $\hat{a}$ instead of $a$ cuz if $a$ represents the true value, $\hat{a}$ would be representing the estimate of the true value, but ain't nobody got time for that, and so lets see how we get all the values for the first row (4th index row, $$x = 14$$ and $$y = 58$$)

$$\begin{align}
y_{pred} &= 1 * (14) + 1 \\
&= 15
\end{align}$$

Then we take the sum of squared error

$$\begin{align}
{err}^2 &= (y - y_{pred})^2 \\
&= (58 - 15)^2 \\
&= 1849
\end{align}$$

Now we need to calculate the gradient to update for gradient descent! There are 2 ways to calculate gradient:

1.**Finite Differencing**

This is where we see how tiny changes in the intercept ($$b$$) or slope ($$a$$) affect the rate of change in y over the rate of change in x.

>   Let's look at column G & H which represent a small change in the intercept ($b$), `errb1`

$$\begin{align}
{err}^2_{b1} &= (y_{pred} - y)^2 \\
&= (ax + (b + 0.01) - y)^2 \\
&= ((1 * 14) + (1 + 0.01) - 58)^2 \\
&= 1848.1401 \\
\frac{\delta(err^2)}{\delta(b_t)} &= \frac{err^2_{b1}- err^2}{(b+0.01) - b} \\
&= \frac{1848.1401 - 1849}{(1.01) - 1} \\
&= -85.99
\end{align}$$

> Let's look at column I & J which represent a small change in the slope ($a$), `erra1`

$$\begin{align}
{err}^2_{a1} &= (y_{pred} - y)^2 \\
&= ((a+0.01)x + b - y)^2 \\
&= ((1.01 * 14) + 1 - 58)^2 \\
&= 1836.9796 \\
\frac{\delta({err}^2)}{\delta(a_t)} &= \frac{err^2_{a1} - err^2}{(b+0.01) - b} \\
&= \frac{1836.9796 - 1849}{1.01 - 1} \\
&= -1202.04
\end{align}$$

2.**Analytically** (Formulae given on top right of the Excel screenshot)

> Let's look at column K which represents the analytical solution of the gradient `de/db`

$$\begin{align}
{err}^2 &= (y_{pred} - y)^2 \\
&= (ax + b - y)^2 \\
\frac{\delta({err}^2)}{\delta(b_{t-1})} &= 2(ax + b - y) \\
&= 2((1 * 14) + 1 - 58) \\
&= -86
\end{align}$$

> Let's look at column L which represents the analytical solution of the gradient `de/da`

$$\begin{align}
{err}^2 &= (y_{pred} - y)^2 \\
&= (ax + b - y)^2 \\
\frac{\delta({err}^2)}{\delta(a_{t-1})} &= x * 2(ax + b - y) \\
&= 14 * 2((1 * 14) + 1 - 58) \\
&= -1204
\end{align}$$


Finite differencing has a lot more calculations involved so we will just stick with the analytical solutions and update our slope and intercept

$$\begin{align}
w_t = w_{t-1} - lr * \frac{\delta L}{\delta w_{t-1}}
\end{align}$$

Therefore, our `new_a` is

$$\begin{align}
a_t &= a_{t-1} - lr * \frac{\delta L}{\delta a_{t-1}} \\
&= 1 - (0.0001 * -1204) \\
&= 1.1204
\end{align}$$

and `new_b` is

$$\begin{align}
b_t &= b_{t-1} - lr * \frac{\delta L}{\delta b_{t-1}} \\
&= 1 - (0.0001 * -86) \\
&= 1.0086
\end{align}$$



Repeat the same procedure multiple times, and in this case it stops at row 32 (its cropped in the picture so we can't see), and that would be 1 epoch, we then copy and paste the latest intercept and slope values to the `cell C1 and cell C2` and do it for another epoch.  (Note: The Run button cell would automatically run the macro which will do 5 epochs for us)

Let's see what happen after 25 epochs.

![epoch25](https://i.imgur.com/x16iDHe.png =800x)

Remember that the ground truth values for intercept and slope are 30 and 2 respectively? The algorithm appears to descent very slowly towards the minimum value, we have to speed this up, obviously without increasing the number of epoch by a gazillion, how do we go about to do that? We are going to use something called **momentum**.






## Momentum

When we were doing gradient descent previously, we update our weights by reducing a learning rate multiplied by the gradient of loss with respect to the weight, in another word, the update of the parameter relies solely on the gradient of loss with respect to the parameter, with we will call it $$w_t$$.

$$\begin{align}
w_t = w_{t-1} - lr * \frac{\delta L}{\delta w_{t}}
\end{align}$$

$$\rho$$ represents the momentum here, let's say $\rho = 0.9$ that means we want to allocate 90% of the update to the previous step and 10% of the update to the gradient. Essentially, it looks something like this

$$\begin{align}
s_t = \rho * s_{t-1} + (1 - \rho) * \frac{\delta L}{\delta w_{t}}  
\end{align}$$

![momt](https://i.imgur.com/eklhe3u.png =800x)

This is also known as a exponentially weighed moving average because $s_t$ replies on $$s_{t-1}$$ which relies on $$s_{t-2}$$ which relies on $$s_{t-3}$$, you get the gist of it. However, only the recent few steps are exponentially higher weighted than the others.

and so our gradient descent with momentum becomes

$$\begin{align}
w_t = w_{t-1} - lr * {s_t}
\end{align}$$

![mmt](https://i.imgur.com/DFdHnp6.png =800x)

As we can see from the Excel, everything is still the same as before, except that we have taken out the finite differencing method to calculate gradient, `column H` and `column I` now uses a new gradient descent update, which takes into account of exponentially weighted moving average of the gradient and the previous step as shown in `column J` and `column K`, everything else should be relatively straightforward.

## RMSProp

RMSProp stands for Root Mean Square Propagation and is very similar to momentum except now its taking exponential average of squares of gradients

$$\begin{align}
s_{t} = \rho * s_{t-1} +  (1 - \rho) * (\frac{\delta L}{\delta w_{t}})^2
\end{align}$$

![gdsq](https://i.imgur.com/TSMEQMZ.png =800x)

If gradient is very small, $s_t$ will be a small number, if gradient is very volatile or large, $s_t$ will be a big number, how does that affect the gradient update?  

well now, the gradient update rule is now as follows

$$\begin{align}
s_{t-1} = \rho * s_{t-2} +  (1 - \rho) * (\frac{\delta L}{\delta w_{t-1}})^2
\end{align}$$

$$\begin{align}
w_t = w_{t-1} - \frac{lr}{\sqrt{s_{t-1}}} * (\frac{\delta L}{\delta w_{t}})
\end{align}$$

![gdsq2](https://i.imgur.com/kDJY8S6.png =800x)

Notice that the learning rate $lr$ is divided by $$\sqrt{s_{t-1}}$$, this is so that each parameter update is using a different learning rate depending on how big or small the previous step is.

Here's the coolest part, after running just 5 epochs, the parameters seem to be much faster converging to the ground truth ($$b = 30$$, $$a = 2$$) than the basic SGD without utilizing momentum or rmsprop.

![rmsp](https://i.imgur.com/V4NkrZi.png =800x)

## Adam

So we now know what momentum and RMSProp are, Adam is basically the combination of both momentum and RMSProp concept, to reiterate and to clarify, I will use superscript to denote the methods

$$\begin{align}
s_t^{momt.} = \rho * s_{t-1}^{momt.} + (1 - \rho) * \frac{\delta L}{\delta w_{t}}  
\end{align}$$

![mom](https://i.imgur.com/JYDxttj.png =800x)

$$\begin{align}
s_t^{rmsp.} = \rho * s_{t-1}^{rmsp.} + (1 - \rho) * (\frac{\delta L}{\delta w_{t}})^2
\end{align}$$

![rmsp](https://i.imgur.com/gJCbght.png =800x)



$$\begin{align}
w_t^{adam} = w_{t-1} - \frac{lr}{\sqrt{s_{t}^{rmsp.}}} *s_t^{momt.}
\end{align}$$

![adam](https://i.imgur.com/wNSscJ1.png =800x)

Let's run 5 epochs and check the result

![adam5epchs](https://i.imgur.com/k4OCltV.png =800x)

Let's run 5 more epochs and see if it will arrive at ground truth

![adam10epchs](https://i.imgur.com/7qwlgwv.png =800x)

we can see $b$ is almost there, but not quite the case for $a$.

If we run this for a few more epochs, we will see that the parameter $$b$$ moves around 30, but $$a$$ is moving either around $$1.7$$ or $$2.3$$, this is most probably because the learning rate is too high, let's drop it down to $$0.1$$ from $$1$$ and run 5 more epochs.

![adam15epchs](https://i.imgur.com/cvwljtp.png =800x)

Ayy, seems like we got it, now let's get back to the MNIST data.




# Back to MNIST


```python
# def update(x,y,lr):
#     opt = optim.Adam(model.parameters(), lr, weight_decay = 1e-5)
#     y_hat = model(x)
#     loss = loss_func(y_hat, y)
#     loss.backward()
#     opt.step() # this is where the for loop for gradient update occurs
#     opt.zero_grad()
#     return loss.item()
```

Thing is we don't usually create optimizer like the way above, because we would just call `Learner` like below, we feed in the databunch (which we assigned to `data`), PyTorch NN.module instance (which we defined as `Mnist_NN()`), our loss function (which we assigned to `loss_func`), and we would like to print accuracy as our metrics.


```python
learn = Learner(data, Mnist_NN(), loss_func=loss_func, metrics=accuracy)
```


```python
learn.lr_find()
learn.recorder.plot()
```





    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.



<img src="{{ site.url }}{{ site.baseurl }}/assets/images/sgd-mnist/output_82_2.png" alt="">


```python
learn.fit_one_cycle(1, 1e-2)
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/sgd-mnist/output_01_0.png" alt="">



`valid_loss` of only 0.1277, that's pretty good considering previously we barely getting below 0.5.

We have always been using `fit_one_cycle` without really knowing what it does, here's what it does


```python
learn.recorder.plot_lr(show_moms=True)
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/sgd-mnist/output_85_0.png" alt="">

The plot on the left is showing that in the beginning we slowly increase our learning rate for almost half the time, and then decrease afterwards for the remaining time, why is that? Because we are not certain which part of the function space we are in, so using a learning rate that is too high will cause the loss to diverge, and thats why we start slow, and after a while, the gradient will most likely be in the direction that we want to go, that's when we start increasing the learning rate, and then we got to anneal the learning rate because using a constant learning rate will most likely get us to the ground truth, it will most likely just jump around the ground truth.  

The plot on the right shows the momentum plot, when the learning rate is low, the momentum is high because if we are using a small learning rate and going to the same direction, we want our parameter to move faster, and when we are using a higher learning rate to jump across the bumpy part of the functions so that the parameter does not stuck in the local minimum, we do not want it to jump too much, so we use a lower momentum.

The combination of the 2 above, is the BTS of `fit_one_cycle`.


```python
learn.recorder.plot_losses()
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/sgd-mnist/output_87_0.png" alt="">

Notice that when we plot the losses with `recorder.plot_losses()` from fast.ai, it plots a clean curve rather than a zig-zaggy plot from before, because fast.ai calculates the exponentially weighted moving average of the losses making it easier for visualization.

Before we call it a day, let's talk a little bit more, particularly about Cross Entropy Loss and softmax.

# Important Terminology

Here's the important terminology in neural network that we have discussed previously, let's talk about those that we have not talked about

- Inputs
- Weights/parameters
- Random
- Activations
- Activation functions / nonlinearities (e.g. ReLU / sigmoid)
- Output
- Loss
- Metric
- **Cross-entropy**
- **Softmax**
- Fine tuning
- Layer deletion and random weights
- Freezing & unfreezing

## Cross Entropy Loss



We know that this is the loss function we used for a classification task, but what is it exactly? Let's try to understand it from a simple example from this [Excel file](https://github.com/fastai/course-v3/blob/master/files/xl/entropy_example.xlsx).

Intuitively speaking, in a classification task, we want a loss function that will
1. Gives little loss when we are prediciting the right category and with high confidence
2. Gives high loss when we are predicicting the wrong category and with high confidence

you get the gist of it.

![xentro](https://i.imgur.com/Rkqj0X1.png =500x)

6th row shows that the ground truth is a cat, and we are predicting it with a high confidence ($0.9$), and that is why the loss is only $0.04576$ as opposed to the 4th row which shows that the confidence is $0.9$ but the ground truth isnt a cat, which results in a cross entropy loss of 1.




## Softmax

We know that if we are trying to predict 5 different classes, the probability of predicting each class must all add up to be 1, or else it would return weird behaviour (e.g. negative cross entropy loss). How do we go about making sure that they all add up to 1? We need to use the correct activation function in the last layer, and in this case, that would be softmax which is an activation function where all activations are either greater than 0 and less than 1, and they add up to a total of 1.

![softmax](https://i.imgur.com/of1DRJP.png =300x)

So lets assume in this example, column B spits out the output of a neural network, how do we make sure they met the criterions? First, we take the exponential of the value, which would ensure that they are always going to be greater than 0, as shown in column C, we then add them all up together which is equal to $12.70$ as shwon in cell C8, we then take the proportion of each class to get its softmax.

When we are doing single label multi-class classification, we generally want softmax as our activation function and cross entropy as our loss. In PyTorch however, when we call `nn.CrossEntropyLoss()` , PyTorch will automatically use softmax as its activation layer for the last layer.

Sometimes, neural network do not output its softmax form, in the case we would have to do softmax manually.

Alright so that is all for this blogpost, in the next blogposts we will work on a tabular data, learn about batchnorm, dropout, data augmentation, convolutions, some computer vision architecture and applications.  Ciao!

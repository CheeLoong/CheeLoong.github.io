---
title: "Deep Learning with resnet34"
date: 2019-03-07
permalink: /pets/
tags: [fastai, deep learning, resnet34, pets]
excerpt: "Meow or Woof?"
mathjax: "true"
---

# What breed is your Meow & Woof?

What if I tell you I can build a model that can classify if an underlying image shows a cat or a dog? Isn't that cool? No?

What if I tell you my model can also classify the breed of your pet? I am a a little bit cooler now huh?

For this blogpost, We are going to use the [Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) by [O. M. Parkhi et al., 2012](http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf) which features **12 cat breeds and 25 dogs breeds**. Our model will need to learn to differentiate between these 37 distinct categories.

According to their paper, **the best accuracy they could get in 2012 was 59.21%**, using a complex model that was specific to pet detection, with separate "Image", "Head", and "Body" models for the pet photos. Let's see how accurate we can be using deep learning!

But, first we gotta set up some configurations...


# Colab Server VM setup & FastAI Configurations

To learn how to set up a Colab Server which supports fastai library and its applications, [click here](https://course.fast.ai/start_colab.html).  There are many other server options, but basically I chose Colab because issa **FREE**.

NB: This is a free service that may not always be available, and requires extra steps to ensure your work is saved. Be sure to read the docs on the Colab web-site to ensure you understand the limitations of the system.


```python
# Permit collaboratory instance to read/write to Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
root_dir = "/content/gdrive/My Drive/"
base_dir = root_dir + 'fastai-v3/'
```

    Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code

    Enter your authorization code:
    Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·
    Mounted at /content/gdrive


We import all the necessary packages. We are going to work with the [fastai V1 library](http://www.fast.ai/2018/10/02/fastai-ai/) which sits on top of [Pytorch 1.0](https://hackernoon.com/pytorch-1-0-468332ba5163). The fastai library provides many useful functions that enable us to quickly and easily build neural networks and train our models.


```python
from fastai.vision import *
from fastai.metrics import error_rate
```


```python
# Install neccessary packages & Create 'data' folder
# Note: Run this after libraries have been imported
!curl https://course.fast.ai/setup/colab | bash
```

      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed
    100   321  100   321    0     0   1099      0 --:--:-- --:--:-- --:--:--  1095
    Updating fastai...
    [31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.[0m
    [31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.[0m
    Done.


Every notebook starts with the following three lines; they ensure that any edits to libraries you make are reloaded here automatically, and also that any charts or images displayed are shown in this notebook.


```python
%reload_ext autoreload
%autoreload 2
%matplotlib inline
```

If you're using a computer with an unusually small GPU, you may get an out of memory error when running this notebook. If this happens, click Kernel->Restart, uncomment the 2nd line below to use a smaller *batch size* (you'll learn all about what this means during the course), and try again.


```python
bs = 64
# bs = 16   # uncomment this line if you run out of memory even after clicking Kernel->Restart
```

# Understanding the Data

We are going to use the `untar_data` function to which we must pass a URL as an argument and which will download and extract the data.


```python
# let's read a bit about the function
help(untar_data)
```

    Help on function untar_data in module fastai.datasets:

    untar_data(url:str, fname:Union[pathlib.Path, str]=None, dest:Union[pathlib.Path, str]=None, data=True, force_download=False) -> pathlib.Path
        Download `url` to `fname` if it doesn't exist, and un-tgz to folder `dest`.




```python
# this is the url for the data, remember to add .tgz at the back
print(URLs.PETS)
```

    https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet



```python
# This download the data to the 'data' subdirectory
path = untar_data(URLs.PETS)
path
```




    PosixPath('/root/.fastai/data/oxford-iiit-pet')




```python
# List what's inside the oxford-iiit-pet folder
path.ls()
```




    [PosixPath('/root/.fastai/data/oxford-iiit-pet/images'),
     PosixPath('/root/.fastai/data/oxford-iiit-pet/annotations')]




```python
path_anno = path/'annotations'
path_img = path/'images'
```

The first thing we do when we approach a problem is to take a look at the data. We _always_ need to understand very well what the problem is and what the data looks like before we can figure out how to solve it. Taking a look at the data means understanding how the data directories are structured, what the labels are and what some sample images look like.

The main difference between the handling of image classification datasets is the way labels are stored. In this particular dataset, labels are stored in the filenames themselves. We will need to extract them to be able to classify the images into the correct categories. Fortunately, the fastai library has a handy function made exactly for this, `ImageDataBunch.from_name_re` gets the labels from the filenames using a [regular expression](https://docs.python.org/3.6/library/re.html).


```python
fnames = get_image_files(path_img)
fnames[:5]
```




    [PosixPath('/root/.fastai/data/oxford-iiit-pet/images/Russian_Blue_55.jpg'),
     PosixPath('/root/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_187.jpg'),
     PosixPath('/root/.fastai/data/oxford-iiit-pet/images/english_cocker_spaniel_118.jpg'),
     PosixPath('/root/.fastai/data/oxford-iiit-pet/images/newfoundland_126.jpg'),
     PosixPath('/root/.fastai/data/oxford-iiit-pet/images/leonberger_177.jpg')]




```python
np.random.seed(2)
pat = r'/([^/]+)_\d+.jpg$' # regular expression pattern to extract labels
```

# Data Preprocessing

Now the following is fairly straightforward, but there are a few that of notes:

`size = 224`

- Size of images: In this post, the images used are square (i.e. height = width), and of size 224 x 224. Images which are of different shape/size are resized and cropped accordingly. It is one of the drawback of deep learning models that they need images of same size. Variable size images are in scope of part 2 of the course.

`ds_tfms=get_transforms()`

- Transform: Not sure the exact transformation going on here, but basically, it does centre cropping and rescaling on images to convert size of images to 224 x 224.

`bs = bs`

- Batch size: number of instances that will be propagated through the network,  [Here's a good example!](https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network)

`data.normalize(imagenet_stats)`

- Image normalization: Individual pixel values range from 0 to 255 and are normalized to bring the value to mean of 0 and standard deviation of 1. [Why do we need to normalize?](https://stackoverflow.com/questions/4674623/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network)

`ImageDataBunch.from_name_re` returns a databunch object which contains training, and validation data, sometimes along with test data as well.


```python
data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=bs)
data.normalize(imagenet_stats)
```




    ImageDataBunch;

    Train: LabelList (5912 items)
    x: ImageList
    Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)
    y: CategoryList
    Russian_Blue,yorkshire_terrier,english_cocker_spaniel,Sphynx,boxer
    Path: /root/.fastai/data/oxford-iiit-pet/images;

    Valid: LabelList (1478 items)
    x: ImageList
    Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)
    y: CategoryList
    Birman,scottish_terrier,english_setter,Russian_Blue,american_pit_bull_terrier
    Path: /root/.fastai/data/oxford-iiit-pet/images;

    Test: None




```python
data.show_batch(rows=3, figsize=(7,6))
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/assets/images/Pets/output_25_0.png" alt="">



```python
print(data.classes)
len(data.classes),data.c
```

    ['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair', 'Egyptian_Mau', 'Maine_Coon', 'Persian', 'Ragdoll', 'Russian_Blue', 'Siamese', 'Sphynx', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond', 'leonberger', 'miniature_pinscher', 'newfoundland', 'pomeranian', 'pug', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier']





    (37, 37)



# Deep Learning with resnet34

Now we will start training our model. We will use a [convolutional neural network](http://cs231n.github.io/convolutional-networks/) backbone and a fully connected head with a single hidden layer as a classifier. Don't know what these things mean? Not to worry, we will dive deeper in the coming lessons. For the moment you need to know that we are building a model which will take images as input and will output the predicted probability for each of the categories (in this case, it will have 37 outputs).

We will train for 4 epochs (4 cycles through all our data).


```python
learn = create_cnn(data, models.resnet34, metrics = error_rate)
```

    Downloading: "https://download.pytorch.org/models/resnet34-333f7ec4.pth" to /root/.torch/models/resnet34-333f7ec4.pth
    87306240it [00:03, 25947832.20it/s]



The first time we run the above code, it was downloading resnet34 pre-trained weights, because this particular model has already been trained on looking at about one and a half million pictures of thousands of different categories of things using an image data set called **ImageNet**.


The idea is that, we do not start off with a model that is totally clueless, this model probably knows something about recognizing images, most probably know the difference between cats and dogs too.


This is called 'Transfer Learning', which takes a pre-trained model that knows how to do a particular task pretty well, and make it so that it can do our task really well, it also greatly reduced model training time.


Also, the `error_rate` metrics that we chose to print in the `create_cnn` actually prints the validation set error rate.


```python
learn.fit_one_cycle(4)
```


Total time: 07:43 <p><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>1.390241</td>
      <td>0.336010</td>
      <td>0.098106</td>
      <td>01:57</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.564996</td>
      <td>0.230974</td>
      <td>0.075778</td>
      <td>01:55</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.350725</td>
      <td>0.203012</td>
      <td>0.066982</td>
      <td>01:57</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.261454</td>
      <td>0.193684</td>
      <td>0.061570</td>
      <td>01:53</td>
    </tr>
  </tbody>
</table>


Deep learning with resnet34 gives an validation error rate of 6.16%, that's 93.84% accuracy with a few line of codes. Obviously we can further improve the model, but this is already a very decent model!

We will explore more on why resnet34 work so well in the future, for now, just embrace the beauty of it.


```python
# save the parameters/tuned weights for the network
learn.save('stage-1')
```

## Results

Let's see what results we have got.

We will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not. In this case the mistakes look reasonable (none of the mistakes seems obviously naive). This is an indicator that our classifier is working correctly.

Furthermore, when we plot the confusion matrix, we can see that the distribution is heavily skewed: the model makes the same mistakes over and over again but it rarely confuses other categories. This suggests that it just finds it difficult to distinguish some specific categories between each other; this is normal behaviour.


```python
type(learn) # learner object has data and trained model
```




    fastai.basic_train.Learner




```python
interp = ClassificationInterpretation.from_learner(learn) # who said deep learning was a black box?
```


```python
interp.plot_top_losses(9, figsize=(15,11))
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/assets/images/Pets/output_38_0.png" alt="">


Basically, the title of each image shows: prediction, actual, loss, probability of actual class.

For example, if we look at the top left image, the model predicts that the breed to be staffordshire bull terrier but it turned out to be german shorthaired, with a loss of 8.03, and a 0% prediction probability of actual class.

Visual inspection also tells us that resnet34 does a poor classification job when the image has some kind of weird contrast level.


```python
# This is to visualize actual vs predicted, but harder to see when its a multi-classification
interp.plot_confusion_matrix(figsize=(12,12), dpi=100)
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/assets/images/Pets/output_40_0.png" alt="">


Seems like resnet34 tend to misclassify 'staffordshire bull terrier' as 'american pit bull terrier', any dog expert here?


```python
# this output actual, predicted, and frequency of misclassification
interp.most_confused(min_val=2)
```




    [('staffordshire_bull_terrier', 'american_pit_bull_terrier', 7),
     ('Ragdoll', 'Birman', 5),
     ('British_Shorthair', 'Russian_Blue', 4),
     ('Egyptian_Mau', 'Bengal', 4),
     ('Maine_Coon', 'Bengal', 4),
     ('Birman', 'Ragdoll', 3),
     ('Siamese', 'Birman', 3),
     ('american_pit_bull_terrier', 'staffordshire_bull_terrier', 3),
     ('Bengal', 'Abyssinian', 2),
     ('Maine_Coon', 'Persian', 2),
     ('Persian', 'Ragdoll', 2),
     ('Ragdoll', 'Maine_Coon', 2),
     ('Russian_Blue', 'Bombay', 2),
     ('american_bulldog', 'staffordshire_bull_terrier', 2),
     ('basset_hound', 'beagle', 2),
     ('beagle', 'basset_hound', 2)]



## Unfreezing, fine-tuning, and learning rates

Since our model is working as we expect it to, we will *unfreeze* our model and train some more.

The basic idea is that when we call `fit_one_cycle`,  the model freeze the early convolutional layers of the network and only train the last few layers which make a prediction, which is a lot faster.


```python
learn.unfreeze()
```


```python
learn.fit_one_cycle(1)
```


Total time: 02:03 <p><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0.509557</td>
      <td>0.292939</td>
      <td>0.102165</td>
      <td>02:03</td>
    </tr>
  </tbody>
</table>


Unexpectedly, when we try to tune more layers of the network, we get a worse result.

Different layers in convolutional neural network represents different levels of semantic complexitiy

By default, the model trains all layers at the same speed. In another word, it will update the early convolutional layers which represent like diagonals and gradients (ImageNet) just as much it tries to update the later convolutional layers which represent the exact specifics of how an eyeball look like, and we do not want that to be the case when we know what we are looking for; cat breeds & dog breeds.


```python
# load our previously saved parameters/weights
learn.load('stage-1');
```

We use `lr_find()` to find out what is the fastest learning rate (how quickly are we updating the parameters in the model) we should pick, without causing the divergent behaviours.


```python
learn.lr_find()
```





    LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.



```python
learn.recorder.plot()
```


<img src="{{ site.url }}{{ site.baseurl }}/assets/images/assets/images/Pets/output_51_0.png" alt="">



```python
# train first layer at lr of 1e-6, and last layer at lr of 1e-4, and every other layers in between the range
# good rule of thumb is to make 2nd part of the slice 10 times smaller (i.e. 1e-04) than your first stage (i.e. 1e-03)
# and first slice should be guided by visualizing lr_finder()
learn.unfreeze()
learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-4))
```


Total time: 04:09 <p><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0.238652</td>
      <td>0.185368</td>
      <td>0.058863</td>
      <td>02:04</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.219355</td>
      <td>0.185097</td>
      <td>0.057510</td>
      <td>02:05</td>
    </tr>
  </tbody>
</table>


That's right, the model has a validation error rate of 5.75%, which is equivalent to a 94.25% of accuracy after learning rate tuning. **Can you believe we best the top researchers in University of Oxford in 2012 with just a few lines of codes?**

That my friend, is known as 'Deep Learning'. In all seriousness, we didn't really cover the maths and theories behind the algorithm, but that's really because I am following through a course by [fast.ai](https://www.fast.ai/) and its following a somewhat unusual learning approach in being *top-down* rather than bottom-up. So rather than starting with theory, and only getting to practical applications later, instead we start with practical applications, and then gradually dig deeper and deeper in to them, learning the theory as needed.

Anyway, there will be more projects coming up as I learn them, thank you for reading and have an awesome day!
